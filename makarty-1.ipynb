{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import system as bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bash(\"\\\n",
    "POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py \\\n",
    ">> log-test_synt_affinity2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "for i in `seq 10`;do echo $i; sleep 1;done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "from IPython.core.magic import register_line_magic\n",
    "\n",
    "\n",
    "@register_line_magic\n",
    "def runrealcmd(command):\n",
    "    process = Popen(command, stdout=PIPE, shell=True, stderr=STDOUT, bufsize=1, close_fds=True)\n",
    "    for line in iter(process.stdout.readline, b''):\n",
    "        print(line.rstrip().decode('utf-8'))\n",
    "    process.stdout.close()\n",
    "    process.wait()\n",
    "\n",
    "@register_line_magic\n",
    "def tox(command):\n",
    "    process = Popen(command, stdout=PIPE, shell=True, stderr=STDOUT, bufsize=1, close_fds=True)\n",
    "    counter = 0\n",
    "    for line in iter(process.stdout.readline, b''):\n",
    "        counter += 1\n",
    "        if counter <= 135: continue\n",
    "        print(line.rstrip().decode('utf-8'))\n",
    "    process.stdout.close()\n",
    "    process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PING www.google.com (74.125.201.106) 56(84) bytes of data.\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=1 ttl=52 time=1.20 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=2 ttl=52 time=1.67 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=3 ttl=52 time=1.57 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=4 ttl=52 time=1.33 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=5 ttl=52 time=1.17 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=6 ttl=52 time=1.49 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=7 ttl=52 time=1.41 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=8 ttl=52 time=1.29 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=9 ttl=52 time=1.45 ms\n",
      "64 bytes from in-in-f106.1e100.net (74.125.201.106): icmp_seq=10 ttl=52 time=1.23 ms\n",
      "\n",
      "--- www.google.com ping statistics ---\n",
      "10 packets transmitted, 10 received, 0% packet loss, time 9014ms\n",
      "rtt min/avg/max/mdev = 1.179/1.386/1.679/0.163 ms\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd ping -c10 www.google.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd for i in `seq 10`;do echo $i; sleep 1;done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodledev inst-nodeps: /home/vasily/artem/kubectl-val/.tox/.tmp/package/5/kubectl-val-0.1.4.tar.gz\n",
      "poodledev installed: aiohttp==3.6.2,altgraph==0.16.1,apipkg==1.5,appdirs==1.4.3,astroid==2.3.3,async-timeout==3.0.1,attrs==19.3.0,bowler==0.8.0,bsdiff4==1.1.5,Cerberus==1.3.2,certifi==2019.11.28,chardet==3.0.4,Click==7.0,coverage==4.5.4,dephell==0.7.9,dephell-archive==0.1.5,dephell-discover==0.2.10,dephell-licenses==0.1.6,dephell-links==0.1.4,dephell-markers==1.0.2,dephell-pythons==0.1.12,dephell-setuptools==0.2.1,dephell-shells==0.1.3,dephell-specifier==0.2.1,dephell-venvs==0.1.17,dephell-versioning==0.1.1,docker==4.1.0,dockerpty==0.4.1,docutils==0.15.2,dsdev-utils==1.0.4,ed25519==1.5,execnet==1.7.1,fissix==19.2b1,Flask==1.1.1,flatdict==3.4.0,gitdb2==2.0.6,GitPython==3.0.5,html5lib==1.0.1,idna==2.8,importlib-metadata==1.1.0,isort==4.3.21,itsdangerous==1.1.0,Jinja2==2.10.3,kubectl-val==0.1.4,lazy-object-proxy==1.4.3,logzero==1.5.0,m2r==0.2.1,MarkupSafe==1.1.1,mccabe==0.6.1,mistune==0.8.4,more-itertools==8.0.0,multidict==4.6.1,packaging==19.2,pbr==5.4.4,pexpect==4.7.0,pluggy==0.13.1,poodle==0.1.10,ptyprocess==0.6.0,py==1.8.0,Pygments==2.5.2,PyInstaller==3.5,pylint==2.4.4,pyparsing==2.4.5,pytest==5.3.1,pytest-cache==1.0,pytest-cov==2.8.1,pytest-pylint==0.14.1,PyUpdater==3.1.1,PyYAML==5.2,requests==2.22.0,ruamel.yaml==0.16.5,ruamel.yaml.clib==0.2.0,sh==1.12.14,shellingham==1.3.1,six==1.13.0,smmap2==2.0.5,stevedore==1.31.0,tabulate==0.8.6,tomlkit==0.5.8,typed-ast==1.4.0,urllib3==1.25.7,wcwidth==0.1.7,webencodings==0.5.1,websocket-client==0.56.0,Werkzeug==0.16.0,wrapt==1.11.2,yarl==1.4.1,yaspin==0.15.0,zipp==0.6.0\n",
      "poodledev run-test-pre: PYTHONHASHSEED='3725075330'\n",
      "poodledev run-test: commands[0] | bash -c 'cd ../poodle && dephell project build --from pyproject.toml'\n",
      "WARNING cannot find tool.dephell section in the config (path=pyproject.toml)\n",
      "INFO dumping... (format=setuppy)\n",
      "INFO dumping... (format=egginfo)\n",
      "INFO dumping... (format=sdist)\n",
      "INFO dumping... (format=wheel)\n",
      "INFO builded\n",
      "poodledev run-test: commands[1] | pip uninstall -y poodle\n",
      "Uninstalling poodle-0.1.10:\n",
      "  Successfully uninstalled poodle-0.1.10\n",
      "poodledev run-test: commands[2] | bash -c 'cd ../poodle && python ./setup.py install'\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing poodle.egg-info/PKG-INFO\n",
      "writing dependency_links to poodle.egg-info/dependency_links.txt\n",
      "writing entry points to poodle.egg-info/entry_points.txt\n",
      "writing requirements to poodle.egg-info/requires.txt\n",
      "writing top-level names to poodle.egg-info/top_level.txt\n",
      "reading manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "writing manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/poodle_main.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/schedule.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/arithmetic.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/pddlSplitter.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/web_solver.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/__init__.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/problem.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/poodle_main.py to poodle_main.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/schedule.py to schedule.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/arithmetic.py to arithmetic.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/pddlSplitter.py to pddlSplitter.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/web_solver.py to web_solver.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/problem.py to problem.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "poodle.__pycache__.poodle_main.cpython-37: module references __file__\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getsourcefile\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.findsource\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getframeinfo\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getouterframes\n",
      "creating 'dist/poodle-0.1.10-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing poodle-0.1.10-py3.7.egg\n",
      "creating /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Extracting poodle-0.1.10-py3.7.egg to /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Adding poodle 0.1.10 to easy-install.pth file\n",
      "Installing poodleserver script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Installed /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Processing dependencies for poodle==0.1.10\n",
      "Searching for requests==2.22.0\n",
      "Best match: requests 2.22.0\n",
      "Adding requests 2.22.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Flask==1.1.1\n",
      "Best match: Flask 1.1.1\n",
      "Adding Flask 1.1.1 to easy-install.pth file\n",
      "Installing flask script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for idna==2.8\n",
      "Best match: idna 2.8\n",
      "Adding idna 2.8 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for certifi==2019.11.28\n",
      "Best match: certifi 2019.11.28\n",
      "Adding certifi 2019.11.28 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.25.7\n",
      "Best match: urllib3 1.25.7\n",
      "Adding urllib3 1.25.7 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Jinja2==2.10.3\n",
      "Best match: Jinja2 2.10.3\n",
      "Adding Jinja2 2.10.3 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Werkzeug==0.16.0\n",
      "Best match: Werkzeug 0.16.0\n",
      "Adding Werkzeug 0.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for itsdangerous==1.1.0\n",
      "Best match: itsdangerous 1.1.0\n",
      "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Finished processing dependencies for poodle==0.1.10\n",
      "poodledev run-test: commands[3] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           548721\n",
      "poodledev run-test: commands[4] | bash -c 'cd ../downward && timeout 40000 poodleserver 2>&1 >/dev/null &'\n",
      "poodledev run-test: commands[5] | python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py::test_3\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python\n",
      "cachedir: .tox/poodledev/.pytest_cache\n",
      "rootdir: /home/vasily/artem/kubectl-val\n",
      "plugins: pylint-0.14.1, cov-2.8.1\n",
      "collecting ... [D 191206 13:57:12 libs_for_tests:31] hello\n",
      "collected 1 item\n",
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "tests/test_synt_affinity.py::test_3 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 1, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod3\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None)\n",
      "mark_3_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, pod1=pod1, pod2=pod5, pod3=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 1, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod3\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fdadbdacfd0>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None)\n",
      "mark_3_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, pod1=pod1, pod2=pod5, pod3=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fdadba961d0>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "44.56s call     tests/test_synt_affinity.py::test_3\n",
      "0.00s setup    tests/test_synt_affinity.py::test_3\n",
      "0.00s teardown tests/test_synt_affinity.py::test_3\n",
      "======================== 1 passed, 1 warning in 49.48s =========================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           550423\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 550422 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHtest_6_3nodes_but_needed5nodes_possible_to_add_2_nodes_suggests_thisT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodledev inst-nodeps: /home/vasily/artem/kubectl-val/.tox/.tmp/package/5/kubectl-val-0.1.4.tar.gz\n",
      "poodledev installed: aiohttp==3.6.2,altgraph==0.16.1,apipkg==1.5,appdirs==1.4.3,astroid==2.3.3,async-timeout==3.0.1,attrs==19.3.0,bowler==0.8.0,bsdiff4==1.1.5,Cerberus==1.3.2,certifi==2019.11.28,chardet==3.0.4,Click==7.0,coverage==4.5.4,dephell==0.7.9,dephell-archive==0.1.5,dephell-discover==0.2.10,dephell-licenses==0.1.6,dephell-links==0.1.4,dephell-markers==1.0.2,dephell-pythons==0.1.12,dephell-setuptools==0.2.1,dephell-shells==0.1.3,dephell-specifier==0.2.1,dephell-venvs==0.1.17,dephell-versioning==0.1.1,docker==4.1.0,dockerpty==0.4.1,docutils==0.15.2,dsdev-utils==1.0.4,ed25519==1.5,execnet==1.7.1,fissix==19.2b1,Flask==1.1.1,flatdict==3.4.0,gitdb2==2.0.6,GitPython==3.0.5,html5lib==1.0.1,idna==2.8,importlib-metadata==1.1.0,isort==4.3.21,itsdangerous==1.1.0,Jinja2==2.10.3,kubectl-val==0.1.4,lazy-object-proxy==1.4.3,logzero==1.5.0,m2r==0.2.1,MarkupSafe==1.1.1,mccabe==0.6.1,mistune==0.8.4,more-itertools==8.0.0,multidict==4.6.1,packaging==19.2,pbr==5.4.4,pexpect==4.7.0,pluggy==0.13.1,poodle==0.1.10,ptyprocess==0.6.0,py==1.8.0,Pygments==2.5.2,PyInstaller==3.5,pylint==2.4.4,pyparsing==2.4.5,pytest==5.3.1,pytest-cache==1.0,pytest-cov==2.8.1,pytest-pylint==0.14.1,PyUpdater==3.1.1,PyYAML==5.2,requests==2.22.0,ruamel.yaml==0.16.5,ruamel.yaml.clib==0.2.0,sh==1.12.14,shellingham==1.3.1,six==1.13.0,smmap2==2.0.5,stevedore==1.31.0,tabulate==0.8.6,tomlkit==0.5.8,typed-ast==1.4.0,urllib3==1.25.7,wcwidth==0.1.7,webencodings==0.5.1,websocket-client==0.56.0,Werkzeug==0.16.0,wrapt==1.11.2,yarl==1.4.1,yaspin==0.15.0,zipp==0.6.0\n",
      "poodledev run-test-pre: PYTHONHASHSEED='1193921303'\n",
      "poodledev run-test: commands[0] | bash -c 'cd ../poodle && dephell project build --from pyproject.toml'\n",
      "WARNING cannot find tool.dephell section in the config (path=pyproject.toml)\n",
      "INFO dumping... (format=setuppy)\n",
      "INFO dumping... (format=egginfo)\n",
      "INFO dumping... (format=sdist)\n",
      "INFO dumping... (format=wheel)\n",
      "INFO builded\n",
      "poodledev run-test: commands[1] | pip uninstall -y poodle\n",
      "Uninstalling poodle-0.1.10:\n",
      "  Successfully uninstalled poodle-0.1.10\n",
      "poodledev run-test: commands[2] | bash -c 'cd ../poodle && python ./setup.py install'\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing poodle.egg-info/PKG-INFO\n",
      "writing dependency_links to poodle.egg-info/dependency_links.txt\n",
      "writing entry points to poodle.egg-info/entry_points.txt\n",
      "writing requirements to poodle.egg-info/requires.txt\n",
      "writing top-level names to poodle.egg-info/top_level.txt\n",
      "reading manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "writing manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/poodle_main.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/schedule.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/arithmetic.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/pddlSplitter.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/web_solver.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/__init__.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/problem.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/poodle_main.py to poodle_main.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/schedule.py to schedule.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/arithmetic.py to arithmetic.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/pddlSplitter.py to pddlSplitter.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/web_solver.py to web_solver.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/problem.py to problem.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "poodle.__pycache__.poodle_main.cpython-37: module references __file__\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getsourcefile\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.findsource\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getframeinfo\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getouterframes\n",
      "creating 'dist/poodle-0.1.10-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing poodle-0.1.10-py3.7.egg\n",
      "creating /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Extracting poodle-0.1.10-py3.7.egg to /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Adding poodle 0.1.10 to easy-install.pth file\n",
      "Installing poodleserver script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Installed /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Processing dependencies for poodle==0.1.10\n",
      "Searching for requests==2.22.0\n",
      "Best match: requests 2.22.0\n",
      "Adding requests 2.22.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Flask==1.1.1\n",
      "Best match: Flask 1.1.1\n",
      "Adding Flask 1.1.1 to easy-install.pth file\n",
      "Installing flask script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for certifi==2019.11.28\n",
      "Best match: certifi 2019.11.28\n",
      "Adding certifi 2019.11.28 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.25.7\n",
      "Best match: urllib3 1.25.7\n",
      "Adding urllib3 1.25.7 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for idna==2.8\n",
      "Best match: idna 2.8\n",
      "Adding idna 2.8 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Jinja2==2.10.3\n",
      "Best match: Jinja2 2.10.3\n",
      "Adding Jinja2 2.10.3 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for itsdangerous==1.1.0\n",
      "Best match: itsdangerous 1.1.0\n",
      "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Werkzeug==0.16.0\n",
      "Best match: Werkzeug 0.16.0\n",
      "Adding Werkzeug 0.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Finished processing dependencies for poodle==0.1.10\n",
      "poodledev run-test: commands[3] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "CLEAN\n",
      "poodledev run-test: commands[4] | bash -c 'cd ../downward && timeout 40000 poodleserver 2>&1 >/dev/null &'\n",
      "poodledev run-test: commands[5] | python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py::test_4\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python\n",
      "cachedir: .tox/poodledev/.pytest_cache\n",
      "rootdir: /home/vasily/artem/kubectl-val\n",
      "plugins: pylint-0.14.1, cov-2.8.1\n",
      "collecting ... [D 191206 13:59:37 libs_for_tests:31] hello\n",
      "collected 1 item\n",
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "tests/test_synt_affinity.py::test_4 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 5, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod4\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 1, pod1=pod4\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 5, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 4, node_of_pod4=node 5, pod1=pod2, pod2=pod5, pod3=pod6, pod4=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 5, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod4\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod3\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 1, pod1=pod4\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod2\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod2\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "manually_initiate_killing_of_pod: pod_killed=pod6\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 5, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod6\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod6, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7f41000d1810>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 4, node_of_pod4=node 5, pod1=pod2, pod2=pod5, pod3=pod6, pod4=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7f41000d1890>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "151.44s call     tests/test_synt_affinity.py::test_4\n",
      "0.00s setup    tests/test_synt_affinity.py::test_4\n",
      "0.00s teardown tests/test_synt_affinity.py::test_4\n",
      "=================== 1 passed, 1 warning in 156.28s (0:02:36) ===================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           550726\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 550725 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%runrealcmd POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHtest_6_3nodes_but_needed5nodes_possible_to_add_2_nodes_suggests_thisT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tests/test_synt_affinity.py::test_5 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5', 'node 6']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5', 'node 6']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 6']\n",
      "## Node:node 6, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4', 'node 5']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 2, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod5\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod5\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 4, pod1=pod1\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 4, pod1=pod2, pod2=pod6, pod3=pod5, pod4=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 2, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod5\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod5\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 4, pod1=pod1\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None)\n",
      "mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 4, pod1=pod2, pod2=pod6, pod3=pod5, pod4=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), service=test-service\n",
      "mark_antiaffinity_prefered_policy_met: service=test-service\n",
      "PASSED\n",
      "\n",
      "========================== slowest 100 test durations ==========================\n",
      "59.01s call     tests/test_synt_affinity.py::test_5\n",
      "0.00s setup    tests/test_synt_affinity.py::test_5\n",
      "0.00s teardown tests/test_synt_affinity.py::test_5\n",
      "======================== 1 passed, 1 warning in 59.65s =========================\n",
      "poodledev run-test: commands[6] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           540200\n",
      "___________________________________ summary ____________________________________\n",
      "  poodledev: commands succeeded\n",
      "  congratulations :)\n",
      "/bin/bash: line 1: 540199 Killed                  timeout 40000 poodleserver 2>&1 > /dev/null\n"
     ]
    }
   ],
   "source": [
    "%tox POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHtest_6_3nodes_but_needed5nodes_possible_to_add_2_nodes_suggests_thisT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "\n",
    "    # manually_initiate_killing_of_pod: pod_killed=pod5\n",
    "    # p.SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
    "    # SelectNode: SelectedNode=node 2, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod6\n",
    "    # SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, pod1=pod1\n",
    "    # KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
    "    # SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod5\n",
    "    # SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod5\n",
    "    # SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod5\n",
    "    # manually_initiate_killing_of_pod: pod_killed=pod1\n",
    "    # KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
    "    # SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
    "    # SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 4, pod1=pod1\n",
    "    # StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
    "    # StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), node=node 4, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
    "    # SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7efbff212290>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None)\n",
    "    # mark_4_pods_of_service_as_not_at_same_node: node_of_pod1=node 1, node_of_pod2=node 2, node_of_pod3=node 3, node_of_pod4=node 4, pod1=pod2, pod2=pod6, pod3=pod5, pod4=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7efbff212210>(name=Scheduler-3563-obj, value=None), service=test-service\n",
    "    # mark_antiaffinity_prefered_policy_met: service=test-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%tox POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_6_3nodes_but_needed4nodes_possible_to_add_2_nodes_suggests_this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                    \n",
    "551922 vasily    20   0 6182404 5.799g  44180 R 100.0  5.7   2:10.97 pypy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "(selectnode globalvar-3567-obj pod-3663-obj node-3639-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3663-obj logsparseinteger-171-obj-num-8 node-4152-obj node-3639-obj)\n",
    "(selectnode globalvar-3567-obj pod-3869-obj node-3639-obj)\n",
    "(selectnode globalvar-3567-obj pod-3711-obj node-3639-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3711-obj logsparseinteger-171-obj-num-8 node-3845-obj node-3639-obj)\n",
    "(selectnode globalvar-3567-obj pod-3759-obj node-4152-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3869-obj logsparseinteger-171-obj-num-8 node-4176-obj node-3639-obj)\n",
    "(manually_initiate_killing_of_pod pod-3663-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj node-3639-obj pod-3663-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3663-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 node-3639-obj pod-3663-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 node-3639-obj pod-3663-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 service-3581-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3 sumresult-576-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 service-3581-obj logsparseinteger-163-obj-num-0 sumresult-293-obj logsparseinteger-165-obj-num-1)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 service-3581-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3 sumresult-576-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 pod-3663-obj node-4152-obj scheduler-3563-obj statussched-85-obj logsparseinteger-163-obj-num-0 sumresult-293-obj logsparseinteger-165-obj-num-1)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 pod-3663-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 pod-3663-obj node-4152-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 node-3639-obj pod-3663-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3663-obj scheduler-3563-obj statussched-85-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 scheduler-3563-obj logsparseinteger-163-obj-num-0)\n",
    "(selectnode globalvar-3567-obj pod-3663-obj node-4176-obj)\n",
    "(add_node node-4176-obj)\n",
    "(setdefaultmemlimitforpodbeforenodeassignment globalvar-3567-obj pod-3663-obj logsparseinteger-169-obj-num-4 node-4061-obj node-4176-obj)\n",
    "(manually_initiate_killing_of_pod pod-3711-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj node-3639-obj pod-3711-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3711-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 node-3639-obj pod-3711-obj logsparseinteger-143-obj-num-6 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-143-obj-num-6 logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4 sumresult-675-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 node-3639-obj pod-3711-obj logsparseinteger-143-obj-num-6 logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4 sumresult-675-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 service-3581-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 service-3581-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 service-3581-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 node-3639-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 node-3639-obj logsparseinteger-143-obj-num-6 logsparseinteger-169-obj-num-4)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 node-3639-obj logsparseinteger-143-obj-num-6 logsparseinteger-169-obj-num-4)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 pod-3711-obj node-3845-obj scheduler-3563-obj statussched-85-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 pod-3711-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 pod-3711-obj node-3845-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 node-3639-obj pod-3711-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3711-obj scheduler-3563-obj statussched-85-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 scheduler-3563-obj logsparseinteger-165-obj-num-1)\n",
    "(selectnode globalvar-3567-obj pod-3711-obj node-3639-obj)\n",
    "(setdefaultmemlimitforpodbeforenodeassignment globalvar-3567-obj pod-3711-obj logsparseinteger-171-obj-num-8 node-4128-obj node-3639-obj)\n",
    "(manually_initiate_killing_of_pod pod-3917-obj)\n",
    "(selectnode globalvar-3567-obj pod-3917-obj node-4128-obj)\n",
    "(setdefaultcpulimitforpodbeforenodeassignment globalvar-3567-obj pod-3917-obj logsparseinteger-171-obj-num-8 node-4176-obj node-4128-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj node-3845-obj pod-3917-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3917-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 node-3845-obj pod-3917-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 node-3845-obj pod-3917-obj logsparseinteger-171-obj-num-8 logsparseinteger-167-obj-num-2 logsparseinteger-143-obj-num-6 sumresult-863-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1 sumresult-388-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 service-3581-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 node-3845-obj logsparseinteger-169-obj-num-4 logsparseinteger-137-obj-num-3 sumresult-576-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 node-3845-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 node-3845-obj logsparseinteger-171-obj-num-8 logsparseinteger-143-obj-num-6)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 pod-3917-obj node-4176-obj scheduler-3563-obj statussched-85-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 pod-3917-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 pod-3917-obj node-4176-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 node-3845-obj pod-3917-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3917-obj scheduler-3563-obj statussched-85-obj)\n",
    "(killpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 scheduler-3563-obj logsparseinteger-167-obj-num-2)\n",
    "(selectnode globalvar-3567-obj pod-3917-obj node-3639-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj pod-3711-obj scheduler-3563-obj node-4128-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3711-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 pod-3711-obj node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 logsparseinteger-167-obj-num-2 greaterequal-2311-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 pod-3711-obj node-4128-obj logsparseinteger-171-obj-num-8 logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 logsparseinteger-167-obj-num-2 greaterequal-2311-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 pod-3711-obj node-4128-obj logsparseinteger-171-obj-num-8 logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 pod-3711-obj node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2 sumresult-298-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 pod-3711-obj scheduler-3563-obj node-3442-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 service-3581-obj logsparseinteger-137-obj-num-3 logsparseinteger-167-obj-num-2 sumresult-482-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 node-4128-obj logsparseinteger-163-obj-num-0 sumresult-293-obj logsparseinteger-165-obj-num-1)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 node-4128-obj logsparseinteger-163-obj-num-0 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 pod-3711-obj service-3581-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2 statuspod-63-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3711-obj node-4128-obj node-3442-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 pod-3711-obj scheduler-3563-obj service-3581-obj logsparseinteger-137-obj-num-3)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-17 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-18 pod-3711-obj statuspod-63-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-19 service-3581-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj pod-3917-obj scheduler-3563-obj node-3639-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3917-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 pod-3917-obj node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 logsparseinteger-143-obj-num-6 greaterequal-2343-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 pod-3917-obj node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 logsparseinteger-143-obj-num-6 greaterequal-2343-obj logsparseinteger-171-obj-num-8)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 pod-3917-obj node-3639-obj logsparseinteger-171-obj-num-8 logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 pod-3917-obj node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 sumresult-675-obj logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 pod-3917-obj scheduler-3563-obj node-3442-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1 sumresult-388-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 service-3581-obj logsparseinteger-167-obj-num-2 logsparseinteger-165-obj-num-1 sumresult-388-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 node-3639-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 node-3639-obj logsparseinteger-169-obj-num-4 logsparseinteger-143-obj-num-6)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 pod-3917-obj service-3581-obj logsparseinteger-167-obj-num-2 sumresult-482-obj logsparseinteger-137-obj-num-3 statuspod-63-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3917-obj node-3639-obj node-3442-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 pod-3917-obj scheduler-3563-obj service-3581-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-17 service-3581-obj logsparseinteger-165-obj-num-1 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-18 pod-3917-obj statuspod-63-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-19 service-3581-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-0 globalvar-3567-obj pod-3663-obj scheduler-3563-obj node-4061-obj service-3581-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-1 pod-3663-obj logsparseinteger-167-obj-num-2 greaterthan-1995-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-2 pod-3663-obj node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-3 logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-4 logsparseinteger-169-obj-num-4 greaterequal-2099-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-5 pod-3663-obj node-4061-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-6 logsparseinteger-169-obj-num-4 greaterequal-2099-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-7 pod-3663-obj node-4061-obj logsparseinteger-169-obj-num-4 logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-8 pod-3663-obj node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-167-obj-num-2 sumresult-487-obj logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-9 pod-3663-obj scheduler-3563-obj node-3442-obj logsparseinteger-165-obj-num-1 logsparseinteger-163-obj-num-0 sumresult-293-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-10 service-3581-obj logsparseinteger-165-obj-num-1 logsparseinteger-163-obj-num-0 sumresult-293-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-11 node-4061-obj logsparseinteger-165-obj-num-1 sumresult-388-obj logsparseinteger-167-obj-num-2)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-12 node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-13 node-4061-obj logsparseinteger-167-obj-num-2 logsparseinteger-169-obj-num-4)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-14 pod-3663-obj service-3581-obj logsparseinteger-137-obj-num-3 sumresult-576-obj logsparseinteger-169-obj-num-4 statuspod-63-obj statusserv-93-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-15 pod-3663-obj node-4061-obj node-3442-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-16 pod-3663-obj scheduler-3563-obj service-3581-obj logsparseinteger-165-obj-num-1)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-17 service-3581-obj logsparseinteger-163-obj-num-0 logsparseinteger-137-obj-num-3)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-18 pod-3663-obj statuspod-63-obj)\n",
    "(startpod_if_deployment_isnull_service_isnotnull_daemonset_isnull-19 service-3581-obj statusserv-93-obj)\n",
    "(schedulercleaned globalvar-3567-obj scheduler-3563-obj statussched-85-obj)\n",
    "(mark_4_pods_of_service_as_not_at_same_node node-3639-obj pod-3917-obj node-3845-obj pod-3869-obj node-4061-obj pod-3663-obj node-4128-obj pod-3711-obj service-3581-obj scheduler-3563-obj logsparseinteger-163-obj-num-0)\n",
    "(mark_antiaffinity_prefered_policy_met logsparseinteger-169-obj-num-4 service-3581-obj booleanobject-20-obj)\n",
    "; cost = 27 (general cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodledev inst-nodeps: /home/vasily/artem/kubectl-val/.tox/.tmp/package/5/kubectl-val-0.1.4.tar.gz\n",
      "poodledev installed: aiohttp==3.6.2,altgraph==0.16.1,apipkg==1.5,appdirs==1.4.3,astroid==2.3.3,async-timeout==3.0.1,attrs==19.3.0,bowler==0.8.0,bsdiff4==1.1.5,Cerberus==1.3.2,certifi==2019.11.28,chardet==3.0.4,Click==7.0,coverage==4.5.4,dephell==0.7.9,dephell-archive==0.1.5,dephell-discover==0.2.10,dephell-licenses==0.1.6,dephell-links==0.1.4,dephell-markers==1.0.2,dephell-pythons==0.1.12,dephell-setuptools==0.2.1,dephell-shells==0.1.3,dephell-specifier==0.2.1,dephell-venvs==0.1.17,dephell-versioning==0.1.1,docker==4.1.0,dockerpty==0.4.1,docutils==0.15.2,dsdev-utils==1.0.4,ed25519==1.5,execnet==1.7.1,fissix==19.2b1,Flask==1.1.1,flatdict==3.4.0,gitdb2==2.0.6,GitPython==3.0.5,html5lib==1.0.1,idna==2.8,importlib-metadata==1.1.0,isort==4.3.21,itsdangerous==1.1.0,Jinja2==2.10.3,kubectl-val==0.1.4,lazy-object-proxy==1.4.3,logzero==1.5.0,m2r==0.2.1,MarkupSafe==1.1.1,mccabe==0.6.1,mistune==0.8.4,more-itertools==8.0.0,multidict==4.6.1,packaging==19.2,pbr==5.4.4,pexpect==4.7.0,pluggy==0.13.1,poodle==0.1.10,ptyprocess==0.6.0,py==1.8.0,Pygments==2.5.2,PyInstaller==3.5,pylint==2.4.4,pyparsing==2.4.5,pytest==5.3.1,pytest-cache==1.0,pytest-cov==2.8.1,pytest-pylint==0.14.1,PyUpdater==3.1.1,PyYAML==5.2,requests==2.22.0,ruamel.yaml==0.16.5,ruamel.yaml.clib==0.2.0,sh==1.12.14,shellingham==1.3.1,six==1.13.0,smmap2==2.0.5,stevedore==1.31.0,tabulate==0.8.6,tomlkit==0.5.8,typed-ast==1.4.0,urllib3==1.25.7,wcwidth==0.1.7,webencodings==0.5.1,websocket-client==0.56.0,Werkzeug==0.16.0,wrapt==1.11.2,yarl==1.4.1,yaspin==0.15.0,zipp==0.6.0\n",
      "poodledev run-test-pre: PYTHONHASHSEED='2487308073'\n",
      "poodledev run-test: commands[0] | bash -c 'cd ../poodle && dephell project build --from pyproject.toml'\n",
      "WARNING cannot find tool.dephell section in the config (path=pyproject.toml)\n",
      "INFO dumping... (format=setuppy)\n",
      "INFO dumping... (format=egginfo)\n",
      "INFO dumping... (format=sdist)\n",
      "INFO dumping... (format=wheel)\n",
      "INFO builded\n",
      "poodledev run-test: commands[1] | pip uninstall -y poodle\n",
      "Uninstalling poodle-0.1.10:\n",
      "  Successfully uninstalled poodle-0.1.10\n",
      "poodledev run-test: commands[2] | bash -c 'cd ../poodle && python ./setup.py install'\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing poodle.egg-info/PKG-INFO\n",
      "writing dependency_links to poodle.egg-info/dependency_links.txt\n",
      "writing entry points to poodle.egg-info/entry_points.txt\n",
      "writing requirements to poodle.egg-info/requires.txt\n",
      "writing top-level names to poodle.egg-info/top_level.txt\n",
      "reading manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "writing manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/poodle_main.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/schedule.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/arithmetic.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/pddlSplitter.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/web_solver.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/__init__.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/problem.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/poodle_main.py to poodle_main.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/schedule.py to schedule.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/arithmetic.py to arithmetic.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/pddlSplitter.py to pddlSplitter.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/web_solver.py to web_solver.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/problem.py to problem.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "poodle.__pycache__.poodle_main.cpython-37: module references __file__\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getsourcefile\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.findsource\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getframeinfo\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getouterframes\n",
      "creating 'dist/poodle-0.1.10-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing poodle-0.1.10-py3.7.egg\n",
      "creating /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Extracting poodle-0.1.10-py3.7.egg to /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Adding poodle 0.1.10 to easy-install.pth file\n",
      "Installing poodleserver script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Installed /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Processing dependencies for poodle==0.1.10\n",
      "Searching for requests==2.22.0\n",
      "Best match: requests 2.22.0\n",
      "Adding requests 2.22.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Flask==1.1.1\n",
      "Best match: Flask 1.1.1\n",
      "Adding Flask 1.1.1 to easy-install.pth file\n",
      "Installing flask script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.25.7\n",
      "Best match: urllib3 1.25.7\n",
      "Adding urllib3 1.25.7 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for idna==2.8\n",
      "Best match: idna 2.8\n",
      "Adding idna 2.8 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for certifi==2019.11.28\n",
      "Best match: certifi 2019.11.28\n",
      "Adding certifi 2019.11.28 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Jinja2==2.10.3\n",
      "Best match: Jinja2 2.10.3\n",
      "Adding Jinja2 2.10.3 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Werkzeug==0.16.0\n",
      "Best match: Werkzeug 0.16.0\n",
      "Adding Werkzeug 0.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for itsdangerous==1.1.0\n",
      "Best match: itsdangerous 1.1.0\n",
      "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Finished processing dependencies for poodle==0.1.10\n",
      "poodledev run-test: commands[3] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           545298\n",
      "poodledev run-test: commands[4] | bash -c 'cd ../downward && timeout 40000 poodleserver 2>&1 >/dev/null &'\n",
      "poodledev run-test: commands[5] | python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py::test_7_3nodes_and_needed3nodes_suggests_movement_of_pods\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python\n",
      "cachedir: .tox/poodledev/.pytest_cache\n",
      "rootdir: /home/vasily/artem/kubectl-val\n",
      "plugins: pylint-0.14.1, cov-2.8.1\n",
      "collecting ... [D 191206 13:11:51 libs_for_tests:31] hello\n",
      "collected 1 item\n",
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ebca1a92258a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runrealcmd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POODLE_LIN_COUNT=12  POODLE_ASTAR_WEIGHT=20  PYTHON=pypy  POODLE_SOLVER_URL=http://localhost:10757  tox -e poodledev  -- -s -x tests/test_synt_affinity.py::test_7_3nodes_and_needed3nodes_suggests_movement_of_pods'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2305\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2307\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2308\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-09ae4cb52495>\u001b[0m in \u001b[0;36mrunrealcmd\u001b[0;34m(command)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrunrealcmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_fds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%runrealcmd POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py::test_7_3nodes_and_needed3nodes_suggests_movement_of_pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poodledev inst-nodeps: /home/vasily/artem/kubectl-val/.tox/.tmp/package/5/kubectl-val-0.1.4.tar.gz\n",
      "poodledev installed: aiohttp==3.6.2,altgraph==0.16.1,apipkg==1.5,appdirs==1.4.3,astroid==2.3.3,async-timeout==3.0.1,attrs==19.3.0,bowler==0.8.0,bsdiff4==1.1.5,Cerberus==1.3.2,certifi==2019.11.28,chardet==3.0.4,Click==7.0,coverage==4.5.4,dephell==0.7.9,dephell-archive==0.1.5,dephell-discover==0.2.10,dephell-licenses==0.1.6,dephell-links==0.1.4,dephell-markers==1.0.2,dephell-pythons==0.1.12,dephell-setuptools==0.2.1,dephell-shells==0.1.3,dephell-specifier==0.2.1,dephell-venvs==0.1.17,dephell-versioning==0.1.1,docker==4.1.0,dockerpty==0.4.1,docutils==0.15.2,dsdev-utils==1.0.4,ed25519==1.5,execnet==1.7.1,fissix==19.2b1,Flask==1.1.1,flatdict==3.4.0,gitdb2==2.0.6,GitPython==3.0.5,html5lib==1.0.1,idna==2.8,importlib-metadata==1.1.0,isort==4.3.21,itsdangerous==1.1.0,Jinja2==2.10.3,kubectl-val==0.1.4,lazy-object-proxy==1.4.3,logzero==1.5.0,m2r==0.2.1,MarkupSafe==1.1.1,mccabe==0.6.1,mistune==0.8.4,more-itertools==8.0.0,multidict==4.6.1,packaging==19.2,pbr==5.4.4,pexpect==4.7.0,pluggy==0.13.1,poodle==0.1.10,ptyprocess==0.6.0,py==1.8.0,Pygments==2.5.2,PyInstaller==3.5,pylint==2.4.4,pyparsing==2.4.5,pytest==5.3.1,pytest-cache==1.0,pytest-cov==2.8.1,pytest-pylint==0.14.1,PyUpdater==3.1.1,PyYAML==5.2,requests==2.22.0,ruamel.yaml==0.16.5,ruamel.yaml.clib==0.2.0,sh==1.12.14,shellingham==1.3.1,six==1.13.0,smmap2==2.0.5,stevedore==1.31.0,tabulate==0.8.6,tomlkit==0.5.8,typed-ast==1.4.0,urllib3==1.25.7,wcwidth==0.1.7,webencodings==0.5.1,websocket-client==0.56.0,Werkzeug==0.16.0,wrapt==1.11.2,yarl==1.4.1,yaspin==0.15.0,zipp==0.6.0\n",
      "poodledev run-test-pre: PYTHONHASHSEED='1447275752'\n",
      "poodledev run-test: commands[0] | bash -c 'cd ../poodle && dephell project build --from pyproject.toml'\n",
      "WARNING cannot find tool.dephell section in the config (path=pyproject.toml)\n",
      "INFO dumping... (format=setuppy)\n",
      "INFO dumping... (format=egginfo)\n",
      "INFO dumping... (format=sdist)\n",
      "INFO dumping... (format=wheel)\n",
      "INFO builded\n",
      "poodledev run-test: commands[1] | pip uninstall -y poodle\n",
      "Uninstalling poodle-0.1.10:\n",
      "  Successfully uninstalled poodle-0.1.10\n",
      "poodledev run-test: commands[2] | bash -c 'cd ../poodle && python ./setup.py install'\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing poodle.egg-info/PKG-INFO\n",
      "writing dependency_links to poodle.egg-info/dependency_links.txt\n",
      "writing entry points to poodle.egg-info/entry_points.txt\n",
      "writing requirements to poodle.egg-info/requires.txt\n",
      "writing top-level names to poodle.egg-info/top_level.txt\n",
      "reading manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "writing manifest file 'poodle.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/poodle_main.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/schedule.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/arithmetic.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/pddlSplitter.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/web_solver.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/__init__.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "copying build/lib/poodle/problem.py -> build/bdist.linux-x86_64/egg/poodle\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/poodle_main.py to poodle_main.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/schedule.py to schedule.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/arithmetic.py to arithmetic.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/pddlSplitter.py to pddlSplitter.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/web_solver.py to web_solver.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/__init__.py to __init__.cpython-37.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/poodle/problem.py to problem.cpython-37.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying poodle.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "poodle.__pycache__.poodle_main.cpython-37: module references __file__\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getsourcefile\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.findsource\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getframeinfo\n",
      "poodle.__pycache__.poodle_main.cpython-37: module MAY be using inspect.getouterframes\n",
      "creating 'dist/poodle-0.1.10-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing poodle-0.1.10-py3.7.egg\n",
      "creating /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Extracting poodle-0.1.10-py3.7.egg to /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Adding poodle 0.1.10 to easy-install.pth file\n",
      "Installing poodleserver script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Installed /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg\n",
      "Processing dependencies for poodle==0.1.10\n",
      "Searching for requests==2.22.0\n",
      "Best match: requests 2.22.0\n",
      "Adding requests 2.22.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Flask==1.1.1\n",
      "Best match: Flask 1.1.1\n",
      "Adding Flask 1.1.1 to easy-install.pth file\n",
      "Installing flask script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for idna==2.8\n",
      "Best match: idna 2.8\n",
      "Adding idna 2.8 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for chardet==3.0.4\n",
      "Best match: chardet 3.0.4\n",
      "Adding chardet 3.0.4 to easy-install.pth file\n",
      "Installing chardetect script to /home/vasily/artem/kubectl-val/.tox/poodledev/bin\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for certifi==2019.11.28\n",
      "Best match: certifi 2019.11.28\n",
      "Adding certifi 2019.11.28 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for urllib3==1.25.7\n",
      "Best match: urllib3 1.25.7\n",
      "Adding urllib3 1.25.7 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Werkzeug==0.16.0\n",
      "Best match: Werkzeug 0.16.0\n",
      "Adding Werkzeug 0.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for itsdangerous==1.1.0\n",
      "Best match: itsdangerous 1.1.0\n",
      "Adding itsdangerous 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for Jinja2==2.10.3\n",
      "Best match: Jinja2 2.10.3\n",
      "Adding Jinja2 2.10.3 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Searching for MarkupSafe==1.1.1\n",
      "Best match: MarkupSafe 1.1.1\n",
      "Adding MarkupSafe 1.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/vasily/artem/kubectl-val/.tox/poodledev/lib/python3.7/site-packages\n",
      "Finished processing dependencies for poodle==0.1.10\n",
      "poodledev run-test: commands[3] | bash -c 'fuser -k -n tcp $(echo -n $POODLE_SOLVER_URL|cut -d: -f3) || echo CLEAN'\n",
      "10757/tcp:           547540\n",
      "poodledev run-test: commands[4] | bash -c 'cd ../downward && timeout 40000 poodleserver 2>&1 >/dev/null &'\n",
      "poodledev run-test: commands[5] | python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py\n",
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.5, pytest-5.3.1, py-1.8.0, pluggy-0.13.1 -- /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python\n",
      "cachedir: .tox/poodledev/.pytest_cache\n",
      "rootdir: /home/vasily/artem/kubectl-val\n",
      "plugins: pylint-0.14.1, cov-2.8.1\n",
      "collecting ... [D 191206 13:15:43 libs_for_tests:31] hello\n",
      "collected 8 items\n",
      "-----------------------------------------------------------------\n",
      "Linting files\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "tests/test_synt_affinity.py PASSED\n",
      "tests/test_synt_affinity.py::test_1 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 10, memCapacity: 10, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 10, memCapacity: 10, CurrentFormalCpuConsumption: 6, CurrentFormalMemConsumption: 6, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 3, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 4, CurrentFormalMemConsumption: 4, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 2, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 3, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5'], IsSearched:  FALSE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 10, memCapacity: 10, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 10, memCapacity: 10, CurrentFormalCpuConsumption: 6, CurrentFormalMemConsumption: 6, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 3, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 4, CurrentFormalMemConsumption: 4, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 2, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 3, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5'], IsSearched:  FALSE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod4\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod4\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod9\n",
      "KillPod_IF_Deployment_isNUll_Service_isNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 3, podBeingKilled=pod9, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod9\n",
      "StartPod_IF_Deployment_isNUll_Service_isNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), node=node 1, podStarted=pod9, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 1, pod1=pod5, pod2=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 3, pod1=pod2, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 3, pod1=pod5, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod4\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod4\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), node=node 4, pod1=pod1\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), memCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod9\n",
      "KillPod_IF_Deployment_isNUll_Service_isNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), nodeWithPod=node 3, podBeingKilled=pod9, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), node=node 3, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None), serviceTargetForPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), pod1=pod9\n",
      "StartPod_IF_Deployment_isNUll_Service_isNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), node=node 1, podStarted=pod9, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d87ded0>(name=GlobalVar-3567-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 1, pod1=pod5, pod2=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 3, pod1=pod2, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 3, pod1=pod5, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d5831d0>(name=Scheduler-3563-obj, value=None)\n",
      "PASSED\n",
      "tests/test_synt_affinity.py::test_2 WARNING: overflow on h^add! Costs clamped to 100000000\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  FALSE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "functional test\n",
      "<==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  FALSE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 4, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod3\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod2\n",
      "manually_initiate_killing_of_pod: pod_killed=pod8\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 5, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod5\n",
      "Add_node: node=node 5\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 2, pod1=pod5\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 2, podStarted=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceTargetForPod=test-service\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod5\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 3, podStarted=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 5, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 1, pod1=pod6, pod2=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 1, pod1=pod5, pod2=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 5, pod1=pod2, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 3, pod1=pod6, pod2=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 5, pod1=pod6, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 5, pod1=pod5, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "---  functional test : Error\n",
      "Plan:\n",
      "SelectNode: SelectedNode=node 4, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod3\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=4>(name=LogSparseInteger-169-obj-num-4, value=4), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 3, pod1=pod1\n",
      "manually_initiate_killing_of_pod: pod_killed=pod1\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), nodeWithPod=node 1, podBeingKilled=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod1\n",
      "SelectNode: SelectedNode=node 1, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod2\n",
      "manually_initiate_killing_of_pod: pod_killed=pod8\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 5, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod5\n",
      "Add_node: node=node 5\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod1\n",
      "SetDefaultCpuLimitForPodBeforeNodeAssignment: cpuCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 2, pod1=pod5\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 2, podStarted=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceTargetForPod=test-service\n",
      "SetDefaultMemLimitForPodBeforeNodeAssignment: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), memCapacity=<LogSparseInteger: value=8>(name=LogSparseInteger-171-obj-num-8, value=8), node=node 5, pod1=pod5\n",
      "manually_initiate_killing_of_pod: pod_killed=pod5\n",
      "KillPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), nodeWithPod=node 2, podBeingKilled=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceOfPod=test-service\n",
      "SelectNode: SelectedNode=node 3, globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), pod1=pod5\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 3, podStarted=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceTargetForPod=test-service\n",
      "StartPod_IF_Deployment_isNUll_Service_isNotNull_Daemonset_isNull: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), node=node 5, podStarted=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None), serviceTargetForPod=test-service\n",
      "SchedulerCleaned: globalVar=<guardctl.model.system.globals.GlobalVar object at 0x7fbc0d3bb090>(name=GlobalVar-7129-obj, value=None), scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 1, pod1=pod6, pod2=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 1, pod1=pod5, pod2=pod2, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 5, pod1=pod2, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 3, pod1=pod6, pod2=pod5, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 5, pod1=pod6, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "Not_at_same_node: node_of_pod2=node 5, pod1=pod5, pod2=pod1, scheduler=<guardctl.model.system.Scheduler.Scheduler object at 0x7fbc0d3adf10>(name=Scheduler-7125-obj, value=None)\n",
      "PASSED\n",
      "tests/test_synt_affinity.py::test_3 <==== Domain Object List =====>\n",
      "----------Pods---------------\n",
      "## Pod:pod1, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod2, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod3, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod4, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 1, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod5, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod6, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod7, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod8, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 2, Metadata_labels:[], hasService: TRUE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "## Pod:pod9, Status: Running, Priority_class: Normal-zero, CpuRequest: 2, MemRequest: 2, CpuLimit: -1, MemLimit: -1, ToNode: Null-Node, AtNode: node 3, Metadata_labels:[], hasService: FALSE, hasDeployment: FALSE, hasDaemonset: FALSE\n",
      "----------Nodes---------------\n",
      "## Node:node 1, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 2', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 2, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 8, CurrentFormalMemConsumption: 8, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 4, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 3', 'node 4', 'node 5']\n",
      "## Node:node 3, cpuCapacity: 4, memCapacity: 4, CurrentFormalCpuConsumption: 2, CurrentFormalMemConsumption: 2, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:Active, AmountOfActivePods: 1, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 4', 'node 5']\n",
      "## Node:node 4, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 5']\n",
      "## Node:node 5, cpuCapacity: 8, memCapacity: 8, CurrentFormalCpuConsumption: 0, CurrentFormalMemConsumption: 0, AmountOfPodsOverwhelmingMemLimits: 0, PodAmount: None, IsNull:FALSE, Status:New, AmountOfActivePods: 0, Searchable: TRUE, IsSearched:  FALSE, different_than:  ['node 1', 'node 2', 'node 3', 'node 4']\n",
      "----------Services---------------\n",
      "## Service: test-service, AmountOfActivePods: 4, Status: Started, Spec_selector: [], Pod_List: ['pod1', 'pod2', 'pod5', 'pod6'], IsSearched:  TRUE\n",
      "## Service: test-service2, AmountOfActivePods: 2, Status: Started, Spec_selector: [], Pod_List: ['pod7', 'pod8'], IsSearched:  FALSE\n",
      "----------PriorityClasses---------------\n",
      "## PriorityClass: high-prio-test 10\n",
      "----------Shedulers---------------\n",
      "## Sheduler: Changed PodList: [] QueueLength: 0\n",
      "----------Deployments------------\n",
      "----------DaemonSets------------\n",
      "----------ReplicaSets------------\n",
      "----------GlobalVar------------\n",
      "['', 'is_service_disrupted', 'FALSE', 'is_deployment_disrupted', 'FALSE', 'is_daemonset_disrupted', 'FALSE', 'is_node_disrupted', 'FALSE']\n",
      "FAILED\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "____________________________________ test_3 ____________________________________\n",
      "\n",
      "    def test_3():\n",
      "        # Initialize scheduler, globalvar\n",
      "        k = KubernetesCluster()\n",
      "        scheduler = next(filter(lambda x: isinstance(x, Scheduler), k.state_objects))\n",
      "        # initial node state\n",
      "        i = 0\n",
      "        j = 0\n",
      "        nodes = []\n",
      "        pods = []\n",
      "\n",
      "        # Service to detecte eviction\n",
      "        s1 = Service()\n",
      "        s1.metadata_name = \"test-service\"\n",
      "        s1.amountOfActivePods = 0\n",
      "        s1.antiaffinity = True\n",
      "        s1.targetAmountOfPodsOnDifferentNodes = 3\n",
      "        s1.isSearched = True\n",
      "\n",
      "        s2 = Service()\n",
      "        s2.metadata_name = \"test-service2\"\n",
      "        s2.amountOfActivePods = 0\n",
      "        # create Deploymnent that we're going to detect failure of...\n",
      "        d = Deployment()\n",
      "        d.spec_replicas = 2\n",
      "        node_item = Node()\n",
      "        node_item.metadata_name = \"node 1\"\n",
      "        node_item.cpuCapacity = 8\n",
      "        node_item.memCapacity = 8\n",
      "        node_item.isNull = False\n",
      "        node_item.status = STATUS_NODE[\"Active\"]\n",
      "        nodes.append(node_item)\n",
      "\n",
      "        pod = build_running_pod_with_d(1,2,2,node_item,None,None,s1,pods)\n",
      "        pod = build_running_pod_with_d(2,2,2,node_item,None,None,s1,pods)\n",
      "        pod = build_running_pod_with_d(3,2,2,node_item,None,None,None,pods)\n",
      "        pod = build_running_pod_with_d(4,2,2,node_item,None,None,None,pods)\n",
      "\n",
      "        node_item = Node()\n",
      "        node_item.metadata_name = \"node 2\"\n",
      "        node_item.cpuCapacity = 8\n",
      "        node_item.memCapacity = 8\n",
      "        node_item.isNull = False\n",
      "        node_item.status = STATUS_NODE[\"Active\"]\n",
      "        nodes.append(node_item)\n",
      "\n",
      "        pod = build_running_pod_with_d(5,2,2,node_item,None,None,s1,pods)\n",
      "        pod = build_running_pod_with_d(6,2,2,node_item,None,None,s1,pods)\n",
      "        pod = build_running_pod_with_d(7,2,2,node_item,None,None,s2,pods)\n",
      "        pod = build_running_pod_with_d(8,2,2,node_item,None,None,s2,pods)\n",
      "\n",
      "        node_item = Node()\n",
      "        node_item.metadata_name = \"node 3\"\n",
      "        node_item.cpuCapacity = 4\n",
      "        node_item.memCapacity = 4\n",
      "        node_item.isNull = False\n",
      "        node_item.status = STATUS_NODE[\"Active\"]\n",
      "        nodes.append(node_item)\n",
      "\n",
      "        pod = build_running_pod_with_d(9,2,2,node_item,None,None,None,pods)\n",
      "\n",
      "\n",
      "        node_item = Node()\n",
      "        node_item.metadata_name = \"node 4\"\n",
      "        node_item.cpuCapacity = 8\n",
      "        node_item.memCapacity = 8\n",
      "        node_item.isNull = False\n",
      "        node_item.status = STATUS_NODE[\"New\"]\n",
      "        nodes.append(node_item)\n",
      "\n",
      "\n",
      "        node_item = Node()\n",
      "        node_item.metadata_name = \"node 5\"\n",
      "        node_item.cpuCapacity = 8\n",
      "        node_item.memCapacity = 8\n",
      "        node_item.isNull = False\n",
      "        node_item.status = STATUS_NODE[\"New\"]\n",
      "        nodes.append(node_item)\n",
      "\n",
      "        for node in nodes:\n",
      "            for node2 in nodes:\n",
      "                if node != node2:\n",
      "                    node2.different_than.add(node)\n",
      "\n",
      "        # priority for pod-to-evict\n",
      "        pc = PriorityClass()\n",
      "        pc.priority = 10\n",
      "        pc.metadata_name = \"high-prio-test\"\n",
      "\n",
      "\n",
      "        k.state_objects.extend(nodes)\n",
      "        k.state_objects.extend(pods)\n",
      "        k.state_objects.extend([pc, s1, s2 ])\n",
      "        create_objects = []\n",
      "        k._build_state()\n",
      "        globalVar = next(filter(lambda x: isinstance(x, GlobalVar), k.state_objects))\n",
      "        scheduler = next(filter(lambda x: isinstance(x, Scheduler), k.state_objects))\n",
      "        class Antiaffinity_prefered_k1(Antiaffinity_prefered):\n",
      "            pass\n",
      "            # def goal(self):\n",
      "                # assert pods[0] in pods[1].not_on_same_node\n",
      "                # assert pods[0] in pods[4].not_on_same_node\n",
      "                # assert pods[1] in pods[4].not_on_same_node\n",
      "                # assert pods[0] in pods[5].not_on_same_node\n",
      "                # assert pods[1] in pods[5].not_on_same_node\n",
      "                # assert pods[4] in pods[5].not_on_same_node\n",
      "\n",
      "        p = Antiaffinity_prefered_k1(k.state_objects)\n",
      "        Antiaffinity_prefered_k1.__name__ = inspect.stack()[0].function\n",
      "        assert_conditions = [\"manually_initiate_killing_of_podt\",\\\n",
      "                            \"Not_at_same_node\",\\\n",
      "                            \"Add_node\"]\n",
      "        not_assert_conditions = []\n",
      "        print_objects(k.state_objects)\n",
      "        test_case = StateSet()\n",
      "        test_case.scheduler = scheduler\n",
      "        test_case.globalVar = globalVar\n",
      "        test_case.pods = pods\n",
      "        test_case.nodes = nodes\n",
      "        services = [s1,s2]\n",
      "        test_case.services = services\n",
      ">       assert_brake = checks_assert_conditions_in_one_mode(k,p,assert_conditions,not_assert_conditions,\"functional test\", DEBUG_MODE)\n",
      "\n",
      "tests/test_synt_affinity.py:440:\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "tests/libs_for_tests.py:584: in checks_assert_conditions_in_one_mode\n",
      "    p.run(timeout=9000)\n",
      "guardctl/misc/problem.py:58: in run\n",
      "    sessionName=sessionName\n",
      ".tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg/poodle/schedule.py:195: in schedule\n",
      "    p = _create_problem(methods, space, exit, goal, sessionName)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "methods = [<bound method KubernetesModel.Evict_and_replace_less_prioritized_pod_when_target_node_is_defined_byCPU of <test_synt_...e_node_outage_searched of <test_synt_affinity.test_3.<locals>.Antiaffinity_prefered_k1 object at 0x7fbc0ccb9890>>, ...]\n",
      "space = [<LogSparseInteger: value=0>, <LogSparseInteger: value=1>, <LogSparseInteger: value=2>, <LogSparseInteger: value=3>, <LogSparseInteger: value=4>, <guardctl.model.kinds.Node.Node object at 0x7fbc0ccdef10>, ...]\n",
      "exit = None, goal = [None], sessionName = 'test_3'\n",
      "\n",
      "    def _create_problem(methods, space, exit=None, goal=None, sessionName=None):\n",
      "        \"\"\"schedule methods within variables space space with exit method exit or goal goal\"\"\"\n",
      "        # 1. for every variable in space,\n",
      "        #    - create a full :predicates description of the object\n",
      "        #    - create init objects\n",
      "        # 2. for every method, add them to planning problem\n",
      "        if not methods: raise ValueError(\"At least one method is required to start scheduling\")\n",
      "        assert isinstance(space, list)\n",
      "        global _selector_out\n",
      "        _selector_out = None\n",
      "        # stringFactory.reset() # TODO: need to implement GC for string factory!!!\n",
      "\n",
      "        class XSProblem(Problem):\n",
      "            pass\n",
      "\n",
      "        if sessionName: XSProblem.__name__ = sessionName\n",
      "        p = XSProblem()\n",
      "        p.objectList = [x for x in space if isinstance(x, Object)]\n",
      "\n",
      "        l_collected_predicates = set()\n",
      "        l_collected_objects = collections.defaultdict(list)\n",
      "        l_collected_classes = set()\n",
      "        l_collected_facts = set()\n",
      "        p.gen_hashnums()\n",
      "        l_collected_goal = []\n",
      "        if goal:\n",
      "            _reset_state()\n",
      "            _compilation_enable()\n",
      "            try:\n",
      "                goal = Select(goal())\n",
      "            except Exception as e:\n",
      "                et, ei, tb = sys.exc_info()\n",
      "                _compilation_enable(False)\n",
      "                raise ei.with_traceback(tb)\n",
      "            if not type(goal) == list:\n",
      "                goal = [goal]\n",
      "            s_collected_goal = set()\n",
      "            for g in goal:\n",
      ">               for ph in g._parse_history:\n",
      "E               AttributeError: 'NoneType' object has no attribute '_parse_history'\n",
      "\n",
      ".tox/poodledev/lib/python3.7/site-packages/poodle-0.1.10-py3.7.egg/poodle/schedule.py:104: AttributeError\n",
      "========================== slowest 100 test durations ==========================\n",
      "51.09s call     tests/test_synt_affinity.py::test_2\n",
      "41.02s call     tests/test_synt_affinity.py::test_1\n",
      "0.68s call     tests/test_synt_affinity.py::test_3\n",
      "0.00s setup    tests/test_synt_affinity.py\n",
      "0.00s teardown tests/test_synt_affinity.py::test_2\n",
      "0.00s teardown tests/test_synt_affinity.py::test_1\n",
      "0.00s call     tests/test_synt_affinity.py\n",
      "0.00s teardown tests/test_synt_affinity.py::test_3\n",
      "0.00s setup    tests/test_synt_affinity.py::test_1\n",
      "0.00s setup    tests/test_synt_affinity.py::test_2\n",
      "0.00s setup    tests/test_synt_affinity.py::test_3\n",
      "0.00s teardown tests/test_synt_affinity.py\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "============== 1 failed, 3 passed, 1 warning in 98.06s (0:01:38) ===============\n",
      "ERROR: InvocationError for command /home/vasily/artem/kubectl-val/.tox/poodledev/bin/python -m pytest -vv --durations=100 --disable-pytest-warnings --pylint --pylint-jobs=4 --pylint-error-types=EF -s -x tests/test_synt_affinity.py (exited with code 1)\n",
      "___________________________________ summary ____________________________________\n",
      "ERROR:   poodledev: commands failed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-c3bd5d0b59dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'runrealcmd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'POODLE_LIN_COUNT=12  POODLE_ASTAR_WEIGHT=20  PYTHON=pypy  POODLE_SOLVER_URL=http://localhost:10757  tox -e poodledev  -- -s -x tests/test_synt_affinity.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2305\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2306\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2307\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2308\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-09ae4cb52495>\u001b[0m in \u001b[0;36mrunrealcmd\u001b[0;34m(command)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrunrealcmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_fds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%runrealcmd POODLE_LIN_COUNT=12 \\\n",
    "POODLE_ASTAR_WEIGHT=20 \\\n",
    "PYTHON=pypy \\\n",
    "POODLE_SOLVER_URL=http://localhost:10757 \\\n",
    "tox -e poodledev \\\n",
    "-- -s -x tests/test_synt_affinity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

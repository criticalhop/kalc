apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"creationTimestamp":"2020-01-15T21:57:26Z","generateName":"redis-master-57fc67768d-","labels":{"app":"redis","pod-template-hash":"57fc67768d","role":"master","tier":"backend"},"name":"redis-master-57fc67768d-2xc2q-1-kalcmoved-1579146336","namespace":"default","resourceVersion":"62208818","selfLink":"/api/v1/namespaces/default/pods/redis-master-57fc67768d-2xc2q-1","uid":"049f76df-37e2-11ea-a0a0-42010a80004b"},"spec":{"containers":[{"image":"k8s.gcr.io/redis:e2e","imagePullPolicy":"IfNotPresent","name":"master","resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"default-token-kchbk","readOnly":true}]}],"dnsPolicy":"ClusterFirst","nodeName":"gke-tesg1-default-pool-ff7a1295-zpc5","priority":0,"restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"serviceAccount":"default","serviceAccountName":"default","terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":300},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":300}],"volumes":[{"name":"default-token-kchbk","secret":{"defaultMode":420,"secretName":"default-token-kchbk"}}]}}
    creationTimestamp: "2020-01-16T03:48:01Z"
    generateName: redis-master-57fc67768d-
    labels:
      app: redis
      pod-template-hash: 57fc67768d
      role: master
      tier: backend
    name: redis-master-57fc67768d-2xc2q-1-kalcmoved-1579146336
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-master-57fc67768d
      uid: 0258e998-3813-11ea-a0a0-42010a80004b
    resourceVersion: "63671013"
    selfLink: /api/v1/namespaces/default/pods/redis-master-57fc67768d-2xc2q-1-kalcmoved-1579146336
    uid: fe12e706-3812-11ea-a0a0-42010a80004b
  spec:
    containers:
    - image: k8s.gcr.io/redis:e2e
      imagePullPolicy: IfNotPresent
      name: master
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-zpc5
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-01-16T03:48:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-01-16T03:48:02Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-01-16T03:48:02Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-01-16T03:48:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e2f587237f44d9372520e82bde282b7af224bfa225636a4b9785882922deb223
      image: k8s.gcr.io/redis:e2e
      imageID: docker-pullable://k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      lastState: {}
      name: master
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-01-16T03:48:02Z"
    hostIP: 10.128.0.16
    phase: Running
    podIP: 10.8.3.52
    qosClass: Burstable
    startTime: "2020-01-16T03:48:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"generateName":"redis-master-57fc67768d-","labels":{"app":"redis","pod-template-hash":"57fc67768d","role":"master","tier":"backend"},"name":"redis-master-57fc67768d-2xc2q-2","namespace":"default"},"spec":{"containers":[{"image":"k8s.gcr.io/redis:e2e","imagePullPolicy":"IfNotPresent","name":"master","resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"nodeName":"gke-tesg1-default-pool-ff7a1295-7kwg"}}
    creationTimestamp: "2020-01-15T21:57:26Z"
    generateName: redis-master-57fc67768d-
    labels:
      app: redis
      pod-template-hash: 57fc67768d
      role: master
      tier: backend
    name: redis-master-57fc67768d-2xc2q-2
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-master-57fc67768d
      uid: 0258e998-3813-11ea-a0a0-42010a80004b
    resourceVersion: "63701978"
    selfLink: /api/v1/namespaces/default/pods/redis-master-57fc67768d-2xc2q-2
    uid: 04a69f5b-37e2-11ea-a0a0-42010a80004b
  spec:
    containers:
    - image: k8s.gcr.io/redis:e2e
      imagePullPolicy: IfNotPresent
      name: master
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e989eca4401bfa0bbac93234beb2cc11d999dd2a1b971816d1924deda22d5434
      image: k8s.gcr.io/redis:e2e
      imageID: docker-pullable://k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      lastState: {}
      name: master
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-01-15T21:57:28Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.8.1.156
    qosClass: Burstable
    startTime: "2020-01-15T21:57:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"generateName":"redis-master-57fc67768d-","labels":{"app":"redis","pod-template-hash":"57fc67768d","role":"master","tier":"backend"},"name":"redis-master-57fc67768d-2xc2q-3","namespace":"default"},"spec":{"containers":[{"image":"k8s.gcr.io/redis:e2e","imagePullPolicy":"IfNotPresent","name":"master","resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"nodeName":"gke-tesg1-default-pool-ff7a1295-7kwg"}}
    creationTimestamp: "2020-01-15T21:57:26Z"
    generateName: redis-master-57fc67768d-
    labels:
      app: redis
      pod-template-hash: 57fc67768d
      role: master
      tier: backend
    name: redis-master-57fc67768d-2xc2q-3
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-master-57fc67768d
      uid: 0258e998-3813-11ea-a0a0-42010a80004b
    resourceVersion: "63701985"
    selfLink: /api/v1/namespaces/default/pods/redis-master-57fc67768d-2xc2q-3
    uid: 04ae9610-37e2-11ea-a0a0-42010a80004b
  spec:
    containers:
    - image: k8s.gcr.io/redis:e2e
      imagePullPolicy: IfNotPresent
      name: master
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T21:57:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://afda4888aaf8651d7de5f0afb25f4c5b0c3a72e53d76ae7c1bc47f18d939ae17
      image: k8s.gcr.io/redis:e2e
      imageID: docker-pullable://k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      lastState: {}
      name: master
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-01-15T21:57:28Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.8.1.155
    qosClass: Burstable
    startTime: "2020-01-15T21:57:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"generateName":"redis-master-57fc67768d-","labels":{"app":"redis","pod-template-hash":"57fc67768d","role":"master","tier":"backend"},"name":"redis-master-57fc67768d-2xc2q-4","namespace":"default"},"spec":{"containers":[{"image":"k8s.gcr.io/redis:e2e","imagePullPolicy":"IfNotPresent","name":"master","resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"nodeName":"gke-tesg1-default-pool-ff7a1295-7kwg"}}
    creationTimestamp: "2020-01-15T21:57:26Z"
    generateName: redis-master-57fc67768d-
    labels:
      app: redis
      pod-template-hash: 57fc67768d
      role: master
      tier: backend
    name: redis-master-57fc67768d-2xc2q-4
    namespace: default
    resourceVersion: "62130203"
    selfLink: /api/v1/namespaces/default/pods/redis-master-57fc67768d-2xc2q-4
    uid: 04b4c9a3-37e2-11ea-a0a0-42010a80004b
  spec:
    containers:
    - image: k8s.gcr.io/redis:e2e
      imagePullPolicy: IfNotPresent
      name: master
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    message: 'Pod Node didn''t have enough resource: cpu, requested: 100, used: 869,
      capacity: 940'
    phase: Failed
    reason: OutOfcpu
    startTime: "2020-01-15T21:57:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2020-01-15T22:00:11Z"
    generateName: redis-master-57fc67768d-
    labels:
      app: redis
      pod-template-hash: 57fc67768d
      role: master
      tier: backend
    name: redis-master-57fc67768d-gb8cc
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-master-57fc67768d
      uid: 0258e998-3813-11ea-a0a0-42010a80004b
    resourceVersion: "63671165"
    selfLink: /api/v1/namespaces/default/pods/redis-master-57fc67768d-gb8cc
    uid: 66ee1cef-37e2-11ea-a0a0-42010a80004b
  spec:
    containers:
    - image: k8s.gcr.io/redis:e2e
      imagePullPolicy: IfNotPresent
      name: master
      ports:
      - containerPort: 6379
        protocol: TCP
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T22:00:11Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T22:00:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T22:00:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-01-15T22:00:11Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e6d39aedbbdd674f71c099095e51b909bd84aaff1c0dd0575f7d87c81299321c
      image: k8s.gcr.io/redis:e2e
      imageID: docker-pullable://k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      lastState: {}
      name: master
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-01-15T22:00:12Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.8.2.158
    qosClass: Burstable
    startTime: "2020-01-15T22:00:11Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2020-01-26T21:54:13Z"
    generateName: redis-master-evict-fd97bd94b-
    labels:
      app: redis-evict
      pod-template-hash: fd97bd94b
      role: master
      tier: backend
    name: redis-master-evict-fd97bd94b-htxzs
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: redis-master-evict-fd97bd94b
      uid: 6ef1dd30-cf5a-11e9-98f3-42010a8000c4
    resourceVersion: "66995075"
    selfLink: /api/v1/namespaces/default/pods/redis-master-evict-fd97bd94b-htxzs
    uid: 63d69054-4086-11ea-a0a0-42010a80004b
  spec:
    containers:
    - image: k8s.gcr.io/redis:e2e
      imagePullPolicy: IfNotPresent
      name: master
      ports:
      - containerPort: 6379
        protocol: TCP
      resources:
        requests:
          cpu: 500m
          memory: 1700280Ki
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-01-27T21:59:16Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-01-27T21:59:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-01-27T21:59:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-01-27T21:59:16Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://410c5d4d17f096261b78b75eb9d027346d63e8ad03ff4e4659af3aa43a09ee66
      image: k8s.gcr.io/redis:e2e
      imageID: docker-pullable://k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      lastState: {}
      name: master
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-01-27T21:59:38Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.8.0.247
    qosClass: Burstable
    startTime: "2020-01-27T21:59:16Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container
        scheduler-pod-officer'
    creationTimestamp: "2019-12-27T17:37:36Z"
    generateName: scheduler-pod-officer-6ccf79f787-
    labels:
      app: scheduler-pod-officer
      pod-template-hash: 6ccf79f787
    name: scheduler-pod-officer-6ccf79f787-pzm5r
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: scheduler-pod-officer-6ccf79f787
      uid: 91c56762-28cf-11ea-a0a0-42010a80004b
    resourceVersion: "63671017"
    selfLink: /api/v1/namespaces/default/pods/scheduler-pod-officer-6ccf79f787-pzm5r
    uid: 9268e50d-28cf-11ea-a0a0-42010a80004b
  spec:
    containers:
    - image: gcr.io/closercriticalhop/my-kube-scheduler:1.0
      imagePullPolicy: Always
      name: scheduler-pod-officer
      resources:
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: scheduler-pod-officer-token-gkphl
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-zpc5
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: scheduler-pod-officer
    serviceAccountName: scheduler-pod-officer
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: scheduler-pod-officer-token-gkphl
      secret:
        defaultMode: 420
        secretName: scheduler-pod-officer-token-gkphl
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-27T17:37:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-27T17:38:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-27T17:38:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-27T17:37:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://b612932fb0a5ac26b244e9c5e1004e27ec5896569727de2b1cc76574699a580f
      image: gcr.io/closercriticalhop/my-kube-scheduler:1.0
      imageID: docker-pullable://gcr.io/closercriticalhop/my-kube-scheduler@sha256:99e537dc22449f3a79a13bf2a1d5d5e5a97176f520987111e76cebb0bc568ca6
      lastState: {}
      name: scheduler-pod-officer
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-27T17:38:05Z"
    hostIP: 10.128.0.16
    phase: Running
    podIP: 10.8.3.49
    qosClass: Burstable
    startTime: "2019-12-27T17:37:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container
        sleep'
    creationTimestamp: "2020-01-26T21:54:13Z"
    generateName: sleep-default-sched-78f7b9dffd-
    labels:
      app: sleep-default-sched
      ggg: ggg
      hhh: hhh1
      pod-template-hash: 78f7b9dffd
    name: sleep-default-sched-78f7b9dffd-dc56t
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sleep-default-sched-78f7b9dffd
      uid: 2ed80f72-2a77-11ea-a0a0-42010a80004b
    resourceVersion: "65677292"
    selfLink: /api/v1/namespaces/default/pods/sleep-default-sched-78f7b9dffd-dc56t
    uid: 63ece130-4086-11ea-a0a0-42010a80004b
  spec:
    containers:
    - command:
      - /bin/sleep
      - infinity
      image: tutum/curl
      imagePullPolicy: IfNotPresent
      name: sleep
      resources:
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-01-26T21:54:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-01-26T21:54:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-01-26T21:54:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-01-26T21:54:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://332c084950de5b450947af387719fe122334ac285705d362f19ee89b0ad02141
      image: tutum/curl:latest
      imageID: docker-pullable://tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f
      lastState: {}
      name: sleep
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2020-01-26T21:54:15Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.8.2.218
    qosClass: Burstable
    startTime: "2020-01-26T21:54:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu request for container
        sleep'
    creationTimestamp: "2019-12-29T20:09:56Z"
    generateName: sleep-default-sched-78f7b9dffd-
    labels:
      app: sleep-default-sched
      ggg: ggg
      hhh: hhh1
      pod-template-hash: 78f7b9dffd
    name: sleep-default-sched-78f7b9dffd-hwr5n
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: sleep-default-sched-78f7b9dffd
      uid: 2ed80f72-2a77-11ea-a0a0-42010a80004b
    resourceVersion: "63671049"
    selfLink: /api/v1/namespaces/default/pods/sleep-default-sched-78f7b9dffd-hwr5n
    uid: 2edc55d8-2a77-11ea-a0a0-42010a80004b
  spec:
    containers:
    - command:
      - /bin/sleep
      - infinity
      image: tutum/curl
      imagePullPolicy: IfNotPresent
      name: sleep
      resources:
        requests:
          cpu: 100m
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-kchbk
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-kchbk
      secret:
        defaultMode: 420
        secretName: default-token-kchbk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-12-29T20:09:56Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-12-29T20:09:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-12-29T20:09:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-12-29T20:09:56Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://cee513992d8222736c3c90d40fd41255c219c21fd1f1be51217a95f79841552c
      image: tutum/curl:latest
      imageID: docker-pullable://tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f
      lastState: {}
      name: sleep
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-12-29T20:09:57Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.8.0.103
    qosClass: Burstable
    startTime: "2019-12-29T20:09:56Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:22:15Z","generateName":"event-exporter-v0.2.4-5f7d5d7dd4-","labels":{"k8s-app":"event-exporter","pod-template-hash":"5f7d5d7dd4","version":"v0.2.4"},"name":"event-exporter-v0.2.4-5f7d5d7dd4-7mfz5","namespace":"kube-system","ownerReferences":[{"apiVersion":"apps/v1","blockOwnerDeletion":true,"controller":true,"kind":"ReplicaSet","name":"event-exporter-v0.2.4-5f7d5d7dd4","uid":"868e50cc-9217-11e9-a049-42010a8000d3"}],"resourceVersion":"766","selfLink":"/api/v1/namespaces/kube-system/pods/event-exporter-v0.2.4-5f7d5d7dd4-7mfz5","uid":"868f7ab9-9217-11e9-a049-42010a8000d3"},"spec":{"containers":[{"command":["/event-exporter","-sink-opts=-stackdriver-resource-model=old"],"image":"k8s.gcr.io/event-exporter:v0.2.4","imagePullPolicy":"IfNotPresent","name":"event-exporter","resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"event-exporter-sa-token-bnkrr","readOnly":true}]},{"command":["/monitor","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","imagePullPolicy":"IfNotPresent","name":"prometheus-to-sd-exporter","resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"event-exporter-sa-token-bnkrr","readOnly":true}]}],"dnsPolicy":"ClusterFirst","nodeName":"gke-tesg1-default-pool-ff7a1295-z7lx","priority":0,"restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"serviceAccount":"event-exporter-sa","serviceAccountName":"event-exporter-sa","terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":300},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":300}],"volumes":[{"hostPath":{"path":"/etc/ssl/certs","type":""},"name":"ssl-certs"},{"name":"event-exporter-sa-token-bnkrr","secret":{"defaultMode":420,"secretName":"event-exporter-sa-token-bnkrr"}}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:26Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:51Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:51Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:26Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://a16d968771a49b28236bf05c103a9f5692560e3b7c69a210b2a38792259c091c","image":"k8s.gcr.io/event-exporter:v0.2.4","imageID":"docker-pullable://k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc","lastState":{},"name":"event-exporter","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:44Z"}}},{"containerID":"docker://89f5b754d796b45fde232a46d1fc9f5908504c442b1a0b31a78ef60c7fbc6f45","image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","imageID":"docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e","lastState":{},"name":"prometheus-to-sd-exporter","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:47Z"}}}],"hostIP":"10.128.0.13","phase":"Running","podIP":"10.8.0.4","qosClass":"BestEffort","startTime":"2019-06-18T22:22:26Z"}}
    creationTimestamp: "2019-06-18T22:22:15Z"
    generateName: event-exporter-v0.2.4-5f7d5d7dd4-
    labels:
      k8s-app: event-exporter
      pod-template-hash: 5f7d5d7dd4
      version: v0.2.4
    name: event-exporter-v0.2.4-5f7d5d7dd4-7mfz5
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: event-exporter-v0.2.4-5f7d5d7dd4
      uid: 868e50cc-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671038"
    selfLink: /api/v1/namespaces/kube-system/pods/event-exporter-v0.2.4-5f7d5d7dd4-7mfz5
    uid: 868f7ab9-9217-11e9-a049-42010a8000d3
  spec:
    containers:
    - command:
      - /event-exporter
      - -sink-opts=-stackdriver-resource-model=old
      image: k8s.gcr.io/event-exporter:v0.2.4
      imagePullPolicy: IfNotPresent
      name: event-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: event-exporter-sa-token-bnkrr
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: event-exporter-sa-token-bnkrr
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: event-exporter-sa
    serviceAccountName: event-exporter-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: ssl-certs
    - name: event-exporter-sa-token-bnkrr
      secret:
        defaultMode: 420
        secretName: event-exporter-sa-token-bnkrr
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:51Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:51Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a16d968771a49b28236bf05c103a9f5692560e3b7c69a210b2a38792259c091c
      image: k8s.gcr.io/event-exporter:v0.2.4
      imageID: docker-pullable://k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc
      lastState: {}
      name: event-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:44Z"
    - containerID: docker://89f5b754d796b45fde232a46d1fc9f5908504c442b1a0b31a78ef60c7fbc6f45
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:47Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.8.0.4
    qosClass: BestEffort
    startTime: "2019-06-18T22:22:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-10-13T12:28:24Z"
    generateName: fluentd-gcp-scaler-5b5ff6f8bd-
    labels:
      k8s-app: fluentd-gcp-scaler
      pod-template-hash: 5b5ff6f8bd
    name: fluentd-gcp-scaler-5b5ff6f8bd-8ndwb
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: fluentd-gcp-scaler-5b5ff6f8bd
      uid: f325a5bd-edb4-11e9-9caf-42010a800177
    resourceVersion: "63701968"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-scaler-5b5ff6f8bd-8ndwb
    uid: f32691e6-edb4-11e9-9caf-42010a800177
  spec:
    containers:
    - command:
      - /scaler.sh
      - --ds-name=fluentd-gcp-v3.2.0
      - --scaling-policy=fluentd-gcp-scaling-policy
      env:
      - name: CPU_REQUEST
        value: 100m
      - name: MEMORY_REQUEST
        value: 200Mi
      - name: CPU_LIMIT
        value: 1000m
      - name: MEMORY_LIMIT
        value: 500Mi
      image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
      imagePullPolicy: IfNotPresent
      name: fluentd-gcp-scaler
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-scaler-token-tb8bz
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp-scaler
    serviceAccountName: fluentd-gcp-scaler
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: fluentd-gcp-scaler-token-tb8bz
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-scaler-token-tb8bz
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:23Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:23Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://a33132b536e8cc0f3b962e5b390b6735c7ea539531536b2571d13d4ab6b4c613
      image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
      imageID: docker-pullable://k8s.gcr.io/fluentd-gcp-scaler@sha256:4f28f10fb89506768910b858f7a18ffb996824a16d70d5ac895e49687df9ff58
      lastState:
        terminated:
          containerID: docker://a7c704e660305e3f0155803fff0cbc33da5709ea0bfd807c7ba401d49fe83918
          exitCode: 255
          finishedAt: "2019-11-28T15:56:33Z"
          reason: Error
          startedAt: "2019-10-13T12:28:29Z"
      name: fluentd-gcp-scaler
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:57:23Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.8.1.27
    qosClass: BestEffort
    startTime: "2019-10-13T12:28:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-10-13T12:28:36Z"
    generateName: fluentd-gcp-v3.2.0-
    labels:
      controller-revision-hash: 68645bfbcf
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "3"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0-8b4np
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-gcp-v3.2.0
      uid: 86ae80c3-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671034"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-v3.2.0-8b4np
    uid: faade544-edb4-11e9-9caf-42010a800177
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-z7lx
    containers:
    - env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: STACKDRIVER_METADATA_AGENT_URL
        value: http://$(NODE_NAME):8799
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
              exit 1;
            fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]]; then
              rm -rf /var/log/fluentd-buffers;
              exit 1;
            fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]]; then
              exit 1;
            fi;
        failureThreshold: 3
        initialDelaySeconds: 600
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: fluentd-gcp
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /etc/google-fluentd/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    dnsPolicy: Default
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    nodeSelector:
      beta.kubernetes.io/fluentd-ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp
    serviceAccountName: fluentd-gcp
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - configMap:
        defaultMode: 420
        name: fluentd-gcp-config-old-v1.2.5
      name: config-volume
    - name: fluentd-gcp-token-vxg67
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-token-vxg67
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:39Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:54Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:54Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:39Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://23a34b905681c8465c12282d04a66f2acbe673744b950820e3d6a5b52f92e16d
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imageID: docker-pullable://gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      lastState: {}
      name: fluentd-gcp
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:28:53Z"
    - containerID: docker://afe9df33ad4ba01eaf2bb6d917d6e305a1a249cf6719ea56f2165fe7164db82f
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:28:54Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.128.0.13
    qosClass: Burstable
    startTime: "2019-10-13T12:28:39Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-10-13T12:28:18Z"
    generateName: fluentd-gcp-v3.2.0-
    labels:
      controller-revision-hash: 68645bfbcf
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "3"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0-94b5t
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-gcp-v3.2.0
      uid: 86ae80c3-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671118"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-v3.2.0-94b5t
    uid: f01d66d4-edb4-11e9-9caf-42010a800177
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-dmtd
    containers:
    - env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: STACKDRIVER_METADATA_AGENT_URL
        value: http://$(NODE_NAME):8799
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
              exit 1;
            fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]]; then
              rm -rf /var/log/fluentd-buffers;
              exit 1;
            fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]]; then
              exit 1;
            fi;
        failureThreshold: 3
        initialDelaySeconds: 600
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: fluentd-gcp
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /etc/google-fluentd/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    dnsPolicy: Default
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-dmtd
    nodeSelector:
      beta.kubernetes.io/fluentd-ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp
    serviceAccountName: fluentd-gcp
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - configMap:
        defaultMode: 420
        name: fluentd-gcp-config-old-v1.2.5
      name: config-volume
    - name: fluentd-gcp-token-vxg67
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-token-vxg67
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:19Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:31Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:31Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:19Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://4260d6578e7c3e24b80a3eed5df9d26912fa8165a05c63fbd8ecbe25e5f139cd
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imageID: docker-pullable://gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      lastState: {}
      name: fluentd-gcp
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:28:30Z"
    - containerID: docker://33296e2a28fbb02e8069a9228e24950d46dc8319915e8c6ddf8af228f0ae4371
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:28:31Z"
    hostIP: 10.128.0.17
    phase: Running
    podIP: 10.128.0.17
    qosClass: Burstable
    startTime: "2019-10-13T12:28:19Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-10-13T12:29:00Z"
    generateName: fluentd-gcp-v3.2.0-
    labels:
      controller-revision-hash: 68645bfbcf
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "3"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0-cn5sx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-gcp-v3.2.0
      uid: 86ae80c3-9217-11e9-a049-42010a8000d3
    resourceVersion: "63701973"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-v3.2.0-cn5sx
    uid: 089920dd-edb5-11e9-9caf-42010a800177
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-7kwg
    containers:
    - env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: STACKDRIVER_METADATA_AGENT_URL
        value: http://$(NODE_NAME):8799
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
              exit 1;
            fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]]; then
              rm -rf /var/log/fluentd-buffers;
              exit 1;
            fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]]; then
              exit 1;
            fi;
        failureThreshold: 3
        initialDelaySeconds: 600
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: fluentd-gcp
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /etc/google-fluentd/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    dnsPolicy: Default
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    nodeSelector:
      beta.kubernetes.io/fluentd-ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp
    serviceAccountName: fluentd-gcp
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - configMap:
        defaultMode: 420
        name: fluentd-gcp-config-old-v1.2.5
      name: config-volume
    - name: fluentd-gcp-token-vxg67
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-token-vxg67
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:29:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:12Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:12Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:29:00Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5235a6328b99c3200eada25457faf80cfd9fbf16c494e403c684c6bc730d0e0f
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imageID: docker-pullable://gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      lastState:
        terminated:
          containerID: docker://c45982d2c41a134dabf7f4bf8730976f3678224470477c2da70ccf29e637150c
          exitCode: 255
          finishedAt: "2019-11-28T15:56:33Z"
          reason: Error
          startedAt: "2019-10-13T12:29:13Z"
      name: fluentd-gcp
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:57:11Z"
    - containerID: docker://e89b3eb545c5421c9ea690f96d7b4d0724e59f79afda921b11176b8579c8a641
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState:
        terminated:
          containerID: docker://ba763e2d5f88316809f26463f3b12d5d7e1779f3b417b403e1c7dd933fb4d210
          exitCode: 255
          finishedAt: "2019-11-28T15:56:33Z"
          reason: Error
          startedAt: "2019-10-13T12:29:13Z"
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:57:11Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.128.0.14
    qosClass: Burstable
    startTime: "2019-10-13T12:29:00Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-10-13T12:29:21Z"
    generateName: fluentd-gcp-v3.2.0-
    labels:
      controller-revision-hash: 68645bfbcf
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "3"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0-dfpmz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-gcp-v3.2.0
      uid: 86ae80c3-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671160"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-v3.2.0-dfpmz
    uid: 1533b53c-edb5-11e9-9caf-42010a800177
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-nvv4
    containers:
    - env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: STACKDRIVER_METADATA_AGENT_URL
        value: http://$(NODE_NAME):8799
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
              exit 1;
            fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]]; then
              rm -rf /var/log/fluentd-buffers;
              exit 1;
            fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]]; then
              exit 1;
            fi;
        failureThreshold: 3
        initialDelaySeconds: 600
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: fluentd-gcp
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /etc/google-fluentd/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    dnsPolicy: Default
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    nodeSelector:
      beta.kubernetes.io/fluentd-ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp
    serviceAccountName: fluentd-gcp
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - configMap:
        defaultMode: 420
        name: fluentd-gcp-config-old-v1.2.5
      name: config-volume
    - name: fluentd-gcp-token-vxg67
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-token-vxg67
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:20Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:20Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://dfb887a4e597df2a18c18afde89c35314f486c20a35d406c6b2f6b3eb77b3b30
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imageID: docker-pullable://gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      lastState: {}
      name: fluentd-gcp
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:30:19Z"
    - containerID: docker://1b429b1ae93d4c51eb83a0720f427f32633c8814cc7f3924edfae1ca6f393a3f
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:30:19Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.128.0.15
    qosClass: Burstable
    startTime: "2019-10-13T12:30:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-10-13T12:30:25Z"
    generateName: fluentd-gcp-v3.2.0-
    labels:
      controller-revision-hash: 68645bfbcf
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      pod-template-generation: "3"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0-jgbz8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: fluentd-gcp-v3.2.0
      uid: 86ae80c3-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671007"
    selfLink: /api/v1/namespaces/kube-system/pods/fluentd-gcp-v3.2.0-jgbz8
    uid: 3bb62b85-edb5-11e9-9caf-42010a800177
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-zpc5
    containers:
    - env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: STACKDRIVER_METADATA_AGENT_URL
        value: http://$(NODE_NAME):8799
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - |
            LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
              exit 1;
            fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]]; then
              rm -rf /var/log/fluentd-buffers;
              exit 1;
            fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]]; then
              exit 1;
            fi;
        failureThreshold: 3
        initialDelaySeconds: 600
        periodSeconds: 60
        successThreshold: 1
        timeoutSeconds: 1
      name: fluentd-gcp
      resources:
        limits:
          cpu: "1"
          memory: 500Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/docker/containers
        name: varlibdockercontainers
        readOnly: true
      - mountPath: /etc/google-fluentd/config.d
        name: config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    - command:
      - /monitor
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd-exporter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: fluentd-gcp-token-vxg67
        readOnly: true
    dnsPolicy: Default
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-zpc5
    nodeSelector:
      beta.kubernetes.io/fluentd-ds-ready: "true"
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: fluentd-gcp
    serviceAccountName: fluentd-gcp
    terminationGracePeriodSeconds: 60
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /var/lib/docker/containers
        type: ""
      name: varlibdockercontainers
    - configMap:
        defaultMode: 420
        name: fluentd-gcp-config-old-v1.2.5
      name: config-volume
    - name: fluentd-gcp-token-vxg67
      secret:
        defaultMode: 420
        secretName: fluentd-gcp-token-vxg67
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:41Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:41Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:30:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://666ebf1b8c87e54b3409911567a30a0e16f32dd254b5e2140564ce2eb5b83938
      image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      imageID: docker-pullable://gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      lastState: {}
      name: fluentd-gcp
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:30:40Z"
    - containerID: docker://98ee7c70ca76756b5f62ef767cebd078a986bc674afb893285527163ef430857
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prometheus-to-sd-exporter
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:30:41Z"
    hostIP: 10.128.0.16
    phase: Running
    podIP: 10.128.0.16
    qosClass: Burstable
    startTime: "2019-10-13T12:30:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-11-15T06:03:15Z"
    generateName: heapster-7f957cc846-
    labels:
      k8s-app: heapster
      pod-template-hash: 7f957cc846
      version: v1.7.0
    name: heapster-7f957cc846-drtl8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: heapster-7f957cc846
      uid: 9d3e62c2-076d-11ea-a0a0-42010a80004b
    resourceVersion: "63671043"
    selfLink: /api/v1/namespaces/kube-system/pods/heapster-7f957cc846-drtl8
    uid: 9d3fcfea-076d-11ea-a0a0-42010a80004b
  spec:
    containers:
    - command:
      - /heapster
      - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
      - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
      image: gke.gcr.io/heapster:v1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8082
          scheme: HTTP
        initialDelaySeconds: 180
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: heapster
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    - command:
      - /monitor
      - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prom-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    - command:
      - /pod_nanny
      - --config-dir=/etc/config
      - --cpu=10m
      - --extra-cpu=0.5m
      - --memory=100Mi
      - --extra-memory=4Mi
      - --threshold=5
      - --deployment=heapster-v1.7.0
      - --container=heapster
      - --poll-period=300000
      - --estimator=exponential
      - --minClusterSize=5
      env:
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/addon-resizer:1.8.3
      imagePullPolicy: IfNotPresent
      name: heapster-nanny
      resources:
        limits:
          cpu: 50m
          memory: 93360Ki
        requests:
          cpu: 50m
          memory: 93360Ki
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: heapster-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      supplementalGroups:
      - 65534
    serviceAccount: heapster
    serviceAccountName: heapster
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: heapster-config
      name: heapster-config-volume
    - name: heapster-token-jx6jf
      secret:
        defaultMode: 420
        secretName: heapster-token-jx6jf
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://de39b25155198076fdead5c4e1c68343f9a7720eaab402253efb399b390e3c33
      image: gke.gcr.io/heapster:v1.7.0
      imageID: docker-pullable://gke.gcr.io/heapster@sha256:a9c899ab08312640c779837e3345c8e0a8591b352cd421167f7c7af852d40393
      lastState: {}
      name: heapster
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:19Z"
    - containerID: docker://d6da2efd1e7cd90f56e3e7f501df691980ceb2c1452fcf04802623485328ec61
      image: k8s.gcr.io/addon-resizer:1.8.3
      imageID: docker-pullable://k8s.gcr.io/addon-resizer@sha256:07353f7b26327f0d933515a22b1de587b040d3d85c464ea299c1b9f242529326
      lastState: {}
      name: heapster-nanny
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:20Z"
    - containerID: docker://b4e26581838c0c596363cc7e3696bc337143699dd1e651785d70945fe7d9ce06
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prom-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:20Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.8.0.128
    qosClass: Burstable
    startTime: "2019-11-15T06:03:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"creationTimestamp":"2019-06-18T22:22:47Z","generateName":"heapster-v1.6.0-beta.1-68cdfd6769-","labels":{"k8s-app":"heapster","pod-template-hash":"68cdfd6769","version":"v1.6.0-beta.1"},"name":"heapster-v1.6.0-beta.1-68cdfd6769-jlmfs","namespace":"kube-system","ownerReferences":[{"apiVersion":"apps/v1","blockOwnerDeletion":true,"controller":true,"kind":"ReplicaSet","name":"heapster-v1.6.0-beta.1-68cdfd6769","uid":"99cfbb83-9217-11e9-a049-42010a8000d3"}],"resourceVersion":"759","selfLink":"/api/v1/namespaces/kube-system/pods/heapster-v1.6.0-beta.1-68cdfd6769-jlmfs","uid":"99d69b3a-9217-11e9-a049-42010a8000d3"},"spec":{"containers":[{"command":["/heapster","--source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id","--sink=stackdriver:?cluster_name=tesg1\u0026use_old_resources=true\u0026use_new_resources=false\u0026min_interval_sec=100\u0026batch_export_timeout_sec=110\u0026cluster_location=us-central1-a"],"image":"k8s.gcr.io/heapster-amd64:v1.6.0-beta.1","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":8082,"scheme":"HTTP"},"initialDelaySeconds":180,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"name":"heapster","resources":{"limits":{"cpu":"13m","memory":"120Mi"},"requests":{"cpu":"13m","memory":"120Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"heapster-token-jx6jf","readOnly":true}]},{"command":["/monitor","--source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","imagePullPolicy":"IfNotPresent","name":"prom-to-sd","resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"heapster-token-jx6jf","readOnly":true}]},{"command":["/pod_nanny","--config-dir=/etc/config","--cpu=10m","--extra-cpu=0.5m","--memory=100Mi","--extra-memory=4Mi","--threshold=5","--deployment=heapster-v1.6.0-beta.1","--container=heapster","--poll-period=300000","--estimator=exponential","--minClusterSize=5"],"env":[{"name":"MY_POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"MY_POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/addon-resizer:1.8.3","imagePullPolicy":"IfNotPresent","name":"heapster-nanny","resources":{"limits":{"cpu":"50m","memory":"92960Ki"},"requests":{"cpu":"50m","memory":"92960Ki"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/etc/config","name":"heapster-config-volume"},{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"heapster-token-jx6jf","readOnly":true}]}],"dnsPolicy":"ClusterFirst","nodeName":"gke-tesg1-default-pool-ff7a1295-nvv4","priority":2000000000,"priorityClassName":"system-cluster-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"serviceAccount":"heapster","serviceAccountName":"heapster","terminationGracePeriodSeconds":30,"tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":300},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":300}],"volumes":[{"configMap":{"defaultMode":420,"name":"heapster-config"},"name":"heapster-config-volume"},{"name":"heapster-token-jx6jf","secret":{"defaultMode":420,"secretName":"heapster-token-jx6jf"}}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:47Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:52Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:52Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:47Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://3aef2e789fe3f521f79bb81fc1b15f7ad0940026ffa20c41b2b3aa64c8d28a7f","image":"k8s.gcr.io/heapster-amd64:v1.6.0-beta.1","imageID":"docker-pullable://k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521","lastState":{},"name":"heapster","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:51Z"}}},{"containerID":"docker://502e64c6c6e0504fe9facf3ab48bdb2d6f75233f1e754c28ed306d09c86fa76d","image":"k8s.gcr.io/addon-resizer:1.8.3","imageID":"docker-pullable://k8s.gcr.io/addon-resizer@sha256:07353f7b26327f0d933515a22b1de587b040d3d85c464ea299c1b9f242529326","lastState":{},"name":"heapster-nanny","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:52Z"}}},{"containerID":"docker://52abaaf1d074315ad5323417fa3bb13d1f7a2b9ac6eb4e0a91b1150f63258878","image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","imageID":"docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e","lastState":{},"name":"prom-to-sd","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:51Z"}}}],"hostIP":"10.128.0.15","phase":"Running","podIP":"10.8.2.3","qosClass":"Burstable","startTime":"2019-06-18T22:22:47Z"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-06-18T22:22:47Z"
    generateName: heapster-v1.6.0-beta.1-68cdfd6769-
    labels:
      k8s-app: heapster
      pod-template-hash: 68cdfd6769
      version: v1.6.0-beta.1
    name: heapster-v1.6.0-beta.1-68cdfd6769-jlmfs
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: heapster-v1.6.0-beta.1-68cdfd6769
      uid: 99cfbb83-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671150"
    selfLink: /api/v1/namespaces/kube-system/pods/heapster-v1.6.0-beta.1-68cdfd6769-jlmfs
    uid: 99d69b3a-9217-11e9-a049-42010a8000d3
  spec:
    containers:
    - command:
      - /heapster
      - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
      - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
      image: k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8082
          scheme: HTTP
        initialDelaySeconds: 180
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: heapster
      resources:
        limits:
          cpu: 13m
          memory: 120Mi
        requests:
          cpu: 13m
          memory: 120Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    - command:
      - /monitor
      - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prom-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    - command:
      - /pod_nanny
      - --config-dir=/etc/config
      - --cpu=10m
      - --extra-cpu=0.5m
      - --memory=100Mi
      - --extra-memory=4Mi
      - --threshold=5
      - --deployment=heapster-v1.6.0-beta.1
      - --container=heapster
      - --poll-period=300000
      - --estimator=exponential
      - --minClusterSize=5
      env:
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/addon-resizer:1.8.3
      imagePullPolicy: IfNotPresent
      name: heapster-nanny
      resources:
        limits:
          cpu: 50m
          memory: 92960Ki
        requests:
          cpu: 50m
          memory: 92960Ki
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: heapster-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: heapster
    serviceAccountName: heapster
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: heapster-config
      name: heapster-config-volume
    - name: heapster-token-jx6jf
      secret:
        defaultMode: 420
        secretName: heapster-token-jx6jf
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:47Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:47Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://3aef2e789fe3f521f79bb81fc1b15f7ad0940026ffa20c41b2b3aa64c8d28a7f
      image: k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      imageID: docker-pullable://k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521
      lastState: {}
      name: heapster
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:51Z"
    - containerID: docker://502e64c6c6e0504fe9facf3ab48bdb2d6f75233f1e754c28ed306d09c86fa76d
      image: k8s.gcr.io/addon-resizer:1.8.3
      imageID: docker-pullable://k8s.gcr.io/addon-resizer@sha256:07353f7b26327f0d933515a22b1de587b040d3d85c464ea299c1b9f242529326
      lastState: {}
      name: heapster-nanny
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:52Z"
    - containerID: docker://52abaaf1d074315ad5323417fa3bb13d1f7a2b9ac6eb4e0a91b1150f63258878
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prom-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:51Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.8.2.3
    qosClass: Burstable
    startTime: "2019-06-18T22:22:47Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-10-13T12:28:23Z"
    generateName: heapster-v1.6.1-6c5f84b7db-
    labels:
      k8s-app: heapster
      pod-template-hash: 6c5f84b7db
      version: v1.6.1
    name: heapster-v1.6.1-6c5f84b7db-v8b5f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: heapster-v1.6.1-6c5f84b7db
      uid: f2941721-edb4-11e9-9caf-42010a800177
    resourceVersion: "63671155"
    selfLink: /api/v1/namespaces/kube-system/pods/heapster-v1.6.1-6c5f84b7db-v8b5f
    uid: f2a12d30-edb4-11e9-9caf-42010a800177
  spec:
    containers:
    - command:
      - /heapster
      - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
      - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
      image: gcr.io/stackdriver-agents/heapster-amd64:v1.6.1
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8082
          scheme: HTTP
        initialDelaySeconds: 180
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: heapster
      resources:
        limits:
          cpu: 13m
          memory: 120Mi
        requests:
          cpu: 13m
          memory: 120Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    - command:
      - /monitor
      - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imagePullPolicy: IfNotPresent
      name: prom-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    - command:
      - /pod_nanny
      - --config-dir=/etc/config
      - --cpu=10m
      - --extra-cpu=0.5m
      - --memory=100Mi
      - --extra-memory=4Mi
      - --threshold=5
      - --deployment=heapster-v1.6.1
      - --container=heapster
      - --poll-period=300000
      - --estimator=exponential
      - --minClusterSize=5
      env:
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/addon-resizer:1.8.3
      imagePullPolicy: IfNotPresent
      name: heapster-nanny
      resources:
        limits:
          cpu: 50m
          memory: 93360Ki
        requests:
          cpu: 50m
          memory: 93360Ki
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: heapster-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: heapster-token-jx6jf
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      fsGroup: 65534
      supplementalGroups:
      - 65534
    serviceAccount: heapster
    serviceAccountName: heapster
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: heapster-config
      name: heapster-config-volume
    - name: heapster-token-jx6jf
      secret:
        defaultMode: 420
        secretName: heapster-token-jx6jf
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://2e1cfd1893541f2c44ffe84374abfa2a07620ab82366c346fbe9afcd728afd65
      image: gcr.io/stackdriver-agents/heapster-amd64:v1.6.1
      imageID: docker-pullable://gcr.io/stackdriver-agents/heapster-amd64@sha256:b00d659e538bedecf7014fffb7a6d0d3377326a921b4416a7727a7022a7be901
      lastState: {}
      name: heapster
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:28:29Z"
    - containerID: docker://8e2562279c49504bed8f4586c1452d6f759b67ce2c16e4632c8d73a133a55942
      image: k8s.gcr.io/addon-resizer:1.8.3
      imageID: docker-pullable://k8s.gcr.io/addon-resizer@sha256:07353f7b26327f0d933515a22b1de587b040d3d85c464ea299c1b9f242529326
      lastState: {}
      name: heapster-nanny
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:28:31Z"
    - containerID: docker://5e50a4cd880a272e183ecf2eabb22956d6cfed8fe74f1c622f1ca2eaf259e275
      image: k8s.gcr.io/prometheus-to-sd:v0.5.0
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:14666989f40bb7c896c3e775a93c6873e2b791d65bc65579f58a078b7f9a764e
      lastState: {}
      name: prom-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-10-13T12:28:30Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.8.2.152
    qosClass: Burstable
    startTime: "2019-10-13T12:28:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-11-15T06:03:27Z"
    generateName: kube-dns-67947d6c68-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 67947d6c68
    name: kube-dns-67947d6c68-277dx
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-67947d6c68
      uid: 9cea8b9b-076d-11ea-a0a0-42010a80004b
    resourceVersion: "63671148"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-67947d6c68-277dx
    uid: a43af329-076d-11ea-a0a0-42010a80004b
  spec:
    containers:
    - args:
      - --domain=cluster.local.
      - --dns-port=10053
      - --config-dir=/kube-dns-config
      - --v=2
      env:
      - name: PROMETHEUS_PORT
        value: "10055"
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/kubedns
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kubedns
      ports:
      - containerPort: 10053
        name: dns-local
        protocol: UDP
      - containerPort: 10053
        name: dns-tcp-local
        protocol: TCP
      - containerPort: 10055
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /kube-dns-config
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    - args:
      - -v=2
      - -logtostderr
      - -configDir=/etc/k8s/dns/dnsmasq-nanny
      - -restartDnsmasq=true
      - --
      - -k
      - --cache-size=1000
      - --no-negcache
      - --log-facility=-
      - --server=/cluster.local/127.0.0.1#10053
      - --server=/in-addr.arpa/127.0.0.1#10053
      - --server=/ip6.arpa/127.0.0.1#10053
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/dnsmasq
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dnsmasq
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      resources:
        requests:
          cpu: 150m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/k8s/dns/dnsmasq-nanny
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    - args:
      - --v=2
      - --logtostderr
      - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
      - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: sidecar
      ports:
      - containerPort: 10054
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    - command:
      - /monitor
      - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      - --v=2
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    dnsPolicy: Default
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-dns
    serviceAccountName: kube-dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-dns
        optional: true
      name: kube-dns-config
    - name: kube-dns-token-cnjsm
      secret:
        defaultMode: 420
        secretName: kube-dns-token-cnjsm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:45Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:45Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:27Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://65e4d3cac5a3cc449224d75a5728ef3de4c086ec22b4e96196ba8e659e16cf75
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:0a70a8a9ae8cfe752021de84f13b3ecd109d9b5fbe3f1541c52fcd1d4c2c0b45
      lastState: {}
      name: dnsmasq
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:32Z"
    - containerID: docker://e827c85e5995724affd59f44a9ccdd6b6f2b1d901560385f2d373a1ec98e3f50
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:a13c60e2a9d49f965095a1e003388926f3f2a6189ed4aecb1541f114c955f8ec
      lastState: {}
      name: kubedns
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:30Z"
    - containerID: docker://1d440e4f3c781a74967df8458ca6c6422acb7605dccad9bd10e8c3d226374fee
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2
      lastState: {}
      name: prometheus-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:36Z"
    - containerID: docker://0d6f4802d841c075e9817a756c3dce0277ec02efa4ac0114aa61f95991f92365
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:e55cbd5361a86bf0a01bfeaca2e958e15571f1e741356eab83bb444a13020d4c
      lastState: {}
      name: sidecar
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:34Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.8.2.150
    qosClass: Burstable
    startTime: "2019-11-15T06:03:27Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-11-15T06:03:15Z"
    generateName: kube-dns-67947d6c68-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 67947d6c68
    name: kube-dns-67947d6c68-6lqpz
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-67947d6c68
      uid: 9cea8b9b-076d-11ea-a0a0-42010a80004b
    resourceVersion: "63671111"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-67947d6c68-6lqpz
    uid: 9ceef3cc-076d-11ea-a0a0-42010a80004b
  spec:
    containers:
    - args:
      - --domain=cluster.local.
      - --dns-port=10053
      - --config-dir=/kube-dns-config
      - --v=2
      env:
      - name: PROMETHEUS_PORT
        value: "10055"
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/kubedns
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: kubedns
      ports:
      - containerPort: 10053
        name: dns-local
        protocol: UDP
      - containerPort: 10053
        name: dns-tcp-local
        protocol: TCP
      - containerPort: 10055
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /readiness
          port: 8081
          scheme: HTTP
        initialDelaySeconds: 3
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /kube-dns-config
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    - args:
      - -v=2
      - -logtostderr
      - -configDir=/etc/k8s/dns/dnsmasq-nanny
      - -restartDnsmasq=true
      - --
      - -k
      - --cache-size=1000
      - --no-negcache
      - --log-facility=-
      - --server=/cluster.local/127.0.0.1#10053
      - --server=/in-addr.arpa/127.0.0.1#10053
      - --server=/ip6.arpa/127.0.0.1#10053
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /healthcheck/dnsmasq
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: dnsmasq
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      resources:
        requests:
          cpu: 150m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/k8s/dns/dnsmasq-nanny
        name: kube-dns-config
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    - args:
      - --v=2
      - --logtostderr
      - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
      - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /metrics
          port: 10054
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: sidecar
      ports:
      - containerPort: 10054
        name: metrics
        protocol: TCP
      resources:
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    - command:
      - /monitor
      - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
      - --stackdriver-prefix=container.googleapis.com/internal/addons
      - --api-override=https://monitoring.googleapis.com/
      - --pod-id=$(POD_NAME)
      - --namespace-id=$(POD_NAMESPACE)
      - --v=2
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-token-cnjsm
        readOnly: true
    dnsPolicy: Default
    nodeName: gke-tesg1-default-pool-ff7a1295-dmtd
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-dns
    serviceAccountName: kube-dns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-dns
        optional: true
      name: kube-dns-config
    - name: kube-dns-token-cnjsm
      secret:
        defaultMode: 420
        secretName: kube-dns-token-cnjsm
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:15Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://dc7fdfc9fecce742fa4371b8e5ba0d623f7bafdbebd2091750e985b18db0a75a
      image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:0a70a8a9ae8cfe752021de84f13b3ecd109d9b5fbe3f1541c52fcd1d4c2c0b45
      lastState: {}
      name: dnsmasq
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:19Z"
    - containerID: docker://43f3b99d471abf7831f96cdfd266d207f48e8863a034596ff794e613af55b92b
      image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:a13c60e2a9d49f965095a1e003388926f3f2a6189ed4aecb1541f114c955f8ec
      lastState: {}
      name: kubedns
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:17Z"
    - containerID: docker://75e641853cea880d6d00a7e2a39311210de303234e4e294919b30c734d151a03
      image: k8s.gcr.io/prometheus-to-sd:v0.4.2
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:aca8ef83a7fae83f1f8583e978dd4d1ff655b9f2ca0a76bda5edce6d8965bdf2
      lastState: {}
      name: prometheus-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:23Z"
    - containerID: docker://7c6c88558d8c3d7102c890ffccdb2d0e2df3e6fe99eda14c60994ca02f4f5127
      image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      imageID: docker-pullable://k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:e55cbd5361a86bf0a01bfeaca2e958e15571f1e741356eab83bb444a13020d4c
      lastState: {}
      name: sidecar
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:21Z"
    hostIP: 10.128.0.17
    phase: Running
    podIP: 10.8.4.93
    qosClass: Burstable
    startTime: "2019-11-15T06:03:15Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"creationTimestamp":"2019-06-18T22:22:36Z","generateName":"kube-dns-autoscaler-76fcd5f658-","labels":{"k8s-app":"kube-dns-autoscaler","pod-template-hash":"76fcd5f658"},"name":"kube-dns-autoscaler-76fcd5f658-rnlsr","namespace":"kube-system","ownerReferences":[{"apiVersion":"apps/v1","blockOwnerDeletion":true,"controller":true,"kind":"ReplicaSet","name":"kube-dns-autoscaler-76fcd5f658","uid":"865eb933-9217-11e9-a049-42010a8000d3"}],"resourceVersion":"662","selfLink":"/api/v1/namespaces/kube-system/pods/kube-dns-autoscaler-76fcd5f658-rnlsr","uid":"931e934b-9217-11e9-a049-42010a8000d3"},"spec":{"containers":[{"command":["/cluster-proportional-autoscaler","--namespace=kube-system","--configmap=kube-dns-autoscaler","--target=Deployment/kube-dns","--default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}}","--logtostderr=true","--v=2"],"image":"k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0","imagePullPolicy":"IfNotPresent","name":"autoscaler","resources":{"requests":{"cpu":"20m","memory":"10Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"kube-dns-autoscaler-token-pk8rd","readOnly":true}]}],"dnsPolicy":"ClusterFirst","nodeName":"gke-tesg1-default-pool-ff7a1295-7kwg","priority":2000000000,"priorityClassName":"system-cluster-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"serviceAccount":"kube-dns-autoscaler","serviceAccountName":"kube-dns-autoscaler","terminationGracePeriodSeconds":30,"tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":300},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":300}],"volumes":[{"name":"kube-dns-autoscaler-token-pk8rd","secret":{"defaultMode":420,"secretName":"kube-dns-autoscaler-token-pk8rd"}}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:36Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:40Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:40Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:36Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://49ed462fc1735138f8d68eda8f2c790c506ef26098acb86af6b0629e2af6b40c","image":"k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0","imageID":"docker-pullable://k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:36359630278b119e7dd78f5437be1c667080108fa59ecba1b81cda3610dcf4d7","lastState":{},"name":"autoscaler","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:39Z"}}}],"hostIP":"10.128.0.14","phase":"Running","podIP":"10.8.1.2","qosClass":"Burstable","startTime":"2019-06-18T22:22:36Z"}}
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-06-18T22:22:36Z"
    generateName: kube-dns-autoscaler-76fcd5f658-
    labels:
      k8s-app: kube-dns-autoscaler
      pod-template-hash: 76fcd5f658
    name: kube-dns-autoscaler-76fcd5f658-rnlsr
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: kube-dns-autoscaler-76fcd5f658
      uid: 865eb933-9217-11e9-a049-42010a8000d3
    resourceVersion: "63701955"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-dns-autoscaler-76fcd5f658-rnlsr
    uid: 931e934b-9217-11e9-a049-42010a8000d3
  spec:
    containers:
    - command:
      - /cluster-proportional-autoscaler
      - --namespace=kube-system
      - --configmap=kube-dns-autoscaler
      - --target=Deployment/kube-dns
      - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
      - --logtostderr=true
      - --v=2
      image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
      imagePullPolicy: IfNotPresent
      name: autoscaler
      resources:
        requests:
          cpu: 20m
          memory: 10Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-dns-autoscaler-token-pk8rd
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-dns-autoscaler
    serviceAccountName: kube-dns-autoscaler
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: kube-dns-autoscaler-token-pk8rd
      secret:
        defaultMode: 420
        secretName: kube-dns-autoscaler-token-pk8rd
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:26Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:26Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://225936f9e2b2ea16f272d9290869cd2bd758a301da47336d595daebd11ccb618
      image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
      imageID: docker-pullable://k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:36359630278b119e7dd78f5437be1c667080108fa59ecba1b81cda3610dcf4d7
      lastState:
        terminated:
          containerID: docker://49ed462fc1735138f8d68eda8f2c790c506ef26098acb86af6b0629e2af6b40c
          exitCode: 255
          finishedAt: "2019-11-28T15:56:33Z"
          reason: Error
          startedAt: "2019-06-18T22:22:39Z"
      name: autoscaler
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:57:25Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.8.1.28
    qosClass: Burstable
    startTime: "2019-06-18T22:22:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"kubernetes.io/config.hash":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.mirror":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.seen":"2019-06-18T22:22:27.732454932Z","kubernetes.io/config.source":"file","scheduler.alpha.kubernetes.io/critical-pod":""},"creationTimestamp":"2019-06-18T22:22:28Z","labels":{"component":"kube-proxy","tier":"node"},"name":"kube-proxy-gke-tesg1-default-pool-ff7a1295-7kwg","namespace":"kube-system","resourceVersion":"589","selfLink":"/api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-7kwg","uid":"8e62eec8-9217-11e9-a049-42010a8000d3"},"spec":{"containers":[{"command":["/bin/sh","-c","exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig --cluster-cidr=10.8.0.0/14 --resource-container=\"\" --oom-score-adj=-998 --v=2 --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m --ipvs-min-sync-period=10s 1\u003e\u003e/var/log/kube-proxy.log 2\u003e\u00261"],"image":"k8s.gcr.io/kube-proxy:v1.12.8-gke.6","imagePullPolicy":"IfNotPresent","name":"kube-proxy","resources":{"requests":{"cpu":"100m"}},"securityContext":{"privileged":true,"procMount":"Default"},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/etc/ssl/certs","name":"etc-ssl-certs","readOnly":true},{"mountPath":"/usr/share/ca-certificates","name":"usr-ca-certs","readOnly":true},{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/kube-proxy/kubeconfig","name":"kubeconfig"},{"mountPath":"/run/xtables.lock","name":"iptableslock"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true}]}],"dnsPolicy":"ClusterFirst","hostNetwork":true,"nodeName":"gke-tesg1-default-pool-ff7a1295-7kwg","priority":2000001000,"priorityClassName":"system-node-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/usr/share/ca-certificates","type":""},"name":"usr-ca-certs"},{"hostPath":{"path":"/etc/ssl/certs","type":""},"name":"etc-ssl-certs"},{"hostPath":{"path":"/var/lib/kube-proxy/kubeconfig","type":"FileOrCreate"},"name":"kubeconfig"},{"hostPath":{"path":"/var/log","type":""},"name":"varlog"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"iptableslock"},{"hostPath":{"path":"/lib/modules","type":""},"name":"lib-modules"}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:31Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:31Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://de0258e51a87031070db79a29231186987b10d21e459586631d801895d0de437","image":"gcr.io/google_containers/kube-proxy:v1.12.8-gke.6","imageID":"docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1","lastState":{},"name":"kube-proxy","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:29Z"}}}],"hostIP":"10.128.0.14","phase":"Running","podIP":"10.128.0.14","qosClass":"Burstable","startTime":"2019-06-18T22:22:28Z"}}
      kubernetes.io/config.hash: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.mirror: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.seen: "2019-06-18T22:22:27.732454932Z"
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-06-18T22:22:28Z"
    labels:
      component: kube-proxy
      tier: node
    name: kube-proxy-gke-tesg1-default-pool-ff7a1295-7kwg
    namespace: kube-system
    resourceVersion: "63701954"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-7kwg
    uid: 8e62eec8-9217-11e9-a049-42010a8000d3
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig
        --cluster-cidr=10.8.0.0/14 --resource-container="" --oom-score-adj=-998 --v=2
        --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true
        --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m
        --ipvs-min-sync-period=10s 1>>/var/log/kube-proxy.log 2>&1
      image: k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
        procMount: Default
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-ca-certs
        readOnly: true
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/kube-proxy/kubeconfig
        name: kubeconfig
      - mountPath: /run/xtables.lock
        name: iptableslock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
        type: ""
      name: usr-ca-certs
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: etc-ssl-certs
    - hostPath:
        path: /var/lib/kube-proxy/kubeconfig
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: iptableslock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:56:38Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:56:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:56:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:56:38Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://40b09868893ce244ed4744347184a96731b2b0b6e53ca7783ecff1ad9ef06954
      image: gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      imageID: docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1
      lastState:
        terminated:
          containerID: docker://de0258e51a87031070db79a29231186987b10d21e459586631d801895d0de437
          exitCode: 255
          finishedAt: "2019-11-28T15:56:33Z"
          reason: Error
          startedAt: "2019-06-18T22:22:29Z"
      name: kube-proxy
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:56:39Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.128.0.14
    qosClass: Burstable
    startTime: "2019-11-28T15:56:38Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"kubernetes.io/config.hash":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.mirror":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.seen":"2019-08-14T20:40:51.776776148Z","kubernetes.io/config.source":"file","scheduler.alpha.kubernetes.io/critical-pod":""},"creationTimestamp":"2019-08-14T20:40:52Z","labels":{"component":"kube-proxy","tier":"node"},"name":"kube-proxy-gke-tesg1-default-pool-ff7a1295-dmtd","namespace":"kube-system","resourceVersion":"14870513","selfLink":"/api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-dmtd","uid":"ce78f015-bed3-11e9-98f3-42010a8000c4"},"spec":{"containers":[{"command":["/bin/sh","-c","exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig --cluster-cidr=10.8.0.0/14 --resource-container=\"\" --oom-score-adj=-998 --v=2 --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m --ipvs-min-sync-period=10s 1\u003e\u003e/var/log/kube-proxy.log 2\u003e\u00261"],"image":"k8s.gcr.io/kube-proxy:v1.12.8-gke.6","imagePullPolicy":"IfNotPresent","name":"kube-proxy","resources":{"requests":{"cpu":"100m"}},"securityContext":{"privileged":true,"procMount":"Default"},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/etc/ssl/certs","name":"etc-ssl-certs","readOnly":true},{"mountPath":"/usr/share/ca-certificates","name":"usr-ca-certs","readOnly":true},{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/kube-proxy/kubeconfig","name":"kubeconfig"},{"mountPath":"/run/xtables.lock","name":"iptableslock"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true}]}],"dnsPolicy":"ClusterFirst","hostNetwork":true,"nodeName":"gke-tesg1-default-pool-ff7a1295-dmtd","priority":2000001000,"priorityClassName":"system-node-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/usr/share/ca-certificates","type":""},"name":"usr-ca-certs"},{"hostPath":{"path":"/etc/ssl/certs","type":""},"name":"etc-ssl-certs"},{"hostPath":{"path":"/var/lib/kube-proxy/kubeconfig","type":"FileOrCreate"},"name":"kubeconfig"},{"hostPath":{"path":"/var/log","type":""},"name":"varlog"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"iptableslock"},{"hostPath":{"path":"/lib/modules","type":""},"name":"lib-modules"}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T20:40:52Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T20:40:55Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T20:40:55Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T20:40:52Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://de8db90e135a4136623efaf5b227b2412b089186c374c05d4bb8db58feadaa1f","image":"gcr.io/google_containers/kube-proxy:v1.12.8-gke.6","imageID":"docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1","lastState":{},"name":"kube-proxy","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-08-14T20:40:54Z"}}}],"hostIP":"10.128.0.17","phase":"Running","podIP":"10.128.0.17","qosClass":"Burstable","startTime":"2019-08-14T20:40:52Z"}}
      kubernetes.io/config.hash: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.mirror: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.seen: "2019-08-14T20:40:51.776776148Z"
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-08-14T20:40:52Z"
    labels:
      component: kube-proxy
      tier: node
    name: kube-proxy-gke-tesg1-default-pool-ff7a1295-dmtd
    namespace: kube-system
    resourceVersion: "63671116"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-dmtd
    uid: ce78f015-bed3-11e9-98f3-42010a8000c4
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig
        --cluster-cidr=10.8.0.0/14 --resource-container="" --oom-score-adj=-998 --v=2
        --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true
        --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m
        --ipvs-min-sync-period=10s 1>>/var/log/kube-proxy.log 2>&1
      image: k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
        procMount: Default
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-ca-certs
        readOnly: true
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/kube-proxy/kubeconfig
        name: kubeconfig
      - mountPath: /run/xtables.lock
        name: iptableslock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-dmtd
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
        type: ""
      name: usr-ca-certs
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: etc-ssl-certs
    - hostPath:
        path: /var/lib/kube-proxy/kubeconfig
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: iptableslock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T20:40:52Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T20:40:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T20:40:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T20:40:52Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://de8db90e135a4136623efaf5b227b2412b089186c374c05d4bb8db58feadaa1f
      image: gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      imageID: docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-08-14T20:40:54Z"
    hostIP: 10.128.0.17
    phase: Running
    podIP: 10.128.0.17
    qosClass: Burstable
    startTime: "2019-08-14T20:40:52Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"kubernetes.io/config.hash":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.mirror":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.seen":"2019-06-18T22:22:26.302258315Z","kubernetes.io/config.source":"file","scheduler.alpha.kubernetes.io/critical-pod":""},"creationTimestamp":"2019-06-18T22:22:26Z","labels":{"component":"kube-proxy","tier":"node"},"name":"kube-proxy-gke-tesg1-default-pool-ff7a1295-nvv4","namespace":"kube-system","resourceVersion":"551","selfLink":"/api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-nvv4","uid":"8d8d6f33-9217-11e9-a049-42010a8000d3"},"spec":{"containers":[{"command":["/bin/sh","-c","exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig --cluster-cidr=10.8.0.0/14 --resource-container=\"\" --oom-score-adj=-998 --v=2 --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m --ipvs-min-sync-period=10s 1\u003e\u003e/var/log/kube-proxy.log 2\u003e\u00261"],"image":"k8s.gcr.io/kube-proxy:v1.12.8-gke.6","imagePullPolicy":"IfNotPresent","name":"kube-proxy","resources":{"requests":{"cpu":"100m"}},"securityContext":{"privileged":true,"procMount":"Default"},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/etc/ssl/certs","name":"etc-ssl-certs","readOnly":true},{"mountPath":"/usr/share/ca-certificates","name":"usr-ca-certs","readOnly":true},{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/kube-proxy/kubeconfig","name":"kubeconfig"},{"mountPath":"/run/xtables.lock","name":"iptableslock"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true}]}],"dnsPolicy":"ClusterFirst","hostNetwork":true,"nodeName":"gke-tesg1-default-pool-ff7a1295-nvv4","priority":2000001000,"priorityClassName":"system-node-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/usr/share/ca-certificates","type":""},"name":"usr-ca-certs"},{"hostPath":{"path":"/etc/ssl/certs","type":""},"name":"etc-ssl-certs"},{"hostPath":{"path":"/var/lib/kube-proxy/kubeconfig","type":"FileOrCreate"},"name":"kubeconfig"},{"hostPath":{"path":"/var/log","type":""},"name":"varlog"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"iptableslock"},{"hostPath":{"path":"/lib/modules","type":""},"name":"lib-modules"}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://1cd3d4600c09bb77b43d71b758f6663db0265f1a7fd92536adc407536a85ab32","image":"gcr.io/google_containers/kube-proxy:v1.12.8-gke.6","imageID":"docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1","lastState":{},"name":"kube-proxy","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:28Z"}}}],"hostIP":"10.128.0.15","phase":"Running","podIP":"10.128.0.15","qosClass":"Burstable","startTime":"2019-06-18T22:22:28Z"}}
      kubernetes.io/config.hash: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.mirror: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.seen: "2019-06-18T22:22:26.302258315Z"
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-06-18T22:22:26Z"
    labels:
      component: kube-proxy
      tier: node
    name: kube-proxy-gke-tesg1-default-pool-ff7a1295-nvv4
    namespace: kube-system
    resourceVersion: "63671142"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-nvv4
    uid: 8d8d6f33-9217-11e9-a049-42010a8000d3
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig
        --cluster-cidr=10.8.0.0/14 --resource-container="" --oom-score-adj=-998 --v=2
        --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true
        --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m
        --ipvs-min-sync-period=10s 1>>/var/log/kube-proxy.log 2>&1
      image: k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
        procMount: Default
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-ca-certs
        readOnly: true
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/kube-proxy/kubeconfig
        name: kubeconfig
      - mountPath: /run/xtables.lock
        name: iptableslock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
        type: ""
      name: usr-ca-certs
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: etc-ssl-certs
    - hostPath:
        path: /var/lib/kube-proxy/kubeconfig
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: iptableslock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:28Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://1cd3d4600c09bb77b43d71b758f6663db0265f1a7fd92536adc407536a85ab32
      image: gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      imageID: docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:28Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.128.0.15
    qosClass: Burstable
    startTime: "2019-06-18T22:22:28Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"kubernetes.io/config.hash":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.mirror":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.seen":"2019-06-18T22:22:26.249512611Z","kubernetes.io/config.source":"file","scheduler.alpha.kubernetes.io/critical-pod":""},"creationTimestamp":"2019-06-18T22:22:26Z","labels":{"component":"kube-proxy","tier":"node"},"name":"kube-proxy-gke-tesg1-default-pool-ff7a1295-z7lx","namespace":"kube-system","resourceVersion":"590","selfLink":"/api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-z7lx","uid":"8d839606-9217-11e9-a049-42010a8000d3"},"spec":{"containers":[{"command":["/bin/sh","-c","exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig --cluster-cidr=10.8.0.0/14 --resource-container=\"\" --oom-score-adj=-998 --v=2 --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m --ipvs-min-sync-period=10s 1\u003e\u003e/var/log/kube-proxy.log 2\u003e\u00261"],"image":"k8s.gcr.io/kube-proxy:v1.12.8-gke.6","imagePullPolicy":"IfNotPresent","name":"kube-proxy","resources":{"requests":{"cpu":"100m"}},"securityContext":{"privileged":true,"procMount":"Default"},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/etc/ssl/certs","name":"etc-ssl-certs","readOnly":true},{"mountPath":"/usr/share/ca-certificates","name":"usr-ca-certs","readOnly":true},{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/kube-proxy/kubeconfig","name":"kubeconfig"},{"mountPath":"/run/xtables.lock","name":"iptableslock"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true}]}],"dnsPolicy":"ClusterFirst","hostNetwork":true,"nodeName":"gke-tesg1-default-pool-ff7a1295-z7lx","priority":2000001000,"priorityClassName":"system-node-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/usr/share/ca-certificates","type":""},"name":"usr-ca-certs"},{"hostPath":{"path":"/etc/ssl/certs","type":""},"name":"etc-ssl-certs"},{"hostPath":{"path":"/var/lib/kube-proxy/kubeconfig","type":"FileOrCreate"},"name":"kubeconfig"},{"hostPath":{"path":"/var/log","type":""},"name":"varlog"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"iptableslock"},{"hostPath":{"path":"/lib/modules","type":""},"name":"lib-modules"}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:26Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:28Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:26Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://6909f72514371a2cc96416c2c073e36cffbc5924cc36fa1f2fb396e05f198758","image":"gcr.io/google_containers/kube-proxy:v1.12.8-gke.6","imageID":"docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1","lastState":{},"name":"kube-proxy","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:27Z"}}}],"hostIP":"10.128.0.13","phase":"Running","podIP":"10.128.0.13","qosClass":"Burstable","startTime":"2019-06-18T22:22:26Z"}}
      kubernetes.io/config.hash: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.mirror: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.seen: "2019-06-18T22:22:26.249512611Z"
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-06-18T22:22:26Z"
    labels:
      component: kube-proxy
      tier: node
    name: kube-proxy-gke-tesg1-default-pool-ff7a1295-z7lx
    namespace: kube-system
    resourceVersion: "63671036"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-z7lx
    uid: 8d839606-9217-11e9-a049-42010a8000d3
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig
        --cluster-cidr=10.8.0.0/14 --resource-container="" --oom-score-adj=-998 --v=2
        --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true
        --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m
        --ipvs-min-sync-period=10s 1>>/var/log/kube-proxy.log 2>&1
      image: k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
        procMount: Default
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-ca-certs
        readOnly: true
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/kube-proxy/kubeconfig
        name: kubeconfig
      - mountPath: /run/xtables.lock
        name: iptableslock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
        type: ""
      name: usr-ca-certs
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: etc-ssl-certs
    - hostPath:
        path: /var/lib/kube-proxy/kubeconfig
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: iptableslock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://6909f72514371a2cc96416c2c073e36cffbc5924cc36fa1f2fb396e05f198758
      image: gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      imageID: docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:27Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.128.0.13
    qosClass: Burstable
    startTime: "2019-06-18T22:22:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"kubernetes.io/config.hash":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.mirror":"55057afd0355181dd59a620b4d1677a3","kubernetes.io/config.seen":"2019-08-14T18:17:23.059098137Z","kubernetes.io/config.source":"file","scheduler.alpha.kubernetes.io/critical-pod":""},"creationTimestamp":"2019-08-14T18:17:23Z","labels":{"component":"kube-proxy","tier":"node"},"name":"kube-proxy-gke-tesg1-default-pool-ff7a1295-zpc5","namespace":"kube-system","resourceVersion":"14842872","selfLink":"/api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-zpc5","uid":"c34051ce-bebf-11e9-98f3-42010a8000c4"},"spec":{"containers":[{"command":["/bin/sh","-c","exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig --cluster-cidr=10.8.0.0/14 --resource-container=\"\" --oom-score-adj=-998 --v=2 --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m --ipvs-min-sync-period=10s 1\u003e\u003e/var/log/kube-proxy.log 2\u003e\u00261"],"image":"k8s.gcr.io/kube-proxy:v1.12.8-gke.6","imagePullPolicy":"IfNotPresent","name":"kube-proxy","resources":{"requests":{"cpu":"100m"}},"securityContext":{"privileged":true,"procMount":"Default"},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/etc/ssl/certs","name":"etc-ssl-certs","readOnly":true},{"mountPath":"/usr/share/ca-certificates","name":"usr-ca-certs","readOnly":true},{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/kube-proxy/kubeconfig","name":"kubeconfig"},{"mountPath":"/run/xtables.lock","name":"iptableslock"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true}]}],"dnsPolicy":"ClusterFirst","hostNetwork":true,"nodeName":"gke-tesg1-default-pool-ff7a1295-zpc5","priority":2000001000,"priorityClassName":"system-node-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/usr/share/ca-certificates","type":""},"name":"usr-ca-certs"},{"hostPath":{"path":"/etc/ssl/certs","type":""},"name":"etc-ssl-certs"},{"hostPath":{"path":"/var/lib/kube-proxy/kubeconfig","type":"FileOrCreate"},"name":"kubeconfig"},{"hostPath":{"path":"/var/log","type":""},"name":"varlog"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"iptableslock"},{"hostPath":{"path":"/lib/modules","type":""},"name":"lib-modules"}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T18:17:23Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T18:17:25Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T18:17:25Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-08-14T18:17:23Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://c299068338607f6d53f4f8badd1eec51c2dcf4ed0099e07e0e13427829ffae71","image":"gcr.io/google_containers/kube-proxy:v1.12.8-gke.6","imageID":"docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1","lastState":{},"name":"kube-proxy","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-08-14T18:17:24Z"}}}],"hostIP":"10.128.0.16","phase":"Running","podIP":"10.128.0.16","qosClass":"Burstable","startTime":"2019-08-14T18:17:23Z"}}
      kubernetes.io/config.hash: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.mirror: 55057afd0355181dd59a620b4d1677a3
      kubernetes.io/config.seen: "2019-08-14T18:17:23.059098137Z"
      kubernetes.io/config.source: file
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2019-08-14T18:17:23Z"
    labels:
      component: kube-proxy
      tier: node
    name: kube-proxy-gke-tesg1-default-pool-ff7a1295-zpc5
    namespace: kube-system
    resourceVersion: "63671010"
    selfLink: /api/v1/namespaces/kube-system/pods/kube-proxy-gke-tesg1-default-pool-ff7a1295-zpc5
    uid: c34051ce-bebf-11e9-98f3-42010a8000c4
  spec:
    containers:
    - command:
      - /bin/sh
      - -c
      - exec kube-proxy --master=https://35.225.186.34 --kubeconfig=/var/lib/kube-proxy/kubeconfig
        --cluster-cidr=10.8.0.0/14 --resource-container="" --oom-score-adj=-998 --v=2
        --feature-gates=DynamicKubeletConfig=false,RotateKubeletServerCertificate=true,ExperimentalCriticalPodAnnotation=true
        --iptables-sync-period=1m --iptables-min-sync-period=10s --ipvs-sync-period=1m
        --ipvs-min-sync-period=10s 1>>/var/log/kube-proxy.log 2>&1
      image: k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources:
        requests:
          cpu: 100m
      securityContext:
        privileged: true
        procMount: Default
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: etc-ssl-certs
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-ca-certs
        readOnly: true
      - mountPath: /var/log
        name: varlog
      - mountPath: /var/lib/kube-proxy/kubeconfig
        name: kubeconfig
      - mountPath: /run/xtables.lock
        name: iptableslock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-zpc5
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    - effect: NoSchedule
      operator: Exists
    volumes:
    - hostPath:
        path: /usr/share/ca-certificates
        type: ""
      name: usr-ca-certs
    - hostPath:
        path: /etc/ssl/certs
        type: ""
      name: etc-ssl-certs
    - hostPath:
        path: /var/lib/kube-proxy/kubeconfig
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /var/log
        type: ""
      name: varlog
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: iptableslock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T18:17:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T18:17:25Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T18:17:25Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-08-14T18:17:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://c299068338607f6d53f4f8badd1eec51c2dcf4ed0099e07e0e13427829ffae71
      image: gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      imageID: docker://sha256:c573953cb25819897ecfd504bcfca49d57066d71d26cf8914d8d2a59455ad4a1
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-08-14T18:17:24Z"
    hostIP: 10.128.0.16
    phase: Running
    podIP: 10.128.0.16
    qosClass: Burstable
    startTime: "2019-08-14T18:17:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{"seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"creationTimestamp":"2019-06-18T22:22:13Z","generateName":"l7-default-backend-6f8697844f-","labels":{"k8s-app":"glbc","name":"glbc","pod-template-hash":"6f8697844f"},"name":"l7-default-backend-6f8697844f-2jxzf","namespace":"kube-system","ownerReferences":[{"apiVersion":"apps/v1","blockOwnerDeletion":true,"controller":true,"kind":"ReplicaSet","name":"l7-default-backend-6f8697844f","uid":"85b5b6e6-9217-11e9-a049-42010a8000d3"}],"resourceVersion":"770","selfLink":"/api/v1/namespaces/kube-system/pods/l7-default-backend-6f8697844f-2jxzf","uid":"85bba5ca-9217-11e9-a049-42010a8000d3"},"spec":{"containers":[{"image":"k8s.gcr.io/defaultbackend-amd64:1.5","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"initialDelaySeconds":30,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"name":"default-http-backend","ports":[{"containerPort":8080,"protocol":"TCP"}],"resources":{"limits":{"cpu":"10m","memory":"20Mi"},"requests":{"cpu":"10m","memory":"20Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","name":"default-token-pbq62","readOnly":true}]}],"dnsPolicy":"ClusterFirst","nodeName":"gke-tesg1-default-pool-ff7a1295-z7lx","priority":0,"restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"serviceAccount":"default","serviceAccountName":"default","terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":300},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":300}],"volumes":[{"name":"default-token-pbq62","secret":{"defaultMode":420,"secretName":"default-token-pbq62"}}]},"status":{"conditions":[{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:26Z","status":"True","type":"Initialized"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:52Z","status":"True","type":"Ready"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:52Z","status":"True","type":"ContainersReady"},{"lastProbeTime":null,"lastTransitionTime":"2019-06-18T22:22:26Z","status":"True","type":"PodScheduled"}],"containerStatuses":[{"containerID":"docker://abc5b2891e304c2d40bb5ca915d2d5583a1a03eb145d8e2ac959bb12e8362329","image":"k8s.gcr.io/defaultbackend-amd64:1.5","imageID":"docker-pullable://k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7","lastState":{},"name":"default-http-backend","ready":true,"restartCount":0,"state":{"running":{"startedAt":"2019-06-18T22:22:50Z"}}}],"hostIP":"10.128.0.13","phase":"Running","podIP":"10.8.0.5","qosClass":"Guaranteed","startTime":"2019-06-18T22:22:26Z"}}
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-06-18T22:22:13Z"
    generateName: l7-default-backend-6f8697844f-
    labels:
      k8s-app: glbc
      name: glbc
      pod-template-hash: 6f8697844f
    name: l7-default-backend-6f8697844f-2jxzf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: l7-default-backend-6f8697844f
      uid: 85b5b6e6-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671054"
    selfLink: /api/v1/namespaces/kube-system/pods/l7-default-backend-6f8697844f-2jxzf
    uid: 85bba5ca-9217-11e9-a049-42010a8000d3
  spec:
    containers:
    - image: k8s.gcr.io/defaultbackend-amd64:1.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /healthz
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 30
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: default-http-backend
      ports:
      - containerPort: 8080
        protocol: TCP
      resources:
        limits:
          cpu: 10m
          memory: 20Mi
        requests:
          cpu: 10m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-pbq62
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-pbq62
      secret:
        defaultMode: 420
        secretName: default-token-pbq62
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:26Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:52Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:52Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-06-18T22:22:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://abc5b2891e304c2d40bb5ca915d2d5583a1a03eb145d8e2ac959bb12e8362329
      image: k8s.gcr.io/defaultbackend-amd64:1.5
      imageID: docker-pullable://k8s.gcr.io/defaultbackend-amd64@sha256:4dc5e07c8ca4e23bddb3153737d7b8c556e5fb2f29c4558b7cd6e6df99c512c7
      lastState: {}
      name: default-http-backend
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-06-18T22:22:50Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.8.0.5
    qosClass: Guaranteed
    startTime: "2019-06-18T22:22:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2020-01-26T21:54:13Z"
    generateName: magalix-agent-6cd478675d-
    labels:
      name: magalix-agent
      pod-template-hash: 6cd478675d
    name: magalix-agent-6cd478675d-x9g7q
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: magalix-agent-6cd478675d
      uid: 15c70fb5-0fe3-11ea-a0a0-42010a80004b
    resourceVersion: "67105666"
    selfLink: /api/v1/namespaces/kube-system/pods/magalix-agent-6cd478675d-x9g7q
    uid: 64316e99-4086-11ea-a0a0-42010a80004b
  spec:
    containers:
    - args:
      - --kube-incluster
      - --gateway=ws://agent-gateway.magalix.com/
      - --source=kubelet
      - --trace-log=/agent.log
      envFrom:
      - secretRef:
          name: magalix-agent
      image: magalixcorp/agent:latest
      imagePullPolicy: Always
      name: agent
      resources:
        limits:
          cpu: "1"
          memory: 750Mi
        requests:
          cpu: 100m
          memory: 200Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: magalix-agent-token-lxznj
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: magalix-agent
    serviceAccountName: magalix-agent
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: magalix-agent-token-lxznj
      secret:
        defaultMode: 420
        secretName: magalix-agent-token-lxznj
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2020-01-26T22:14:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2020-01-28T06:21:44Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2020-01-28T06:21:44Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2020-01-26T22:14:17Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://e960f7a47cb42a49d4002fda0947e260fe7bf8946739815aa49f47cbb1cbb911
      image: magalixcorp/agent:latest
      imageID: docker-pullable://magalixcorp/agent@sha256:6b70c4adacd72552a4fd0994fb2de4cd7d0b1550cb630a6bd750507d23878d08
      lastState:
        terminated:
          containerID: docker://4c4b3ccdb4d4f652920c77cf7a3739729f7e873f4458e14f29970cd806150410
          exitCode: 137
          finishedAt: "2020-01-28T06:21:41Z"
          reason: OOMKilled
          startedAt: "2020-01-27T19:59:02Z"
      name: agent
      ready: true
      restartCount: 6
      state:
        running:
          startedAt: "2020-01-28T06:21:43Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.8.2.220
    qosClass: Burstable
    startTime: "2020-01-26T22:14:17Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
      seccomp.security.alpha.kubernetes.io/pod: docker/default
    creationTimestamp: "2019-10-13T12:28:13Z"
    generateName: metrics-server-v0.3.1-74dc55f45c-
    labels:
      k8s-app: metrics-server
      pod-template-hash: 74dc55f45c
      version: v0.3.1
    name: metrics-server-v0.3.1-74dc55f45c-qdbrd
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: metrics-server-v0.3.1-74dc55f45c
      uid: ece49c9e-edb4-11e9-9caf-42010a800177
    resourceVersion: "63701956"
    selfLink: /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-74dc55f45c-qdbrd
    uid: ece5c9ba-edb4-11e9-9caf-42010a800177
  spec:
    containers:
    - command:
      - /metrics-server
      - --metric-resolution=30s
      - --kubelet-port=10255
      - --deprecated-kubelet-completely-insecure=true
      - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
      image: k8s.gcr.io/metrics-server-amd64:v0.3.1
      imagePullPolicy: IfNotPresent
      name: metrics-server
      ports:
      - containerPort: 443
        name: https
        protocol: TCP
      resources:
        limits:
          cpu: 43m
          memory: 55Mi
        requests:
          cpu: 43m
          memory: 55Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: metrics-server-token-fvqsq
        readOnly: true
    - command:
      - /pod_nanny
      - --config-dir=/etc/config
      - --cpu=40m
      - --extra-cpu=0.5m
      - --memory=35Mi
      - --extra-memory=4Mi
      - --threshold=5
      - --deployment=metrics-server-v0.3.1
      - --container=metrics-server
      - --poll-period=300000
      - --estimator=exponential
      - --minClusterSize=5
      env:
      - name: MY_POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: MY_POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: k8s.gcr.io/addon-resizer:1.8.3
      imagePullPolicy: IfNotPresent
      name: metrics-server-nanny
      resources:
        limits:
          cpu: 100m
          memory: 300Mi
        requests:
          cpu: 5m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/config
        name: metrics-server-config-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: metrics-server-token-fvqsq
        readOnly: true
    dnsPolicy: ClusterFirst
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: metrics-server
    serviceAccountName: metrics-server
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: metrics-server-config
      name: metrics-server-config-volume
    - name: metrics-server-token-fvqsq
      secret:
        defaultMode: 420
        secretName: metrics-server-token-fvqsq
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-10-13T12:28:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://19efc38b86b6a77756611809cc43dd08dd133212c1aff14ab07c49b028cb2334
      image: k8s.gcr.io/metrics-server-amd64:v0.3.1
      imageID: docker-pullable://k8s.gcr.io/metrics-server-amd64@sha256:78938f933822856f443e6827fe5b37d6cc2f74ae888ac8b33d06fdbe5f8c658b
      lastState:
        terminated:
          containerID: docker://08179eac8149df2d01a910758344ac484b0b1231f33dd92ba0c3b9054b39e549
          exitCode: 255
          finishedAt: "2019-11-28T15:56:32Z"
          reason: Error
          startedAt: "2019-10-13T12:28:15Z"
      name: metrics-server
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:57:26Z"
    - containerID: docker://df6139c5023d46538e82d66290850413278ab07b1f136d25209c69fd9c52a62f
      image: k8s.gcr.io/addon-resizer:1.8.3
      imageID: docker-pullable://k8s.gcr.io/addon-resizer@sha256:07353f7b26327f0d933515a22b1de587b040d3d85c464ea299c1b9f242529326
      lastState:
        terminated:
          containerID: docker://3bbebb1bba75fe2506c181b2fcd14735e89cd06bca68c423e8234ff8a9e1002a
          exitCode: 255
          finishedAt: "2019-11-28T15:56:32Z"
          reason: Error
          startedAt: "2019-10-13T12:28:15Z"
      name: metrics-server-nanny
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:57:27Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.8.1.30
    qosClass: Burstable
    startTime: "2019-10-13T12:28:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-11-15T06:03:23Z"
    generateName: prometheus-to-sd-
    labels:
      controller-revision-hash: 5959586fd8
      k8s-app: prometheus-to-sd
      pod-template-generation: "3"
    name: prometheus-to-sd-6c98w
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-to-sd
      uid: 86c4ab08-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671023"
    selfLink: /api/v1/namespaces/kube-system/pods/prometheus-to-sd-6c98w
    uid: a1972ae6-076d-11ea-a0a0-42010a80004b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-zpc5
    containers:
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=container.googleapis.com/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources:
        limits:
          cpu: 3m
          memory: 20Mi
        requests:
          cpu: 1m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-lbndt
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-zpc5
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-to-sd
    serviceAccountName: prometheus-to-sd
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: prometheus-to-sd-token-lbndt
      secret:
        defaultMode: 420
        secretName: prometheus-to-sd-token-lbndt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:23Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://9c5a841f3f8a4f58b66f86a7d115c9059fee816ee9a8a77bd316bfe7404f8032
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:d4b75e5d161b3f767ec3da910805d5fed39b38b7fe2178a733d58e1aea621078
      lastState: {}
      name: prometheus-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:27Z"
    hostIP: 10.128.0.16
    phase: Running
    podIP: 10.128.0.16
    qosClass: Burstable
    startTime: "2019-11-15T06:03:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-11-15T06:03:42Z"
    generateName: prometheus-to-sd-
    labels:
      controller-revision-hash: 5959586fd8
      k8s-app: prometheus-to-sd
      pod-template-generation: "3"
    name: prometheus-to-sd-bmjnl
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-to-sd
      uid: 86c4ab08-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671041"
    selfLink: /api/v1/namespaces/kube-system/pods/prometheus-to-sd-bmjnl
    uid: ad376beb-076d-11ea-a0a0-42010a80004b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-z7lx
    containers:
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=container.googleapis.com/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources:
        limits:
          cpu: 3m
          memory: 20Mi
        requests:
          cpu: 1m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-lbndt
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-z7lx
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-to-sd
    serviceAccountName: prometheus-to-sd
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: prometheus-to-sd-token-lbndt
      secret:
        defaultMode: 420
        secretName: prometheus-to-sd-token-lbndt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:47Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:47Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://947702f889461426e5b96ad186e5a65a3d9b7fac8c23ae0307fa170ba214950a
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:d4b75e5d161b3f767ec3da910805d5fed39b38b7fe2178a733d58e1aea621078
      lastState: {}
      name: prometheus-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:47Z"
    hostIP: 10.128.0.13
    phase: Running
    podIP: 10.128.0.13
    qosClass: Burstable
    startTime: "2019-11-15T06:03:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-11-15T06:03:49Z"
    generateName: prometheus-to-sd-
    labels:
      controller-revision-hash: 5959586fd8
      k8s-app: prometheus-to-sd
      pod-template-generation: "3"
    name: prometheus-to-sd-q54j2
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-to-sd
      uid: 86c4ab08-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671145"
    selfLink: /api/v1/namespaces/kube-system/pods/prometheus-to-sd-q54j2
    uid: b10ae40a-076d-11ea-a0a0-42010a80004b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-nvv4
    containers:
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=container.googleapis.com/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources:
        limits:
          cpu: 3m
          memory: 20Mi
        requests:
          cpu: 1m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-lbndt
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-nvv4
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-to-sd
    serviceAccountName: prometheus-to-sd
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: prometheus-to-sd-token-lbndt
      secret:
        defaultMode: 420
        secretName: prometheus-to-sd-token-lbndt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:49Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:49Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://5aea2fe4e7c22a0fe50128eb6110b605fdd91e255216cf644691a4567941d247
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:d4b75e5d161b3f767ec3da910805d5fed39b38b7fe2178a733d58e1aea621078
      lastState: {}
      name: prometheus-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:53Z"
    hostIP: 10.128.0.15
    phase: Running
    podIP: 10.128.0.15
    qosClass: Burstable
    startTime: "2019-11-15T06:03:49Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-11-15T06:03:36Z"
    generateName: prometheus-to-sd-
    labels:
      controller-revision-hash: 5959586fd8
      k8s-app: prometheus-to-sd
      pod-template-generation: "3"
    name: prometheus-to-sd-xrqf8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-to-sd
      uid: 86c4ab08-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671122"
    selfLink: /api/v1/namespaces/kube-system/pods/prometheus-to-sd-xrqf8
    uid: a969c39e-076d-11ea-a0a0-42010a80004b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-dmtd
    containers:
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=container.googleapis.com/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources:
        limits:
          cpu: 3m
          memory: 20Mi
        requests:
          cpu: 1m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-lbndt
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-dmtd
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-to-sd
    serviceAccountName: prometheus-to-sd
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: prometheus-to-sd-token-lbndt
      secret:
        defaultMode: 420
        secretName: prometheus-to-sd-token-lbndt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:36Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:40Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:40Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:36Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://ebeb4334e79bdc65eb5cf37e6192c3680544e013b9b27e20ff9c4aaac3f40f5e
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:d4b75e5d161b3f767ec3da910805d5fed39b38b7fe2178a733d58e1aea621078
      lastState: {}
      name: prometheus-to-sd
      ready: true
      restartCount: 0
      state:
        running:
          startedAt: "2019-11-15T06:03:40Z"
    hostIP: 10.128.0.17
    phase: Running
    podIP: 10.128.0.17
    qosClass: Burstable
    startTime: "2019-11-15T06:03:36Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2019-11-15T06:03:29Z"
    generateName: prometheus-to-sd-
    labels:
      controller-revision-hash: 5959586fd8
      k8s-app: prometheus-to-sd
      pod-template-generation: "3"
    name: prometheus-to-sd-z99rw
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: prometheus-to-sd
      uid: 86c4ab08-9217-11e9-a049-42010a8000d3
    resourceVersion: "63701959"
    selfLink: /api/v1/namespaces/kube-system/pods/prometheus-to-sd-z99rw
    uid: a598cfa8-076d-11ea-a0a0-42010a80004b
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - gke-tesg1-default-pool-ff7a1295-7kwg
    containers:
    - command:
      - /monitor
      - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
      - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total
      - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
      - --stackdriver-prefix=container.googleapis.com/internal/nodes
      - --api-override=https://monitoring.googleapis.com/
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imagePullPolicy: IfNotPresent
      name: prometheus-to-sd
      resources:
        limits:
          cpu: 3m
          memory: 20Mi
        requests:
          cpu: 1m
          memory: 20Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: prometheus-to-sd-token-lbndt
        readOnly: true
    dnsPolicy: ClusterFirst
    hostNetwork: true
    nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
    nodeSelector:
      beta.kubernetes.io/os: linux
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: prometheus-to-sd
    serviceAccountName: prometheus-to-sd
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: prometheus-to-sd-token-lbndt
      secret:
        defaultMode: 420
        secretName: prometheus-to-sd-token-lbndt
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2019-11-28T15:57:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2019-11-15T06:03:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: docker://99b953fb3d0a2b6418cfa1cacc9acb4dbbcbe4302018c10d2bac1b05976d1b04
      image: k8s.gcr.io/prometheus-to-sd:v0.8.1
      imageID: docker-pullable://k8s.gcr.io/prometheus-to-sd@sha256:d4b75e5d161b3f767ec3da910805d5fed39b38b7fe2178a733d58e1aea621078
      lastState:
        terminated:
          containerID: docker://fc5fb0ec5e954360fe15777d5d1ada47015655b05d342b6ca76469a6b8f32019
          exitCode: 255
          finishedAt: "2019-11-28T15:56:32Z"
          reason: Error
          startedAt: "2019-11-15T06:03:34Z"
      name: prometheus-to-sd
      ready: true
      restartCount: 1
      state:
        running:
          startedAt: "2019-11-28T15:57:13Z"
    hostIP: 10.128.0.14
    phase: Running
    podIP: 10.128.0.14
    qosClass: Burstable
    startTime: "2019-11-15T06:03:29Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-07-24T20:32:15Z","labels":{"app":"guestbook","tier":"frontend"},"name":"frontend","namespace":"default","resourceVersion":"9417148","selfLink":"/api/v1/namespaces/default/services/frontend","uid":"1fc62248-ae52-11e9-98f3-42010a8000c4"},"spec":{"clusterIP":"10.0.8.96","externalTrafficPolicy":"Cluster","ports":[{"nodePort":32297,"port":80,"protocol":"TCP","targetPort":80}],"selector":{"app":"guestbook","tier":"frontend"},"sessionAffinity":"None","type":"LoadBalancer"},"status":{"loadBalancer":{"ingress":[{"ip":"34.66.209.227"}]}}}
    creationTimestamp: "2019-07-24T20:32:15Z"
    labels:
      app: guestbook
      tier: frontend
    name: frontend
    namespace: default
    resourceVersion: "24658624"
    selfLink: /api/v1/namespaces/default/services/frontend
    uid: 1fc62248-ae52-11e9-98f3-42010a8000c4
  spec:
    clusterIP: 10.0.8.96
    externalTrafficPolicy: Cluster
    ports:
    - nodePort: 32297
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: guestbook
      tier: frontend
    sessionAffinity: None
    type: LoadBalancer
  status:
    loadBalancer:
      ingress:
      - ip: 34.66.209.227
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:21:51Z","labels":{"component":"apiserver","provider":"kubernetes"},"name":"kubernetes","namespace":"default","resourceVersion":"16","selfLink":"/api/v1/namespaces/default/services/kubernetes","uid":"785f5181-9217-11e9-a049-42010a8000d3"},"spec":{"clusterIP":"10.0.0.1","ports":[{"name":"https","port":443,"protocol":"TCP","targetPort":443}],"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-06-18T22:21:51Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "24658625"
    selfLink: /api/v1/namespaces/default/services/kubernetes
    uid: 785f5181-9217-11e9-a049-42010a8000d3
  spec:
    clusterIP: 10.0.0.1
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-07-24T20:32:15Z","labels":{"app":"redis","role":"master","tier":"backend"},"name":"redis-master","namespace":"default","resourceVersion":"9416863","selfLink":"/api/v1/namespaces/default/services/redis-master","uid":"1fb431db-ae52-11e9-98f3-42010a8000c4"},"spec":{"clusterIP":"10.0.5.82","ports":[{"port":6379,"protocol":"TCP","targetPort":6379}],"selector":{"app":"redis","role":"master","tier":"backend"},"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-07-24T20:32:15Z"
    labels:
      app: redis
      role: master
      tier: backend
    name: redis-master
    namespace: default
    resourceVersion: "24658626"
    selfLink: /api/v1/namespaces/default/services/redis-master
    uid: 1fb431db-ae52-11e9-98f3-42010a8000c4
  spec:
    clusterIP: 10.0.5.82
    ports:
    - port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: redis
      role: master
      tier: backend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-09-04T21:24:52Z","labels":{"app":"redis-evict","role":"master","tier":"backend"},"name":"redis-master-evict","namespace":"default","resourceVersion":"20774605","selfLink":"/api/v1/namespaces/default/services/redis-master-evict","uid":"6ee8b5c7-cf5a-11e9-98f3-42010a8000c4"},"spec":{"clusterIP":"10.0.13.148","ports":[{"port":6379,"protocol":"TCP","targetPort":6379}],"selector":{"app":"redis-evict","role":"master","tier":"backend"},"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-09-04T21:24:52Z"
    labels:
      app: redis-evict
      role: master
      tier: backend
    name: redis-master-evict
    namespace: default
    resourceVersion: "24658629"
    selfLink: /api/v1/namespaces/default/services/redis-master-evict
    uid: 6ee8b5c7-cf5a-11e9-98f3-42010a8000c4
  spec:
    clusterIP: 10.0.13.148
    ports:
    - port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: redis-evict
      role: master
      tier: backend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-07-24T20:32:15Z","labels":{"app":"redis","role":"slave","tier":"backend"},"name":"redis-slave","namespace":"default","resourceVersion":"9416869","selfLink":"/api/v1/namespaces/default/services/redis-slave","uid":"1fb9a12c-ae52-11e9-98f3-42010a8000c4"},"spec":{"clusterIP":"10.0.15.103","ports":[{"port":6379,"protocol":"TCP","targetPort":6379}],"selector":{"app":"redis","role":"slave","tier":"backend"},"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-07-24T20:32:15Z"
    labels:
      app: redis
      role: slave
      tier: backend
    name: redis-slave
    namespace: default
    resourceVersion: "24658631"
    selfLink: /api/v1/namespaces/default/services/redis-slave
    uid: 1fb9a12c-ae52-11e9-98f3-42010a8000c4
  spec:
    clusterIP: 10.0.15.103
    ports:
    - port: 6379
      protocol: TCP
      targetPort: 6379
    selector:
      app: redis
      role: slave
      tier: backend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-07-24T20:32:15Z","labels":{"app":"redis","role":"slave-unlimit","tier":"backend"},"name":"redis-slave-unlimit","namespace":"default","resourceVersion":"9416896","selfLink":"/api/v1/namespaces/default/services/redis-slave-unlimit","uid":"1fd2c61c-ae52-11e9-98f3-42010a8000c4"},"spec":{"clusterIP":"10.0.2.135","ports":[{"port":6366,"protocol":"TCP","targetPort":6366}],"selector":{"app":"redis","role":"slave-unlimit","tier":"backend"},"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-07-24T20:32:15Z"
    labels:
      app: redis
      role: slave-unlimit
      tier: backend
    name: redis-slave-unlimit
    namespace: default
    resourceVersion: "24658633"
    selfLink: /api/v1/namespaces/default/services/redis-slave-unlimit
    uid: 1fd2c61c-ae52-11e9-98f3-42010a8000c4
  spec:
    clusterIP: 10.0.2.135
    ports:
    - port: 6366
      protocol: TCP
      targetPort: 6366
    selector:
      app: redis
      role: slave-unlimit
      tier: backend
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:22:13Z","labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"glbc","kubernetes.io/cluster-service":"true","kubernetes.io/name":"GLBCDefaultBackend"},"name":"default-http-backend","namespace":"kube-system","resourceVersion":"301","selfLink":"/api/v1/namespaces/kube-system/services/default-http-backend","uid":"85c65302-9217-11e9-a049-42010a8000d3"},"spec":{"clusterIP":"10.0.13.226","externalTrafficPolicy":"Cluster","ports":[{"name":"http","nodePort":32064,"port":80,"protocol":"TCP","targetPort":8080}],"selector":{"k8s-app":"glbc"},"sessionAffinity":"None","type":"NodePort"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-06-18T22:22:13Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: glbc
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: GLBCDefaultBackend
    name: default-http-backend
    namespace: kube-system
    resourceVersion: "24658634"
    selfLink: /api/v1/namespaces/kube-system/services/default-http-backend
    uid: 85c65302-9217-11e9-a049-42010a8000d3
  spec:
    clusterIP: 10.0.13.226
    externalTrafficPolicy: Cluster
    ports:
    - name: http
      nodePort: 32064
      port: 80
      protocol: TCP
      targetPort: 8080
    selector:
      k8s-app: glbc
    sessionAffinity: None
    type: NodePort
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:22:14Z","labels":{"addonmanager.kubernetes.io/mode":"Reconcile","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Heapster"},"name":"heapster","namespace":"kube-system","resourceVersion":"320","selfLink":"/api/v1/namespaces/kube-system/services/heapster","uid":"86138bdd-9217-11e9-a049-42010a8000d3"},"spec":{"clusterIP":"10.0.6.216","ports":[{"port":80,"protocol":"TCP","targetPort":8082}],"selector":{"k8s-app":"heapster"},"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-06-18T22:22:14Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Heapster
    name: heapster
    namespace: kube-system
    resourceVersion: "24658636"
    selfLink: /api/v1/namespaces/kube-system/services/heapster
    uid: 86138bdd-9217-11e9-a049-42010a8000d3
  spec:
    clusterIP: 10.0.6.216
    ports:
    - port: 80
      protocol: TCP
      targetPort: 8082
    selector:
      k8s-app: heapster
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:22:14Z","labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns","kubernetes.io/cluster-service":"true","kubernetes.io/name":"KubeDNS"},"name":"kube-dns","namespace":"kube-system","resourceVersion":"324","selfLink":"/api/v1/namespaces/kube-system/services/kube-dns","uid":"861c8257-9217-11e9-a049-42010a8000d3"},"spec":{"clusterIP":"10.0.0.10","ports":[{"name":"dns","port":53,"protocol":"UDP","targetPort":53},{"name":"dns-tcp","port":53,"protocol":"TCP","targetPort":53}],"selector":{"k8s-app":"kube-dns"},"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-06-18T22:22:14Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: KubeDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "24658638"
    selfLink: /api/v1/namespaces/kube-system/services/kube-dns
    uid: 861c8257-9217-11e9-a049-42010a8000d3
  spec:
    clusterIP: 10.0.0.10
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:22:17Z","labels":{"addonmanager.kubernetes.io/mode":"Reconcile","kubernetes.io/cluster-service":"true","kubernetes.io/name":"Metrics-server"},"name":"metrics-server","namespace":"kube-system","resourceVersion":"404","selfLink":"/api/v1/namespaces/kube-system/services/metrics-server","uid":"87b5c60f-9217-11e9-a049-42010a8000d3"},"spec":{"clusterIP":"10.0.9.3","ports":[{"port":443,"protocol":"TCP","targetPort":"https"}],"selector":{"k8s-app":"metrics-server"},"sessionAffinity":"None","type":"ClusterIP"},"status":{"loadBalancer":{}}}
    creationTimestamp: "2019-06-18T22:22:17Z"
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: Metrics-server
    name: metrics-server
    namespace: kube-system
    resourceVersion: "24658640"
    selfLink: /api/v1/namespaces/kube-system/services/metrics-server
    uid: 87b5c60f-9217-11e9-a049-42010a8000d3
  spec:
    clusterIP: 10.0.9.3
    ports:
    - port: 443
      protocol: TCP
      targetPort: https
    selector:
      k8s-app: metrics-server
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-gcp","kubernetes.io/cluster-service":"true","version":"v3.2.0"},"name":"fluentd-gcp-v3.2.0","namespace":"kube-system"},"spec":{"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"fluentd-gcp","kubernetes.io/cluster-service":"true","version":"v3.2.0"}},"spec":{"containers":[{"env":[{"name":"NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"STACKDRIVER_METADATA_AGENT_URL","value":"http://$(NODE_NAME):8799"}],"image":"gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8","livenessProbe":{"exec":{"command":["/bin/sh","-c","LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then\n  exit 1;\nfi; touch -d \"${STUCK_THRESHOLD_SECONDS} seconds ago\" /tmp/marker-stuck; if [[ -z \"$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)\" ]]; then\n  rm -rf /var/log/fluentd-buffers;\n  exit 1;\nfi; touch -d \"${LIVENESS_THRESHOLD_SECONDS} seconds ago\" /tmp/marker-liveness; if [[ -z \"$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)\" ]]; then\n  exit 1;\nfi;\n"]},"initialDelaySeconds":600,"periodSeconds":60},"name":"fluentd-gcp","volumeMounts":[{"mountPath":"/var/log","name":"varlog"},{"mountPath":"/var/lib/docker/containers","name":"varlibdockercontainers","readOnly":true},{"mountPath":"/etc/google-fluentd/config.d","name":"config-volume"}]},{"command":["/monitor","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prometheus-to-sd-exporter"}],"dnsPolicy":"Default","hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/fluentd-ds-ready":"true"},"priorityClassName":"system-node-critical","serviceAccountName":"fluentd-gcp","terminationGracePeriodSeconds":60,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/var/log"},"name":"varlog"},{"hostPath":{"path":"/var/lib/docker/containers"},"name":"varlibdockercontainers"},{"configMap":{"name":"fluentd-gcp-config-old-v1.2.5"},"name":"config-volume"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2019-06-18T22:22:15Z"
    generation: 3
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-gcp
      kubernetes.io/cluster-service: "true"
      version: v3.2.0
    name: fluentd-gcp-v3.2.0
    namespace: kube-system
    resourceVersion: "63701974"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/fluentd-gcp-v3.2.0
    uid: 86ae80c3-9217-11e9-a049-42010a8000d3
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: fluentd-gcp
        kubernetes.io/cluster-service: "true"
        version: v3.2.0
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp
          kubernetes.io/cluster-service: "true"
          version: v3.2.0
      spec:
        containers:
        - env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: STACKDRIVER_METADATA_AGENT_URL
            value: http://$(NODE_NAME):8799
          image: gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - |
                LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300}; STUCK_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-900}; if [ ! -e /var/log/fluentd-buffers ]; then
                  exit 1;
                fi; touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-stuck -print -quit)" ]]; then
                  rm -rf /var/log/fluentd-buffers;
                  exit 1;
                fi; touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness; if [[ -z "$(find /var/log/fluentd-buffers -type f -newer /tmp/marker-liveness -print -quit)" ]]; then
                  exit 1;
                fi;
            failureThreshold: 3
            initialDelaySeconds: 600
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 1
          name: fluentd-gcp
          resources:
            limits:
              cpu: "1"
              memory: 500Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/log
            name: varlog
          - mountPath: /var/lib/docker/containers
            name: varlibdockercontainers
            readOnly: true
          - mountPath: /etc/google-fluentd/config.d
            name: config-volume
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=fluentd:http://localhost:24231?whitelisted=stackdriver_successful_requests_count,stackdriver_failed_requests_count,stackdriver_ingested_entries_count,stackdriver_dropped_entries_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/fluentd-ds-ready: "true"
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp
        serviceAccountName: fluentd-gcp
        terminationGracePeriodSeconds: 60
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /var/log
            type: ""
          name: varlog
        - hostPath:
            path: /var/lib/docker/containers
            type: ""
          name: varlibdockercontainers
        - configMap:
            defaultMode: 420
            name: fluentd-gcp-config-old-v1.2.5
          name: config-volume
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 5
    desiredNumberScheduled: 5
    numberAvailable: 5
    numberMisscheduled: 0
    numberReady: 5
    observedGeneration: 3
    updatedNumberScheduled: 5
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"metadata-proxy","kubernetes.io/cluster-service":"true","version":"v0.1"},"name":"metadata-proxy-v0.1","namespace":"kube-system"},"spec":{"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"metadata-proxy","kubernetes.io/cluster-service":"true","version":"v0.1"}},"spec":{"containers":[{"image":"k8s.gcr.io/metadata-proxy:v0.1.10","name":"metadata-proxy","resources":{"limits":{"cpu":"30m","memory":"25Mi"},"requests":{"cpu":"30m","memory":"25Mi"}},"securityContext":{"privileged":true}},{"command":["/monitor","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--source=metadata_proxy:http://127.0.0.1:989?whitelisted=request_count","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prometheus-to-sd-exporter","resources":{"limits":{"cpu":"2m","memory":"20Mi"},"requests":{"cpu":"2m","memory":"20Mi"}}}],"dnsPolicy":"Default","hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/metadata-proxy-ready":"true"},"priorityClassName":"system-node-critical","serviceAccountName":"metadata-proxy","terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2019-06-18T22:22:15Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: metadata-proxy
      kubernetes.io/cluster-service: "true"
      version: v0.1
    name: metadata-proxy-v0.1
    namespace: kube-system
    resourceVersion: "383"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/metadata-proxy-v0.1
    uid: 86e5032f-9217-11e9-a049-42010a8000d3
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metadata-proxy
        kubernetes.io/cluster-service: "true"
        version: v0.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: metadata-proxy
          kubernetes.io/cluster-service: "true"
          version: v0.1
      spec:
        containers:
        - image: k8s.gcr.io/metadata-proxy:v0.1.10
          imagePullPolicy: IfNotPresent
          name: metadata-proxy
          resources:
            limits:
              cpu: 30m
              memory: 25Mi
            requests:
              cpu: 30m
              memory: 25Mi
          securityContext:
            privileged: true
            procMount: Default
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=metadata_proxy:http://127.0.0.1:989?whitelisted=request_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources:
            limits:
              cpu: 2m
              memory: 20Mi
            requests:
              cpu: 2m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/metadata-proxy-ready: "true"
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metadata-proxy
        serviceAccountName: metadata-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"nvidia-gpu-device-plugin"},"name":"nvidia-gpu-device-plugin","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"nvidia-gpu-device-plugin"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"nvidia-gpu-device-plugin"}},"spec":{"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"cloud.google.com/gke-accelerator","operator":"Exists"}]}]}}},"containers":[{"command":["/usr/bin/nvidia-gpu-device-plugin","-logtostderr"],"image":"k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d","name":"nvidia-gpu-device-plugin","resources":{"limits":{"cpu":"50m","memory":"10Mi"},"requests":{"cpu":"50m","memory":"10Mi"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/device-plugin","name":"device-plugin"},{"mountPath":"/dev","name":"dev"}]}],"priorityClassName":"system-node-critical","tolerations":[{"effect":"NoExecute","operator":"Exists"},{"effect":"NoSchedule","operator":"Exists"}],"volumes":[{"hostPath":{"path":"/var/lib/kubelet/device-plugins"},"name":"device-plugin"},{"hostPath":{"path":"/dev"},"name":"dev"}]}},"updateStrategy":{"type":"RollingUpdate"}}}
    creationTimestamp: "2019-06-18T22:22:26Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: nvidia-gpu-device-plugin
    name: nvidia-gpu-device-plugin
    namespace: kube-system
    resourceVersion: "455"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/nvidia-gpu-device-plugin
    uid: 8d6c9472-9217-11e9-a049-42010a8000d3
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: nvidia-gpu-device-plugin
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: nvidia-gpu-device-plugin
      spec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: cloud.google.com/gke-accelerator
                  operator: Exists
        containers:
        - command:
          - /usr/bin/nvidia-gpu-device-plugin
          - -logtostderr
          image: k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d
          imagePullPolicy: IfNotPresent
          name: nvidia-gpu-device-plugin
          resources:
            limits:
              cpu: 50m
              memory: 10Mi
            requests:
              cpu: 50m
              memory: 10Mi
          securityContext:
            privileged: true
            procMount: Default
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /device-plugin
            name: device-plugin
          - mountPath: /dev
            name: dev
        dnsPolicy: ClusterFirst
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          operator: Exists
        - effect: NoSchedule
          operator: Exists
        volumes:
        - hostPath:
            path: /var/lib/kubelet/device-plugins
            type: ""
          name: device-plugin
        - hostPath:
            path: /dev
            type: ""
          name: dev
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 0
    desiredNumberScheduled: 0
    numberMisscheduled: 0
    numberReady: 0
    observedGeneration: 1
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","kubernetes.io/cluster-service":"true"},"name":"prometheus-to-sd","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"prometheus-to-sd"}},"template":{"metadata":{"labels":{"k8s-app":"prometheus-to-sd"}},"spec":{"containers":[{"command":["/monitor","--source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds\u0026metricsPrefix=container.googleapis.com/internal/addons","--source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total","--source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total\u0026podIdLabel=pod\u0026namespaceIdLabel=namespace\u0026containerNameLabel=container","--stackdriver-prefix=container.googleapis.com/internal/nodes","--api-override=https://monitoring.googleapis.com/"],"image":"k8s.gcr.io/prometheus-to-sd:v0.8.1","imagePullPolicy":"IfNotPresent","name":"prometheus-to-sd","resources":{"limits":{"cpu":"3m","memory":"20Mi"},"requests":{"cpu":"1m","memory":"20Mi"}}}],"hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/os":"linux"},"priorityClassName":"system-node-critical","serviceAccountName":"prometheus-to-sd","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}]}}}}
    creationTimestamp: "2019-06-18T22:22:15Z"
    generation: 3
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      kubernetes.io/cluster-service: "true"
    name: prometheus-to-sd
    namespace: kube-system
    resourceVersion: "63701965"
    selfLink: /apis/apps/v1/namespaces/kube-system/daemonsets/prometheus-to-sd
    uid: 86c4ab08-9217-11e9-a049-42010a8000d3
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: prometheus-to-sd
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: prometheus-to-sd
      spec:
        containers:
        - command:
          - /monitor
          - --source=kube-proxy:http://localhost:10249?whitelisted=sync_proxy_rules_latency_microseconds&metricsPrefix=container.googleapis.com/internal/addons
          - --source=kubelet:http://localhost:10255?whitelisted=docker_operations,docker_operations_errors,runtime_operations,runtime_operations_errors,runtime_operations_latency_microseconds,pleg_relist_latency_microseconds,pod_start_latency_microseconds,rest_client_requests_total,storage_operation_duration_seconds,storage_operation_errors_total,run_podsandbox_duration_seconds,run_podsandbox_errors_total
          - --source=kubelet:http://localhost:10255/metrics/probes?whitelisted=prober_probe_total&podIdLabel=pod&namespaceIdLabel=namespace&containerNameLabel=container
          - --stackdriver-prefix=container.googleapis.com/internal/nodes
          - --api-override=https://monitoring.googleapis.com/
          image: k8s.gcr.io/prometheus-to-sd:v0.8.1
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd
          resources:
            limits:
              cpu: 3m
              memory: 20Mi
            requests:
              cpu: 1m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: prometheus-to-sd
        serviceAccountName: prometheus-to-sd
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 5
    desiredNumberScheduled: 5
    numberAvailable: 5
    numberMisscheduled: 0
    numberReady: 5
    observedGeneration: 3
    updatedNumberScheduled: 5
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"1"},"creationTimestamp":"2020-01-16T03:46:33Z","generation":1,"labels":{"app":"redis","role":"master","tier":"backend"},"name":"redis-master","namespace":"default","resourceVersion":"62208493","selfLink":"/apis/extensions/v1beta1/namespaces/default/deployments/redis-master","uid":"c99ad6dd-3812-11ea-a0a0-42010a80004b"},"spec":{"progressDeadlineSeconds":600,"replicas":4,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"redis","role":"master","tier":"backend"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"redis","role":"master","tier":"backend"}},"spec":{"containers":[{"image":"k8s.gcr.io/redis:e2e","imagePullPolicy":"IfNotPresent","name":"master","ports":[{"containerPort":6379,"protocol":"TCP"}],"resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":4,"conditions":[{"lastTransitionTime":"2020-01-16T03:46:33Z","lastUpdateTime":"2020-01-16T03:46:33Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"},{"lastTransitionTime":"2020-01-16T03:46:33Z","lastUpdateTime":"2020-01-16T03:46:33Z","message":"ReplicaSet \"redis-master-57fc67768d\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"}],"observedGeneration":1,"readyReplicas":4,"replicas":4,"updatedReplicas":4}}
    creationTimestamp: "2020-01-16T03:48:08Z"
    generation: 1
    labels:
      app: redis
      role: master
      tier: backend
    name: redis-master
    namespace: default
    resourceVersion: "63701988"
    selfLink: /apis/apps/v1/namespaces/default/deployments/redis-master
    uid: 02791b8d-3813-11ea-a0a0-42010a80004b
  spec:
    progressDeadlineSeconds: 600
    replicas: 4
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: redis
        role: master
        tier: backend
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          role: master
          tier: backend
      spec:
        containers:
        - image: k8s.gcr.io/redis:e2e
          imagePullPolicy: IfNotPresent
          name: master
          ports:
          - containerPort: 6379
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 4
    conditions:
    - lastTransitionTime: "2020-01-16T03:48:08Z"
      lastUpdateTime: "2020-01-16T03:48:08Z"
      message: ReplicaSet "redis-master-57fc67768d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-20T18:57:59Z"
      lastUpdateTime: "2020-01-20T18:57:59Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 4
    replicas: 4
    updatedReplicas: 4
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"1"},"creationTimestamp":"2019-09-04T21:24:52Z","generation":1,"name":"redis-master-evict","namespace":"default","resourceVersion":"54312180","selfLink":"/apis/extensions/v1beta1/namespaces/default/deployments/redis-master-evict","uid":"6ef03f30-cf5a-11e9-98f3-42010a8000c4"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"redis-evict","role":"master","tier":"backend"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"redis-evict","role":"master","tier":"backend"}},"spec":{"containers":[{"image":"k8s.gcr.io/redis:e2e","imagePullPolicy":"IfNotPresent","name":"master","ports":[{"containerPort":6379,"protocol":"TCP"}],"resources":{"requests":{"cpu":"500m","memory":"1700280Ki"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":1,"conditions":[{"lastTransitionTime":"2019-09-04T21:24:52Z","lastUpdateTime":"2019-09-04T21:24:53Z","message":"ReplicaSet \"redis-master-evict-fd97bd94b\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"},{"lastTransitionTime":"2019-12-22T16:32:53Z","lastUpdateTime":"2019-12-22T16:32:53Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"}],"observedGeneration":1,"readyReplicas":1,"replicas":1,"updatedReplicas":1}}
    creationTimestamp: "2019-09-04T21:24:52Z"
    generation: 2
    labels:
      app: redis-evict
      role: master
      tier: backend
    name: redis-master-evict
    namespace: default
    resourceVersion: "66995078"
    selfLink: /apis/apps/v1/namespaces/default/deployments/redis-master-evict
    uid: 6ef03f30-cf5a-11e9-98f3-42010a8000c4
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: redis-evict
        role: master
        tier: backend
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis-evict
          role: master
          tier: backend
      spec:
        containers:
        - image: k8s.gcr.io/redis:e2e
          imagePullPolicy: IfNotPresent
          name: master
          ports:
          - containerPort: 6379
            protocol: TCP
          resources:
            requests:
              cpu: 500m
              memory: 1700280Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-09-04T21:24:52Z"
      lastUpdateTime: "2019-09-04T21:24:53Z"
      message: ReplicaSet "redis-master-evict-fd97bd94b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-27T21:59:39Z"
      lastUpdateTime: "2020-01-27T21:59:39Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"1"},"creationTimestamp":"2019-12-27T17:37:35Z","generation":1,"labels":{"app":"scheduler-pod-officer"},"name":"scheduler-pod-officer","namespace":"default","resourceVersion":"55941998","selfLink":"/apis/extensions/v1beta1/namespaces/default/deployments/scheduler-pod-officer","uid":"91c33014-28cf-11ea-a0a0-42010a80004b"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"scheduler-pod-officer"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"scheduler-pod-officer"}},"spec":{"containers":[{"image":"gcr.io/closercriticalhop/my-kube-scheduler:1.0","imagePullPolicy":"Always","name":"scheduler-pod-officer","resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"serviceAccount":"scheduler-pod-officer","serviceAccountName":"scheduler-pod-officer","terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":1,"conditions":[{"lastTransitionTime":"2019-12-27T17:38:06Z","lastUpdateTime":"2019-12-27T17:38:06Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"},{"lastTransitionTime":"2019-12-27T17:37:35Z","lastUpdateTime":"2019-12-27T17:38:06Z","message":"ReplicaSet \"scheduler-pod-officer-6ccf79f787\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"}],"observedGeneration":1,"readyReplicas":1,"replicas":1,"updatedReplicas":1}}
    creationTimestamp: "2019-12-27T17:37:35Z"
    generation: 2
    labels:
      app: scheduler-pod-officer
    name: scheduler-pod-officer
    namespace: default
    resourceVersion: "63671021"
    selfLink: /apis/apps/v1/namespaces/default/deployments/scheduler-pod-officer
    uid: 91c33014-28cf-11ea-a0a0-42010a80004b
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: scheduler-pod-officer
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: scheduler-pod-officer
      spec:
        containers:
        - image: gcr.io/closercriticalhop/my-kube-scheduler:1.0
          imagePullPolicy: Always
          name: scheduler-pod-officer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: scheduler-pod-officer
        serviceAccountName: scheduler-pod-officer
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-12-27T17:37:35Z"
      lastUpdateTime: "2019-12-27T17:38:06Z"
      message: ReplicaSet "scheduler-pod-officer-6ccf79f787" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-20T16:40:23Z"
      lastUpdateTime: "2020-01-20T16:40:23Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "9"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"9"},"creationTimestamp":"2019-12-29T20:09:56Z","generation":1,"labels":{"app":"sleep-default-sched","tier":"gggg"},"name":"sleep-default-sched","namespace":"default","resourceVersion":"56621775","selfLink":"/apis/extensions/v1beta1/namespaces/default/deployments/sleep-default-sched","uid":"2ed20653-2a77-11ea-a0a0-42010a80004b"},"spec":{"progressDeadlineSeconds":600,"replicas":2,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"sleep-default-sched"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"sleep-default-sched","ggg":"ggg","hhh":"hhh1"}},"spec":{"containers":[{"command":["/bin/sleep","infinity"],"image":"tutum/curl","imagePullPolicy":"IfNotPresent","name":"sleep","resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":2,"conditions":[{"lastTransitionTime":"2019-12-29T20:09:57Z","lastUpdateTime":"2019-12-29T20:09:57Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"},{"lastTransitionTime":"2019-12-29T20:09:56Z","lastUpdateTime":"2019-12-29T20:09:57Z","message":"ReplicaSet \"sleep-default-sched-78f7b9dffd\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"}],"observedGeneration":1,"readyReplicas":2,"replicas":2,"updatedReplicas":2}}
    creationTimestamp: "2019-12-29T20:09:56Z"
    generation: 2
    labels:
      app: sleep-default-sched
      tier: gggg
    name: sleep-default-sched
    namespace: default
    resourceVersion: "65677294"
    selfLink: /apis/apps/v1/namespaces/default/deployments/sleep-default-sched
    uid: 2ed20653-2a77-11ea-a0a0-42010a80004b
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: sleep-default-sched
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: sleep-default-sched
          ggg: ggg
          hhh: hhh1
      spec:
        containers:
        - command:
          - /bin/sleep
          - infinity
          image: tutum/curl
          imagePullPolicy: IfNotPresent
          name: sleep
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2019-12-29T20:09:56Z"
      lastUpdateTime: "2019-12-29T20:09:57Z"
      message: ReplicaSet "sleep-default-sched-78f7b9dffd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-26T21:54:15Z"
      lastUpdateTime: "2020-01-26T21:54:15Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"event-exporter","kubernetes.io/cluster-service":"true","version":"v0.2.4"},"name":"event-exporter-v0.2.4","namespace":"kube-system"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"k8s-app":"event-exporter","version":"v0.2.4"}},"spec":{"containers":[{"command":["/event-exporter","-sink-opts=-stackdriver-resource-model=old"],"image":"k8s.gcr.io/event-exporter:v0.2.4","name":"event-exporter"},{"command":["/monitor","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prometheus-to-sd-exporter"}],"serviceAccountName":"event-exporter-sa","terminationGracePeriodSeconds":30,"volumes":[{"hostPath":{"path":"/etc/ssl/certs"},"name":"ssl-certs"}]}}}}
    creationTimestamp: "2019-06-18T22:22:15Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: event-exporter
      kubernetes.io/cluster-service: "true"
      version: v0.2.4
    name: event-exporter-v0.2.4
    namespace: kube-system
    resourceVersion: "63671040"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/event-exporter-v0.2.4
    uid: 868d3e25-9217-11e9-a049-42010a8000d3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 2
    selector:
      matchLabels:
        k8s-app: event-exporter
        version: v0.2.4
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: event-exporter
          version: v0.2.4
      spec:
        containers:
        - command:
          - /event-exporter
          - -sink-opts=-stackdriver-resource-model=old
          image: k8s.gcr.io/event-exporter:v0.2.4
          imagePullPolicy: IfNotPresent
          name: event-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: event-exporter-sa
        serviceAccountName: event-exporter-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /etc/ssl/certs
            type: ""
          name: ssl-certs
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-06-18T22:22:15Z"
      lastUpdateTime: "2019-06-18T22:22:53Z"
      message: ReplicaSet "event-exporter-v0.2.4-5f7d5d7dd4" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-20T16:40:26Z"
      lastUpdateTime: "2020-01-20T16:40:26Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"fluentd-gcp-scaler","version":"v0.5.1"},"name":"fluentd-gcp-scaler","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"fluentd-gcp-scaler"}},"template":{"metadata":{"labels":{"k8s-app":"fluentd-gcp-scaler"}},"spec":{"containers":[{"command":["/scaler.sh","--ds-name=fluentd-gcp-v3.2.0","--scaling-policy=fluentd-gcp-scaling-policy"],"env":[{"name":"CPU_REQUEST","value":"100m"},{"name":"MEMORY_REQUEST","value":"200Mi"},{"name":"CPU_LIMIT","value":"1000m"},{"name":"MEMORY_LIMIT","value":"500Mi"}],"image":"k8s.gcr.io/fluentd-gcp-scaler:0.5.2","name":"fluentd-gcp-scaler"}],"serviceAccountName":"fluentd-gcp-scaler"}}}}
    creationTimestamp: "2019-06-18T22:22:28Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: fluentd-gcp-scaler
      version: v0.5.1
    name: fluentd-gcp-scaler
    namespace: kube-system
    resourceVersion: "63701970"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/fluentd-gcp-scaler
    uid: 8e47dc54-9217-11e9-a049-42010a8000d3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: fluentd-gcp-scaler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp-scaler
      spec:
        containers:
        - command:
          - /scaler.sh
          - --ds-name=fluentd-gcp-v3.2.0
          - --scaling-policy=fluentd-gcp-scaling-policy
          env:
          - name: CPU_REQUEST
            value: 100m
          - name: MEMORY_REQUEST
            value: 200Mi
          - name: CPU_LIMIT
            value: 1000m
          - name: MEMORY_LIMIT
            value: 500Mi
          image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
          imagePullPolicy: IfNotPresent
          name: fluentd-gcp-scaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp-scaler
        serviceAccountName: fluentd-gcp-scaler
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-06-18T22:22:28Z"
      lastUpdateTime: "2019-10-13T12:28:30Z"
      message: ReplicaSet "fluentd-gcp-scaler-5b5ff6f8bd" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-20T18:57:58Z"
      lastUpdateTime: "2020-01-20T18:57:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"heapster","kubernetes.io/cluster-service":"true","version":"v1.7.0"},"name":"heapster","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"heapster","version":"v1.7.0"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"heapster","version":"v1.7.0"}},"spec":{"containers":[{"command":["/heapster","--source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id","--sink=stackdriver:?cluster_name=tesg1\u0026use_old_resources=true\u0026use_new_resources=false\u0026min_interval_sec=100\u0026batch_export_timeout_sec=110\u0026cluster_location=us-central1-a"],"image":"gke.gcr.io/heapster:v1.7.0","livenessProbe":{"httpGet":{"path":"/healthz","port":8082,"scheme":"HTTP"},"initialDelaySeconds":180,"timeoutSeconds":5},"name":"heapster"},{"command":["/monitor","--source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prom-to-sd"},{"command":["/pod_nanny","--config-dir=/etc/config","--cpu=10m","--extra-cpu=0.5m","--memory=100Mi","--extra-memory=4Mi","--threshold=5","--deployment=heapster-v1.7.0","--container=heapster","--poll-period=300000","--estimator=exponential","--minClusterSize=5"],"env":[{"name":"MY_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"MY_POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/addon-resizer:1.8.3","name":"heapster-nanny","resources":{"limits":{"cpu":"50m","memory":"93360Ki"},"requests":{"cpu":"50m","memory":"93360Ki"}},"volumeMounts":[{"mountPath":"/etc/config","name":"heapster-config-volume"}]}],"priorityClassName":"system-cluster-critical","securityContext":{"fsGroup":65534,"supplementalGroups":[65534]},"serviceAccountName":"heapster","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"heapster-config"},"name":"heapster-config-volume"}]}}}}
    creationTimestamp: "2019-11-15T06:03:15Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: heapster
      kubernetes.io/cluster-service: "true"
      version: v1.7.0
    name: heapster
    namespace: kube-system
    resourceVersion: "63671046"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/heapster
    uid: 9d3c9ca6-076d-11ea-a0a0-42010a80004b
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: heapster
        version: v1.7.0
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          version: v1.7.0
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gke.gcr.io/heapster:v1.7.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.7.0
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 93360Ki
            requests:
              cpu: 50m
              memory: 93360Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-11-15T06:03:15Z"
      lastUpdateTime: "2019-11-15T06:03:15Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"heapster","kubernetes.io/cluster-service":"true","version":"v1.6.0-beta.1"},"name":"heapster-v1.6.0-beta.1","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"heapster","version":"v1.6.0-beta.1"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"heapster","version":"v1.6.0-beta.1"}},"spec":{"containers":[{"command":["/heapster","--source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id","--sink=stackdriver:?cluster_name=tesg1\u0026use_old_resources=true\u0026use_new_resources=false\u0026min_interval_sec=100\u0026batch_export_timeout_sec=110\u0026cluster_location=us-central1-a"],"image":"k8s.gcr.io/heapster-amd64:v1.6.0-beta.1","livenessProbe":{"httpGet":{"path":"/healthz","port":8082,"scheme":"HTTP"},"initialDelaySeconds":180,"timeoutSeconds":5},"name":"heapster"},{"command":["/monitor","--source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prom-to-sd"},{"command":["/pod_nanny","--config-dir=/etc/config","--cpu=10m","--extra-cpu=0.5m","--memory=100Mi","--extra-memory=4Mi","--threshold=5","--deployment=heapster-v1.6.0-beta.1","--container=heapster","--poll-period=300000","--estimator=exponential","--minClusterSize=5"],"env":[{"name":"MY_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"MY_POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/addon-resizer:1.8.3","name":"heapster-nanny","resources":{"limits":{"cpu":"50m","memory":"92960Ki"},"requests":{"cpu":"50m","memory":"92960Ki"}},"volumeMounts":[{"mountPath":"/etc/config","name":"heapster-config-volume"}]}],"priorityClassName":"system-cluster-critical","serviceAccountName":"heapster","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"heapster-config"},"name":"heapster-config-volume"}]}}}}
    creationTimestamp: "2019-06-18T22:22:14Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: heapster
      kubernetes.io/cluster-service: "true"
      version: v1.6.0-beta.1
    name: heapster-v1.6.0-beta.1
    namespace: kube-system
    resourceVersion: "63671167"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/heapster-v1.6.0-beta.1
    uid: 85ea2b9f-9217-11e9-a049-42010a8000d3
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: heapster
        version: v1.6.0-beta.1
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          version: v1.6.0-beta.1
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources:
            limits:
              cpu: 13m
              memory: 120Mi
            requests:
              cpu: 13m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.6.0-beta.1
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 92960Ki
            requests:
              cpu: 50m
              memory: 92960Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-06-18T22:22:14Z"
      lastUpdateTime: "2019-06-18T22:22:14Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"heapster","kubernetes.io/cluster-service":"true","version":"v1.6.1"},"name":"heapster-v1.6.1","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"heapster","version":"v1.6.1"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"heapster","version":"v1.6.1"}},"spec":{"containers":[{"command":["/heapster","--source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id","--sink=stackdriver:?cluster_name=tesg1\u0026use_old_resources=true\u0026use_new_resources=false\u0026min_interval_sec=100\u0026batch_export_timeout_sec=110\u0026cluster_location=us-central1-a"],"image":"gcr.io/stackdriver-agents/heapster-amd64:v1.6.1","livenessProbe":{"httpGet":{"path":"/healthz","port":8082,"scheme":"HTTP"},"initialDelaySeconds":180,"timeoutSeconds":5},"name":"heapster"},{"command":["/monitor","--source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.5.0","name":"prom-to-sd"},{"command":["/pod_nanny","--config-dir=/etc/config","--cpu=10m","--extra-cpu=0.5m","--memory=100Mi","--extra-memory=4Mi","--threshold=5","--deployment=heapster-v1.6.1","--container=heapster","--poll-period=300000","--estimator=exponential","--minClusterSize=5"],"env":[{"name":"MY_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"MY_POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/addon-resizer:1.8.3","name":"heapster-nanny","resources":{"limits":{"cpu":"50m","memory":"93360Ki"},"requests":{"cpu":"50m","memory":"93360Ki"}},"volumeMounts":[{"mountPath":"/etc/config","name":"heapster-config-volume"}]}],"priorityClassName":"system-cluster-critical","securityContext":{"fsGroup":65534,"supplementalGroups":[65534]},"serviceAccountName":"heapster","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"heapster-config"},"name":"heapster-config-volume"}]}}}}
    creationTimestamp: "2019-10-13T12:28:13Z"
    generation: 2
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: heapster
      kubernetes.io/cluster-service: "true"
      version: v1.6.1
    name: heapster-v1.6.1
    namespace: kube-system
    resourceVersion: "63671166"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/heapster-v1.6.1
    uid: ecb7de09-edb4-11e9-9caf-42010a800177
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: heapster
        version: v1.6.1
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          version: v1.6.1
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gcr.io/stackdriver-agents/heapster-amd64:v1.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources:
            limits:
              cpu: 13m
              memory: 120Mi
            requests:
              cpu: 13m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.6.1
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 93360Ki
            requests:
              cpu: 50m
              memory: 93360Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-10-13T12:28:13Z"
      lastUpdateTime: "2019-10-13T12:28:13Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 2
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "2"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns","kubernetes.io/cluster-service":"true"},"name":"kube-dns","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"kube-dns"}},"strategy":{"rollingUpdate":{"maxSurge":"10%","maxUnavailable":0}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"kube-dns"}},"spec":{"containers":[{"args":["--domain=cluster.local.","--dns-port=10053","--config-dir=/kube-dns-config","--v=2"],"env":[{"name":"PROMETHEUS_PORT","value":"10055"}],"image":"k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/healthcheck/kubedns","port":10054,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"kubedns","ports":[{"containerPort":10053,"name":"dns-local","protocol":"UDP"},{"containerPort":10053,"name":"dns-tcp-local","protocol":"TCP"},{"containerPort":10055,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"httpGet":{"path":"/readiness","port":8081,"scheme":"HTTP"},"initialDelaySeconds":3,"timeoutSeconds":5},"resources":{"limits":{"memory":"170Mi"},"requests":{"cpu":"100m","memory":"70Mi"}},"volumeMounts":[{"mountPath":"/kube-dns-config","name":"kube-dns-config"}]},{"args":["-v=2","-logtostderr","-configDir=/etc/k8s/dns/dnsmasq-nanny","-restartDnsmasq=true","--","-k","--cache-size=1000","--no-negcache","--log-facility=-","--server=/cluster.local/127.0.0.1#10053","--server=/in-addr.arpa/127.0.0.1#10053","--server=/ip6.arpa/127.0.0.1#10053"],"image":"k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/healthcheck/dnsmasq","port":10054,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"dnsmasq","ports":[{"containerPort":53,"name":"dns","protocol":"UDP"},{"containerPort":53,"name":"dns-tcp","protocol":"TCP"}],"resources":{"requests":{"cpu":"150m","memory":"20Mi"}},"volumeMounts":[{"mountPath":"/etc/k8s/dns/dnsmasq-nanny","name":"kube-dns-config"}]},{"args":["--v=2","--logtostderr","--probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV","--probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV"],"image":"k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4","livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/metrics","port":10054,"scheme":"HTTP"},"initialDelaySeconds":60,"successThreshold":1,"timeoutSeconds":5},"name":"sidecar","ports":[{"containerPort":10054,"name":"metrics","protocol":"TCP"}],"resources":{"requests":{"cpu":"10m","memory":"20Mi"}}},{"command":["/monitor","--source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits","--stackdriver-prefix=container.googleapis.com/internal/addons","--api-override=https://monitoring.googleapis.com/","--pod-id=$(POD_NAME)","--namespace-id=$(POD_NAMESPACE)","--v=2"],"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/prometheus-to-sd:v0.4.2","name":"prometheus-to-sd"}],"dnsPolicy":"Default","priorityClassName":"system-cluster-critical","serviceAccountName":"kube-dns","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"kube-dns","optional":true},"name":"kube-dns-config"}]}}}}
    creationTimestamp: "2019-06-18T22:22:14Z"
    generation: 3
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
    name: kube-dns
    namespace: kube-system
    resourceVersion: "63671162"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/kube-dns
    uid: 8648c0a7-9217-11e9-a049-42010a8000d3
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 10%
        maxUnavailable: 0
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --no-negcache
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
          image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          - --v=2
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.4.2
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2020-01-20T16:40:37Z"
      lastUpdateTime: "2020-01-20T16:40:37Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns-autoscaler","kubernetes.io/cluster-service":"true"},"name":"kube-dns-autoscaler","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"kube-dns-autoscaler"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"kube-dns-autoscaler"}},"spec":{"containers":[{"command":["/cluster-proportional-autoscaler","--namespace=kube-system","--configmap=kube-dns-autoscaler","--target=Deployment/kube-dns","--default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}}","--logtostderr=true","--v=2"],"image":"k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0","name":"autoscaler","resources":{"requests":{"cpu":"20m","memory":"10Mi"}}}],"priorityClassName":"system-cluster-critical","serviceAccountName":"kube-dns-autoscaler","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}]}}}}
    creationTimestamp: "2019-06-18T22:22:14Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: kube-dns-autoscaler
      kubernetes.io/cluster-service: "true"
    name: kube-dns-autoscaler
    namespace: kube-system
    resourceVersion: "63701964"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/kube-dns-autoscaler
    uid: 865dc484-9217-11e9-a049-42010a8000d3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns-autoscaler
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns-autoscaler
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=kube-dns-autoscaler
          - --target=Deployment/kube-dns
          - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
          - --logtostderr=true
          - --v=2
          image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns-autoscaler
        serviceAccountName: kube-dns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-06-18T22:22:14Z"
      lastUpdateTime: "2019-06-18T22:22:40Z"
      message: ReplicaSet "kube-dns-autoscaler-76fcd5f658" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-20T18:57:58Z"
      lastUpdateTime: "2020-01-20T18:57:58Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"glbc","kubernetes.io/cluster-service":"true","kubernetes.io/name":"GLBC"},"name":"l7-default-backend","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"glbc"}},"template":{"metadata":{"annotations":{"seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"glbc","name":"glbc"}},"spec":{"containers":[{"image":"k8s.gcr.io/defaultbackend-amd64:1.5","livenessProbe":{"httpGet":{"path":"/healthz","port":8080,"scheme":"HTTP"},"initialDelaySeconds":30,"timeoutSeconds":5},"name":"default-http-backend","ports":[{"containerPort":8080}],"resources":{"limits":{"cpu":"10m","memory":"20Mi"},"requests":{"cpu":"10m","memory":"20Mi"}}}]}}}}
    creationTimestamp: "2019-06-18T22:22:13Z"
    generation: 1
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: glbc
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: GLBC
    name: l7-default-backend
    namespace: kube-system
    resourceVersion: "63671058"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/l7-default-backend
    uid: 85b3f91d-9217-11e9-a049-42010a8000d3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: glbc
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: glbc
          name: glbc
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend-amd64:1.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: default-http-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources:
            limits:
              cpu: 10m
              memory: 20Mi
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-06-18T22:22:13Z"
      lastUpdateTime: "2019-06-18T22:22:54Z"
      message: ReplicaSet "l7-default-backend-6f8697844f" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    - lastTransitionTime: "2020-01-20T16:40:27Z"
      lastUpdateTime: "2020-01-20T16:40:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"magalix-agent","app.kubernetes.io/name":"magalix-agent","app.kubernetes.io/tier":"backend","app.kubernetes.io/version":"1"},"name":"magalix-agent","namespace":"kube-system"},"spec":{"replicas":1,"template":{"metadata":{"labels":{"name":"magalix-agent"}},"spec":{"containers":[{"args":["--kube-incluster","--gateway=ws://agent-gateway.magalix.com/","--source=kubelet","--trace-log=/agent.log"],"envFrom":[{"secretRef":{"name":"magalix-agent"}}],"image":"magalixcorp/agent:latest","imagePullPolicy":"Always","name":"agent","resources":{"limits":{"cpu":1,"memory":"750Mi"},"requests":{"cpu":"100m","memory":"200Mi"}}}],"restartPolicy":"Always","serviceAccountName":"magalix-agent"}}}}
    creationTimestamp: "2019-11-26T00:24:18Z"
    generation: 1
    labels:
      app.kubernetes.io/component: magalix-agent
      app.kubernetes.io/name: magalix-agent
      app.kubernetes.io/tier: backend
      app.kubernetes.io/version: "1"
    name: magalix-agent
    namespace: kube-system
    resourceVersion: "67105668"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/magalix-agent
    uid: 15c4a310-0fe3-11ea-a0a0-42010a80004b
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        name: magalix-agent
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: magalix-agent
      spec:
        containers:
        - args:
          - --kube-incluster
          - --gateway=ws://agent-gateway.magalix.com/
          - --source=kubelet
          - --trace-log=/agent.log
          envFrom:
          - secretRef:
              name: magalix-agent
          image: magalixcorp/agent:latest
          imagePullPolicy: Always
          name: agent
          resources:
            limits:
              cpu: "1"
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: magalix-agent
        serviceAccountName: magalix-agent
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-11-26T00:24:18Z"
      lastUpdateTime: "2019-11-26T00:24:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"metrics-server","kubernetes.io/cluster-service":"true","version":"v0.3.1"},"name":"metrics-server-v0.3.1","namespace":"kube-system"},"spec":{"selector":{"matchLabels":{"k8s-app":"metrics-server","version":"v0.3.1"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":"","seccomp.security.alpha.kubernetes.io/pod":"docker/default"},"labels":{"k8s-app":"metrics-server","version":"v0.3.1"},"name":"metrics-server"},"spec":{"containers":[{"command":["/metrics-server","--metric-resolution=30s","--kubelet-port=10255","--deprecated-kubelet-completely-insecure=true","--kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP"],"image":"k8s.gcr.io/metrics-server-amd64:v0.3.1","name":"metrics-server","ports":[{"containerPort":443,"name":"https","protocol":"TCP"}]},{"command":["/pod_nanny","--config-dir=/etc/config","--cpu=40m","--extra-cpu=0.5m","--memory=35Mi","--extra-memory=4Mi","--threshold=5","--deployment=metrics-server-v0.3.1","--container=metrics-server","--poll-period=300000","--estimator=exponential","--minClusterSize=5"],"env":[{"name":"MY_POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"MY_POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"k8s.gcr.io/addon-resizer:1.8.3","name":"metrics-server-nanny","resources":{"limits":{"cpu":"100m","memory":"300Mi"},"requests":{"cpu":"5m","memory":"50Mi"}},"volumeMounts":[{"mountPath":"/etc/config","name":"metrics-server-config-volume"}]}],"priorityClassName":"system-cluster-critical","serviceAccountName":"metrics-server","tolerations":[{"key":"CriticalAddonsOnly","operator":"Exists"}],"volumes":[{"configMap":{"name":"metrics-server-config"},"name":"metrics-server-config-volume"}]}}}}
    creationTimestamp: "2019-06-18T22:22:16Z"
    generation: 3
    labels:
      addonmanager.kubernetes.io/mode: Reconcile
      k8s-app: metrics-server
      kubernetes.io/cluster-service: "true"
      version: v0.3.1
    name: metrics-server-v0.3.1
    namespace: kube-system
    resourceVersion: "63701963"
    selfLink: /apis/apps/v1/namespaces/kube-system/deployments/metrics-server-v0.3.1
    uid: 8768fc58-9217-11e9-a049-42010a8000d3
  spec:
    progressDeadlineSeconds: 2147483647
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: metrics-server
        version: v0.3.1
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 43m
              memory: 55Mi
            requests:
              cpu: 43m
              memory: 55Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2019-06-18T22:22:16Z"
      lastUpdateTime: "2019-06-18T22:22:16Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "4"
      deployment.kubernetes.io/max-replicas: "5"
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"ReplicaSet","metadata":{"annotations":{"deployment.kubernetes.io/desired-replicas":"4","deployment.kubernetes.io/max-replicas":"5","deployment.kubernetes.io/revision":"1"},"creationTimestamp":"2020-01-16T03:46:32Z","generation":1,"labels":{"app":"redis","pod-template-hash":"57fc67768d","role":"master","tier":"backend"},"name":"redis-master-57fc67768d","namespace":"default","resourceVersion":"62208812","selfLink":"/apis/extensions/v1beta1/namespaces/default/replicasets/redis-master-57fc67768d","uid":"c97cfa09-3812-11ea-a0a0-42010a80004b"},"spec":{"replicas":4,"selector":{"matchLabels":{"app":"redis","pod-template-hash":"57fc67768d","role":"master","tier":"backend"}},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"redis","pod-template-hash":"57fc67768d","role":"master","tier":"backend"}},"spec":{"containers":[{"image":"k8s.gcr.io/redis:e2e","imagePullPolicy":"IfNotPresent","name":"master","ports":[{"containerPort":6379,"protocol":"TCP"}],"resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":4,"fullyLabeledReplicas":4,"observedGeneration":1,"readyReplicas":4,"replicas":4}}
    creationTimestamp: "2020-01-16T03:48:08Z"
    generation: 1
    labels:
      app: redis
      pod-template-hash: 57fc67768d
      role: master
      tier: backend
    name: redis-master-57fc67768d
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis-master
      uid: 02791b8d-3813-11ea-a0a0-42010a80004b
    resourceVersion: "63701986"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/redis-master-57fc67768d
    uid: 0258e998-3813-11ea-a0a0-42010a80004b
  spec:
    replicas: 4
    selector:
      matchLabels:
        app: redis
        pod-template-hash: 57fc67768d
        role: master
        tier: backend
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis
          pod-template-hash: 57fc67768d
          role: master
          tier: backend
      spec:
        containers:
        - image: k8s.gcr.io/redis:e2e
          imagePullPolicy: IfNotPresent
          name: master
          ports:
          - containerPort: 6379
            protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 4
    fullyLabeledReplicas: 4
    observedGeneration: 1
    readyReplicas: 4
    replicas: 4
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-09-04T21:24:52Z"
    generation: 1
    labels:
      app: redis-evict
      pod-template-hash: fd97bd94b
      role: master
      tier: backend
    name: redis-master-evict-fd97bd94b
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: redis-master-evict
      uid: 6ef03f30-cf5a-11e9-98f3-42010a8000c4
    resourceVersion: "66995076"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/redis-master-evict-fd97bd94b
    uid: 6ef1dd30-cf5a-11e9-98f3-42010a8000c4
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: redis-evict
        pod-template-hash: fd97bd94b
        role: master
        tier: backend
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: redis-evict
          pod-template-hash: fd97bd94b
          role: master
          tier: backend
      spec:
        containers:
        - image: k8s.gcr.io/redis:e2e
          imagePullPolicy: IfNotPresent
          name: master
          ports:
          - containerPort: 6379
            protocol: TCP
          resources:
            requests:
              cpu: 500m
              memory: 1700280Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-12-27T17:37:35Z"
    generation: 1
    labels:
      app: scheduler-pod-officer
      pod-template-hash: 6ccf79f787
    name: scheduler-pod-officer-6ccf79f787
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: scheduler-pod-officer
      uid: 91c33014-28cf-11ea-a0a0-42010a80004b
    resourceVersion: "63671019"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/scheduler-pod-officer-6ccf79f787
    uid: 91c56762-28cf-11ea-a0a0-42010a80004b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: scheduler-pod-officer
        pod-template-hash: 6ccf79f787
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: scheduler-pod-officer
          pod-template-hash: 6ccf79f787
      spec:
        containers:
        - image: gcr.io/closercriticalhop/my-kube-scheduler:1.0
          imagePullPolicy: Always
          name: scheduler-pod-officer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: scheduler-pod-officer
        serviceAccountName: scheduler-pod-officer
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "4"
      deployment.kubernetes.io/revision-history: "2"
    creationTimestamp: "2019-12-27T17:11:41Z"
    generation: 8
    labels:
      app: sleep-default-sched
      pod-template-hash: 5d5fd99d98
    name: sleep-default-sched-5d5fd99d98
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sleep-default-sched
      uid: 2ed20653-2a77-11ea-a0a0-42010a80004b
    resourceVersion: "56621749"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/sleep-default-sched-5d5fd99d98
    uid: f38e6328-28cb-11ea-a0a0-42010a80004b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: sleep-default-sched
        pod-template-hash: 5d5fd99d98
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: sleep-default-sched
          pod-template-hash: 5d5fd99d98
      spec:
        containers:
        - command:
          - /bin/sleep
          - infinity
          image: tutum/curl
          imagePullPolicy: IfNotPresent
          name: sleep
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: scheduler-pod-officer
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 8
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "6"
    creationTimestamp: "2019-12-28T18:28:33Z"
    generation: 4
    labels:
      app: sleep-default-sched
      hhh: hhh1
      pod-template-hash: 66c666cfb8
    name: sleep-default-sched-66c666cfb8
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sleep-default-sched
      uid: 2ed20653-2a77-11ea-a0a0-42010a80004b
    resourceVersion: "56621750"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/sleep-default-sched-66c666cfb8
    uid: dae91a45-299f-11ea-a0a0-42010a80004b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: sleep-default-sched
        pod-template-hash: 66c666cfb8
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: sleep-default-sched
          hhh: hhh1
          pod-template-hash: 66c666cfb8
      spec:
        containers:
        - command:
          - /bin/sleep
          - infinity
          image: tutum/curl
          imagePullPolicy: IfNotPresent
          name: sleep
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "9"
    creationTimestamp: "2019-12-29T20:09:56Z"
    generation: 1
    labels:
      app: sleep-default-sched
      ggg: ggg
      hhh: hhh1
      pod-template-hash: 78f7b9dffd
    name: sleep-default-sched-78f7b9dffd
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sleep-default-sched
      uid: 2ed20653-2a77-11ea-a0a0-42010a80004b
    resourceVersion: "65677293"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/sleep-default-sched-78f7b9dffd
    uid: 2ed80f72-2a77-11ea-a0a0-42010a80004b
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: sleep-default-sched
        pod-template-hash: 78f7b9dffd
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: sleep-default-sched
          ggg: ggg
          hhh: hhh1
          pod-template-hash: 78f7b9dffd
      spec:
        containers:
        - command:
          - /bin/sleep
          - infinity
          image: tutum/curl
          imagePullPolicy: IfNotPresent
          name: sleep
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "5"
      deployment.kubernetes.io/revision-history: 1,3
    creationTimestamp: "2019-12-27T16:59:33Z"
    generation: 11
    labels:
      app: sleep-default-sched
      pod-template-hash: 8d5fb8d9
    name: sleep-default-sched-8d5fb8d9
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sleep-default-sched
      uid: 2ed20653-2a77-11ea-a0a0-42010a80004b
    resourceVersion: "56621748"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/sleep-default-sched-8d5fb8d9
    uid: 419f1448-28ca-11ea-a0a0-42010a80004b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: sleep-default-sched
        pod-template-hash: 8d5fb8d9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: sleep-default-sched
          pod-template-hash: 8d5fb8d9
      spec:
        containers:
        - command:
          - /bin/sleep
          - infinity
          image: tutum/curl
          imagePullPolicy: IfNotPresent
          name: sleep
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 11
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "8"
    creationTimestamp: "2019-12-28T18:55:09Z"
    generation: 4
    labels:
      app: sleep-default-sched
      ggg: ggg
      hhh: hhh1
      pod-template-hash: 9896dfb5d
    name: sleep-default-sched-9896dfb5d
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: sleep-default-sched
      uid: 2ed20653-2a77-11ea-a0a0-42010a80004b
    resourceVersion: "56621751"
    selfLink: /apis/apps/v1/namespaces/default/replicasets/sleep-default-sched-9896dfb5d
    uid: 921f31c8-29a3-11ea-a0a0-42010a80004b
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: sleep-default-sched
        pod-template-hash: 9896dfb5d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: sleep-default-sched
          ggg: ggg
          hhh: hhh1
          pod-template-hash: 9896dfb5d
      spec:
        containers:
        - command:
          - /bin/sleep
          - infinity
          image: tutum/curl
          imagePullPolicy: IfNotPresent
          name: sleep
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeName: gke-tesg1-default-pool-ff7a1295-7kwg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-06-18T22:22:15Z"
    generation: 1
    labels:
      k8s-app: event-exporter
      pod-template-hash: 5f7d5d7dd4
      version: v0.2.4
    name: event-exporter-v0.2.4-5f7d5d7dd4
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: event-exporter-v0.2.4
      uid: 868d3e25-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671039"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/event-exporter-v0.2.4-5f7d5d7dd4
    uid: 868e50cc-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: event-exporter
        pod-template-hash: 5f7d5d7dd4
        version: v0.2.4
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: event-exporter
          pod-template-hash: 5f7d5d7dd4
          version: v0.2.4
      spec:
        containers:
        - command:
          - /event-exporter
          - -sink-opts=-stackdriver-resource-model=old
          image: k8s.gcr.io/event-exporter:v0.2.4
          imagePullPolicy: IfNotPresent
          name: event-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --source=event_exporter:http://localhost:80?whitelisted=stackdriver_sink_received_entry_count,stackdriver_sink_request_count,stackdriver_sink_successfully_sent_entry_count
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd-exporter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: event-exporter-sa
        serviceAccountName: event-exporter-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /etc/ssl/certs
            type: ""
          name: ssl-certs
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2019-10-13T12:28:24Z"
    generation: 1
    labels:
      k8s-app: fluentd-gcp-scaler
      pod-template-hash: 5b5ff6f8bd
    name: fluentd-gcp-scaler-5b5ff6f8bd
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: fluentd-gcp-scaler
      uid: 8e47dc54-9217-11e9-a049-42010a8000d3
    resourceVersion: "63701969"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/fluentd-gcp-scaler-5b5ff6f8bd
    uid: f325a5bd-edb4-11e9-9caf-42010a800177
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: fluentd-gcp-scaler
        pod-template-hash: 5b5ff6f8bd
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp-scaler
          pod-template-hash: 5b5ff6f8bd
      spec:
        containers:
        - command:
          - /scaler.sh
          - --ds-name=fluentd-gcp-v3.2.0
          - --scaling-policy=fluentd-gcp-scaling-policy
          env:
          - name: CPU_REQUEST
            value: 100m
          - name: MEMORY_REQUEST
            value: 200Mi
          - name: CPU_LIMIT
            value: 1000m
          - name: MEMORY_LIMIT
            value: 500Mi
          image: k8s.gcr.io/fluentd-gcp-scaler:0.5.2
          imagePullPolicy: IfNotPresent
          name: fluentd-gcp-scaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp-scaler
        serviceAccountName: fluentd-gcp-scaler
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-06-18T22:22:28Z"
    generation: 2
    labels:
      k8s-app: fluentd-gcp-scaler
      pod-template-hash: 7b895cbc89
    name: fluentd-gcp-scaler-7b895cbc89
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: fluentd-gcp-scaler
      uid: 8e47dc54-9217-11e9-a049-42010a8000d3
    resourceVersion: "31647238"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/fluentd-gcp-scaler-7b895cbc89
    uid: 8e49db2f-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: fluentd-gcp-scaler
        pod-template-hash: 7b895cbc89
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: fluentd-gcp-scaler
          pod-template-hash: 7b895cbc89
      spec:
        containers:
        - command:
          - /scaler.sh
          - --ds-name=fluentd-gcp-v3.2.0
          - --scaling-policy=fluentd-gcp-scaling-policy
          env:
          - name: CPU_REQUEST
            value: 100m
          - name: MEMORY_REQUEST
            value: 200Mi
          - name: CPU_LIMIT
            value: 1000m
          - name: MEMORY_LIMIT
            value: 500Mi
          image: k8s.gcr.io/fluentd-gcp-scaler:0.5.1
          imagePullPolicy: IfNotPresent
          name: fluentd-gcp-scaler
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: fluentd-gcp-scaler
        serviceAccountName: fluentd-gcp-scaler
        terminationGracePeriodSeconds: 30
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-11-15T06:03:15Z"
    generation: 1
    labels:
      k8s-app: heapster
      pod-template-hash: 7f957cc846
      version: v1.7.0
    name: heapster-7f957cc846
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: heapster
      uid: 9d3c9ca6-076d-11ea-a0a0-42010a80004b
    resourceVersion: "63671044"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/heapster-7f957cc846
    uid: 9d3e62c2-076d-11ea-a0a0-42010a80004b
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: heapster
        pod-template-hash: 7f957cc846
        version: v1.7.0
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          pod-template-hash: 7f957cc846
          version: v1.7.0
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gke.gcr.io/heapster:v1.7.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.7.0
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 93360Ki
            requests:
              cpu: 50m
              memory: 93360Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2019-06-18T22:22:47Z"
    generation: 1
    labels:
      k8s-app: heapster
      pod-template-hash: 68cdfd6769
      version: v1.6.0-beta.1
    name: heapster-v1.6.0-beta.1-68cdfd6769
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: heapster-v1.6.0-beta.1
      uid: 85ea2b9f-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671156"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/heapster-v1.6.0-beta.1-68cdfd6769
    uid: 99cfbb83-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: heapster
        pod-template-hash: 68cdfd6769
        version: v1.6.0-beta.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          pod-template-hash: 68cdfd6769
          version: v1.6.0-beta.1
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources:
            limits:
              cpu: 13m
              memory: 120Mi
            requests:
              cpu: 13m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.6.0-beta.1
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 92960Ki
            requests:
              cpu: 50m
              memory: 92960Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-06-18T22:22:14Z"
    generation: 2
    labels:
      k8s-app: heapster
      pod-template-hash: 6c56bcd877
      version: v1.6.0-beta.1
    name: heapster-v1.6.0-beta.1-6c56bcd877
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: heapster-v1.6.0-beta.1
      uid: 85ea2b9f-9217-11e9-a049-42010a8000d3
    resourceVersion: "717"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/heapster-v1.6.0-beta.1-6c56bcd877
    uid: 85eb9547-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: heapster
        pod-template-hash: 6c56bcd877
        version: v1.6.0-beta.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          pod-template-hash: 6c56bcd877
          version: v1.6.0-beta.1
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.6.0-beta.1
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 92960Ki
            requests:
              cpu: 50m
              memory: 92960Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2019-10-13T12:28:23Z"
    generation: 1
    labels:
      k8s-app: heapster
      pod-template-hash: 6c5f84b7db
      version: v1.6.1
    name: heapster-v1.6.1-6c5f84b7db
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: heapster-v1.6.1
      uid: ecb7de09-edb4-11e9-9caf-42010a800177
    resourceVersion: "63671157"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/heapster-v1.6.1-6c5f84b7db
    uid: f2941721-edb4-11e9-9caf-42010a800177
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: heapster
        pod-template-hash: 6c5f84b7db
        version: v1.6.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          pod-template-hash: 6c5f84b7db
          version: v1.6.1
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gcr.io/stackdriver-agents/heapster-amd64:v1.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources:
            limits:
              cpu: 13m
              memory: 120Mi
            requests:
              cpu: 13m
              memory: 120Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.6.1
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 93360Ki
            requests:
              cpu: 50m
              memory: 93360Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-10-13T12:28:13Z"
    generation: 2
    labels:
      k8s-app: heapster
      pod-template-hash: 859bfc4cc8
      version: v1.6.1
    name: heapster-v1.6.1-859bfc4cc8
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: heapster-v1.6.1
      uid: ecb7de09-edb4-11e9-9caf-42010a800177
    resourceVersion: "31647189"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/heapster-v1.6.1-859bfc4cc8
    uid: ecb99cd9-edb4-11e9-9caf-42010a800177
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: heapster
        pod-template-hash: 859bfc4cc8
        version: v1.6.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: heapster
          pod-template-hash: 859bfc4cc8
          version: v1.6.1
      spec:
        containers:
        - command:
          - /heapster
          - --source=kubernetes.summary_api:?host_id_annotation=container.googleapis.com/instance_id
          - --sink=stackdriver:?cluster_name=tesg1&use_old_resources=true&use_new_resources=false&min_interval_sec=100&batch_export_timeout_sec=110&cluster_location=us-central1-a
          image: gcr.io/stackdriver-agents/heapster-amd64:v1.6.1
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 180
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: heapster
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=heapster:http://localhost:8082?whitelisted=stackdriver_requests_count,stackdriver_timeseries_count
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.5.0
          imagePullPolicy: IfNotPresent
          name: prom-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=10m
          - --extra-cpu=0.5m
          - --memory=100Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=heapster-v1.6.1
          - --container=heapster
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: heapster-nanny
          resources:
            limits:
              cpu: 50m
              memory: 93360Ki
            requests:
              cpu: 50m
              memory: 93360Ki
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: heapster-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          fsGroup: 65534
          supplementalGroups:
          - 65534
        serviceAccount: heapster
        serviceAccountName: heapster
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: heapster-config
          name: heapster-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2019-11-15T06:03:15Z"
    generation: 2
    labels:
      k8s-app: kube-dns
      pod-template-hash: 67947d6c68
    name: kube-dns-67947d6c68
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns
      uid: 8648c0a7-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671152"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/kube-dns-67947d6c68
    uid: 9cea8b9b-076d-11ea-a0a0-42010a80004b
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 67947d6c68
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 67947d6c68
      spec:
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --no-negcache
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
          image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          - --v=2
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.4.2
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 2
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-06-18T22:22:14Z"
    generation: 1
    labels:
      k8s-app: kube-dns-autoscaler
      pod-template-hash: 76fcd5f658
    name: kube-dns-autoscaler-76fcd5f658
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns-autoscaler
      uid: 865dc484-9217-11e9-a049-42010a8000d3
    resourceVersion: "63701957"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/kube-dns-autoscaler-76fcd5f658
    uid: 865eb933-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: kube-dns-autoscaler
        pod-template-hash: 76fcd5f658
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns-autoscaler
          pod-template-hash: 76fcd5f658
      spec:
        containers:
        - command:
          - /cluster-proportional-autoscaler
          - --namespace=kube-system
          - --configmap=kube-dns-autoscaler
          - --target=Deployment/kube-dns
          - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true}}
          - --logtostderr=true
          - --v=2
          image: k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
          imagePullPolicy: IfNotPresent
          name: autoscaler
          resources:
            requests:
              cpu: 20m
              memory: 10Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns-autoscaler
        serviceAccountName: kube-dns-autoscaler
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-06-18T22:22:14Z"
    generation: 4
    labels:
      k8s-app: kube-dns
      pod-template-hash: b46cc9485
    name: kube-dns-b46cc9485
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: kube-dns
      uid: 8648c0a7-9217-11e9-a049-42010a8000d3
    resourceVersion: "42240030"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/kube-dns-b46cc9485
    uid: 8649f3ba-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: b46cc9485
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: b46cc9485
      spec:
        containers:
        - args:
          - --domain=cluster.local.
          - --dns-port=10053
          - --config-dir=/kube-dns-config
          - --v=2
          env:
          - name: PROMETHEUS_PORT
            value: "10055"
          image: k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.13
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/kubedns
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: kubedns
          ports:
          - containerPort: 10053
            name: dns-local
            protocol: UDP
          - containerPort: 10053
            name: dns-tcp-local
            protocol: TCP
          - containerPort: 10055
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /readiness
              port: 8081
              scheme: HTTP
            initialDelaySeconds: 3
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /kube-dns-config
            name: kube-dns-config
        - args:
          - -v=2
          - -logtostderr
          - -configDir=/etc/k8s/dns/dnsmasq-nanny
          - -restartDnsmasq=true
          - --
          - -k
          - --cache-size=1000
          - --no-negcache
          - --log-facility=-
          - --server=/cluster.local/127.0.0.1#10053
          - --server=/in-addr.arpa/127.0.0.1#10053
          - --server=/ip6.arpa/127.0.0.1#10053
          image: k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.13
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthcheck/dnsmasq
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: dnsmasq
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          resources:
            requests:
              cpu: 150m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/k8s/dns/dnsmasq-nanny
            name: kube-dns-config
        - args:
          - --v=2
          - --logtostderr
          - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,SRV
          - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,SRV
          image: k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.13
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /metrics
              port: 10054
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: sidecar
          ports:
          - containerPort: 10054
            name: metrics
            protocol: TCP
          resources:
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /monitor
          - --source=kubedns:http://localhost:10054?whitelisted=probe_kubedns_latency_ms,probe_kubedns_errors,dnsmasq_misses,dnsmasq_hits
          - --stackdriver-prefix=container.googleapis.com/internal/addons
          - --api-override=https://monitoring.googleapis.com/
          - --pod-id=$(POD_NAME)
          - --namespace-id=$(POD_NAMESPACE)
          - --v=2
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/prometheus-to-sd:v0.4.2
          imagePullPolicy: IfNotPresent
          name: prometheus-to-sd
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: Default
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-dns
        serviceAccountName: kube-dns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-dns
            optional: true
          name: kube-dns-config
  status:
    observedGeneration: 4
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-06-18T22:22:13Z"
    generation: 1
    labels:
      k8s-app: glbc
      name: glbc
      pod-template-hash: 6f8697844f
    name: l7-default-backend-6f8697844f
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: l7-default-backend
      uid: 85b3f91d-9217-11e9-a049-42010a8000d3
    resourceVersion: "63671055"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/l7-default-backend-6f8697844f
    uid: 85b5b6e6-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: glbc
        pod-template-hash: 6f8697844f
    template:
      metadata:
        annotations:
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: glbc
          name: glbc
          pod-template-hash: 6f8697844f
      spec:
        containers:
        - image: k8s.gcr.io/defaultbackend-amd64:1.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: default-http-backend
          ports:
          - containerPort: 8080
            protocol: TCP
          resources:
            limits:
              cpu: 10m
              memory: 20Mi
            requests:
              cpu: 10m
              memory: 20Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-11-26T00:24:18Z"
    generation: 1
    labels:
      name: magalix-agent
      pod-template-hash: 6cd478675d
    name: magalix-agent-6cd478675d
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: magalix-agent
      uid: 15c4a310-0fe3-11ea-a0a0-42010a80004b
    resourceVersion: "67105667"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/magalix-agent-6cd478675d
    uid: 15c70fb5-0fe3-11ea-a0a0-42010a80004b
  spec:
    replicas: 1
    selector:
      matchLabels:
        name: magalix-agent
        pod-template-hash: 6cd478675d
    template:
      metadata:
        creationTimestamp: null
        labels:
          name: magalix-agent
          pod-template-hash: 6cd478675d
      spec:
        containers:
        - args:
          - --kube-incluster
          - --gateway=ws://agent-gateway.magalix.com/
          - --source=kubelet
          - --trace-log=/agent.log
          envFrom:
          - secretRef:
              name: magalix-agent
          image: magalixcorp/agent:latest
          imagePullPolicy: Always
          name: agent
          resources:
            limits:
              cpu: "1"
              memory: 750Mi
            requests:
              cpu: 100m
              memory: 200Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: magalix-agent
        serviceAccountName: magalix-agent
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2019-06-18T22:22:47Z"
    generation: 2
    labels:
      k8s-app: metrics-server
      pod-template-hash: 5b4d6d8d98
      version: v0.3.1
    name: metrics-server-v0.3.1-5b4d6d8d98
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: 8768fc58-9217-11e9-a049-42010a8000d3
    resourceVersion: "31647095"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-5b4d6d8d98
    uid: 99e7e4e3-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 5b4d6d8d98
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 5b4d6d8d98
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 43m
              memory: 55Mi
            requests:
              cpu: 43m
              memory: 55Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2019-10-13T12:28:13Z"
    generation: 1
    labels:
      k8s-app: metrics-server
      pod-template-hash: 74dc55f45c
      version: v0.3.1
    name: metrics-server-v0.3.1-74dc55f45c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: 8768fc58-9217-11e9-a049-42010a8000d3
    resourceVersion: "63701958"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-74dc55f45c
    uid: ece49c9e-edb4-11e9-9caf-42010a800177
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 74dc55f45c
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 74dc55f45c
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources:
            limits:
              cpu: 43m
              memory: 55Mi
            requests:
              cpu: 43m
              memory: 55Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2019-06-18T22:22:16Z"
    generation: 2
    labels:
      k8s-app: metrics-server
      pod-template-hash: 856c65f8fd
      version: v0.3.1
    name: metrics-server-v0.3.1-856c65f8fd
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: metrics-server-v0.3.1
      uid: 8768fc58-9217-11e9-a049-42010a8000d3
    resourceVersion: "732"
    selfLink: /apis/apps/v1/namespaces/kube-system/replicasets/metrics-server-v0.3.1-856c65f8fd
    uid: 877213f8-9217-11e9-a049-42010a8000d3
  spec:
    replicas: 0
    selector:
      matchLabels:
        k8s-app: metrics-server
        pod-template-hash: 856c65f8fd
        version: v0.3.1
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
          seccomp.security.alpha.kubernetes.io/pod: docker/default
        creationTimestamp: null
        labels:
          k8s-app: metrics-server
          pod-template-hash: 856c65f8fd
          version: v0.3.1
        name: metrics-server
      spec:
        containers:
        - command:
          - /metrics-server
          - --metric-resolution=30s
          - --kubelet-port=10255
          - --deprecated-kubelet-completely-insecure=true
          image: k8s.gcr.io/metrics-server-amd64:v0.3.1
          imagePullPolicy: IfNotPresent
          name: metrics-server
          ports:
          - containerPort: 443
            name: https
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        - command:
          - /pod_nanny
          - --config-dir=/etc/config
          - --cpu=40m
          - --extra-cpu=0.5m
          - --memory=35Mi
          - --extra-memory=4Mi
          - --threshold=5
          - --deployment=metrics-server-v0.3.1
          - --container=metrics-server
          - --poll-period=300000
          - --estimator=exponential
          - --minClusterSize=5
          env:
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: k8s.gcr.io/addon-resizer:1.8.3
          imagePullPolicy: IfNotPresent
          name: metrics-server-nanny
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 5m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/config
            name: metrics-server-config-volume
        dnsPolicy: ClusterFirst
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: metrics-server
        serviceAccountName: metrics-server
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: metrics-server-config
          name: metrics-server-config-volume
  status:
    observedGeneration: 2
    replicas: 0
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
apiVersion: v1
items:
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      container.googleapis.com/instance_id: "8200410742980402968"
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2019-06-18T22:22:28Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/fluentd-ds-ready: "true"
      beta.kubernetes.io/instance-type: n1-standard-1
      beta.kubernetes.io/os: linux
      cloud.google.com/gke-nodepool: default-pool
      cloud.google.com/gke-os-distribution: cos
      disktype: test
      failure-domain.beta.kubernetes.io/region: us-central1
      failure-domain.beta.kubernetes.io/zone: us-central1-a
      kubernetes.io/hostname: gke-tesg1-default-pool-ff7a1295-7kwg
    name: gke-tesg1-default-pool-ff7a1295-7kwg
    resourceVersion: "67989791"
    selfLink: /api/v1/nodes/gke-tesg1-default-pool-ff7a1295-7kwg
    uid: 8e5838dd-9217-11e9-a049-42010a8000d3
  spec:
    podCIDR: 10.8.1.0/24
    providerID: gce://closercriticalhop/us-central1-a/gke-tesg1-default-pool-ff7a1295-7kwg
  status:
    addresses:
    - address: 10.128.0.14
      type: InternalIP
    - address: 35.188.138.174
      type: ExternalIP
    - address: gke-tesg1-default-pool-ff7a1295-7kwg.us-central1-a.c.closercriticalhop.internal
      type: InternalDNS
    - address: gke-tesg1-default-pool-ff7a1295-7kwg.us-central1-a.c.closercriticalhop.internal
      type: Hostname
    allocatable:
      attachable-volumes-gce-pd: "128"
      cpu: 940m
      ephemeral-storage: "47093746742"
      hugepages-2Mi: "0"
      memory: 2702284Ki
      pods: "110"
    capacity:
      attachable-volumes-gce-pd: "128"
      cpu: "1"
      ephemeral-storage: 98868448Ki
      hugepages-2Mi: "0"
      memory: 3787724Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2020-01-31T01:15:45Z"
      lastTransitionTime: "2019-11-28T15:56:37Z"
      message: kernel has no deadlock
      reason: KernelHasNoDeadlock
      status: "False"
      type: KernelDeadlock
    - lastHeartbeatTime: "2020-01-31T01:15:45Z"
      lastTransitionTime: "2019-11-28T15:56:37Z"
      message: Filesystem is not read-only
      reason: FilesystemIsNotReadOnly
      status: "False"
      type: ReadonlyFilesystem
    - lastHeartbeatTime: "2020-01-31T01:15:45Z"
      lastTransitionTime: "2019-11-28T16:01:38Z"
      message: node is functioning properly
      reason: UnregisterNetDevice
      status: "False"
      type: FrequentUnregisterNetDevice
    - lastHeartbeatTime: "2020-01-31T01:15:45Z"
      lastTransitionTime: "2019-11-28T16:01:38Z"
      message: kubelet is functioning properly
      reason: FrequentKubeletRestart
      status: "False"
      type: FrequentKubeletRestart
    - lastHeartbeatTime: "2020-01-31T01:15:45Z"
      lastTransitionTime: "2019-11-28T16:01:39Z"
      message: docker is functioning properly
      reason: FrequentDockerRestart
      status: "False"
      type: FrequentDockerRestart
    - lastHeartbeatTime: "2020-01-31T01:15:45Z"
      lastTransitionTime: "2019-11-28T16:01:41Z"
      message: containerd is functioning properly
      reason: FrequentContainerdRestart
      status: "False"
      type: FrequentContainerdRestart
    - lastHeartbeatTime: "2020-01-31T01:15:45Z"
      lastTransitionTime: "2019-11-28T16:01:38Z"
      message: docker overlay2 is functioning properly
      reason: CorruptDockerOverlay2
      status: "False"
      type: CorruptDockerOverlay2
    - lastHeartbeatTime: "2019-11-15T06:02:40Z"
      lastTransitionTime: "2019-11-15T06:02:40Z"
      message: NodeController create implicit route
      reason: RouteCreated
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2020-01-31T01:16:35Z"
      lastTransitionTime: "2020-01-20T18:57:41Z"
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: "2020-01-31T01:16:35Z"
      lastTransitionTime: "2020-01-20T18:57:41Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2020-01-31T01:16:35Z"
      lastTransitionTime: "2020-01-20T18:57:41Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2020-01-31T01:16:35Z"
      lastTransitionTime: "2019-06-18T22:22:28Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2020-01-31T01:16:35Z"
      lastTransitionTime: "2020-01-20T18:57:41Z"
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - gcr.io/closercriticalhop/my-kube-scheduler@sha256:b6b1d6088140479122c35f5d084dbe896cebed527c68e4f42da9d7bff5c4178e
      - gcr.io/closercriticalhop/my-kube-scheduler:1.0
      sizeBytes: 783983242
    - names:
      - gcr.io/google-samples/gb-frontend@sha256:d44e7d7491a537f822e7fe8615437e4a8a08f3a7a1d7d4cb9066b92f7556ba6d
      - gcr.io/google-samples/gb-frontend:v4
      sizeBytes: 512161546
    - names:
      - k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      - k8s.gcr.io/redis:e2e
      sizeBytes: 419003740
    - names:
      - k8s.gcr.io/node-problem-detector@sha256:f95cab985c26b2f46e9bd43283e0bfa88860c14e0fb0649266babe8b65e9eb2b
      - k8s.gcr.io/node-problem-detector:v0.4.1
      sizeBytes: 286572743
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      sizeBytes: 264721247
    - names:
      - tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f
      - tutum/curl:latest
      sizeBytes: 224373689
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:f8d5231b67b9c53f60068b535a11811d29d1b3efd53d2b79f2a2591ea338e4f2
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:0.6-1.6.0-1
      sizeBytes: 223242132
    - names:
      - gcr.io/google-containers/fluentd-elasticsearch@sha256:5a704c386f66bb3c24e3bcf2e94269c426f1473100fcd37b31579ca8b709c558
      - gcr.io/google-containers/fluentd-elasticsearch:v2.4.0
      sizeBytes: 139904122
    - names:
      - k8s.gcr.io/fluentd-elasticsearch@sha256:a54e7a450c0bdd19f49f56e487427a08c50f99ea8f8846179acf7d4182ce1fc0
      - k8s.gcr.io/fluentd-elasticsearch:v2.2.0
      sizeBytes: 138313727
    - names:
      - gcr.io/google_samples/gb-redisslave@sha256:90f62695e641e1a27d1a5e0bbb8b622205a48e18311b51b0da419ffad24b9016
      - gcr.io/google_samples/gb-redisslave:v1
      sizeBytes: 109508753
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
      sizeBytes: 102319441
    - names:
      - gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      - k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      sizeBytes: 100625828
    - names:
      - k8s.gcr.io/fluentd-gcp-scaler@sha256:4f28f10fb89506768910b858f7a18ffb996824a16d70d5ac895e49687df9ff58
      - k8s.gcr.io/fluentd-gcp-scaler:0.5.2
      sizeBytes: 90498960
    - names:
      - k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff
      - k8s.gcr.io/fluentd-gcp-scaler:0.5.1
      sizeBytes: 86637208
    - names:
      - k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521
      - k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      sizeBytes: 76016169
    - names:
      - k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca
      - k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3
      sizeBytes: 71797285
    - names:
      - k8s.gcr.io/kube-addon-manager@sha256:d53486c3a0b49ebee019932878dc44232735d5622a51dbbdcec7124199020d09
      - k8s.gcr.io/kube-addon-manager:v8.7
      sizeBytes: 63322109
    - names:
      - k8s.gcr.io/cpvpa-amd64@sha256:cfe7b0a11c9c8e18c87b1eb34fef9a7cbb8480a8da11fc2657f78dbf4739f869
      - k8s.gcr.io/cpvpa-amd64:v0.6.0
      sizeBytes: 51785854
    - names:
      - k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:618a82fa66cf0c75e4753369a6999032372be7308866fc9afb381789b1e5ad52
      - k8s.gcr.io/k8s-dns-kube-dns@sha256:c54a527a4ba8f1bc15e4796b09bf5d69313c7f42af9911dc437e056c0264a2fe
      - k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.13
      - k8s.gcr.io/k8s-dns-kube-dns:1.14.13
      sizeBytes: 51157394
    - names:
      - magalixcorp/agent@sha256:6b70c4adacd72552a4fd0994fb2de4cd7d0b1550cb630a6bd750507d23878d08
      - magalixcorp/agent:latest
      sizeBytes: 50888217
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:36359630278b119e7dd78f5437be1c667080108fa59ecba1b81cda3610dcf4d7
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
      sizeBytes: 50258329
    - names:
      - k8s.gcr.io/ip-masq-agent-amd64@sha256:269e0fb9d53fd37f7a135d6a55ea265a67279ba218aa148323f015cf70167340
      - k8s.gcr.io/ip-masq-agent-amd64:v2.3.0
      sizeBytes: 50144412
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:003f98d9f411ddfa6ff6d539196355e03ddd69fa4ed38c7ffb8fec6f729afe2d
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2
      sizeBytes: 49648481
    - names:
      - k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc
      - k8s.gcr.io/event-exporter:v0.2.4
      sizeBytes: 47261019
    - names:
      - k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:cedc8fe2098dffc26d17f64061296b7aa54258a31513b6c52df271a98bb522b3
      - k8s.gcr.io/k8s-dns-sidecar@sha256:c4f36a12fa297c48cb80779c89f6a58818d1fa7edccb08bc67d79acacc3871d7
      - k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.13
      - k8s.gcr.io/k8s-dns-sidecar:1.14.13
      sizeBytes: 42852039
    nodeInfo:
      architecture: amd64
      bootID: 59774689-8b2c-416c-91ad-22fc3f89b3ad
      containerRuntimeVersion: docker://17.3.2
      kernelVersion: 4.14.119+
      kubeProxyVersion: v1.12.8-gke.6
      kubeletVersion: v1.12.8-gke.6
      machineID: 4d7cd66a9a4a51086a345bbc612469c5
      operatingSystem: linux
      osImage: Container-Optimized OS from Google
      systemUUID: 4D7CD66A-9A4A-5108-6A34-5BBC612469C5
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      container.googleapis.com/instance_id: "1990270469742907227"
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2019-08-14T20:40:52Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/fluentd-ds-ready: "true"
      beta.kubernetes.io/instance-type: n1-standard-1
      beta.kubernetes.io/os: linux
      cloud.google.com/gke-nodepool: default-pool
      cloud.google.com/gke-os-distribution: cos
      failure-domain.beta.kubernetes.io/region: us-central1
      failure-domain.beta.kubernetes.io/zone: us-central1-a
      kubernetes.io/hostname: gke-tesg1-default-pool-ff7a1295-dmtd
    name: gke-tesg1-default-pool-ff7a1295-dmtd
    resourceVersion: "67989785"
    selfLink: /api/v1/nodes/gke-tesg1-default-pool-ff7a1295-dmtd
    uid: ce6972f8-bed3-11e9-98f3-42010a8000c4
  spec:
    podCIDR: 10.8.4.0/24
    providerID: gce://closercriticalhop/us-central1-a/gke-tesg1-default-pool-ff7a1295-dmtd
  status:
    addresses:
    - address: 10.128.0.17
      type: InternalIP
    - address: 35.222.104.34
      type: ExternalIP
    - address: gke-tesg1-default-pool-ff7a1295-dmtd.us-central1-a.c.closercriticalhop.internal
      type: InternalDNS
    - address: gke-tesg1-default-pool-ff7a1295-dmtd.us-central1-a.c.closercriticalhop.internal
      type: Hostname
    allocatable:
      attachable-volumes-gce-pd: "128"
      cpu: 940m
      ephemeral-storage: "47093746742"
      hugepages-2Mi: "0"
      memory: 2702284Ki
      pods: "110"
    capacity:
      attachable-volumes-gce-pd: "128"
      cpu: "1"
      ephemeral-storage: 98868448Ki
      hugepages-2Mi: "0"
      memory: 3787724Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2020-01-31T01:16:33Z"
      lastTransitionTime: "2019-08-14T20:40:45Z"
      message: Filesystem is not read-only
      reason: FilesystemIsNotReadOnly
      status: "False"
      type: ReadonlyFilesystem
    - lastHeartbeatTime: "2020-01-31T01:16:33Z"
      lastTransitionTime: "2019-08-14T20:45:46Z"
      message: node is functioning properly
      reason: UnregisterNetDevice
      status: "False"
      type: FrequentUnregisterNetDevice
    - lastHeartbeatTime: "2020-01-31T01:16:33Z"
      lastTransitionTime: "2019-08-14T20:45:46Z"
      message: kubelet is functioning properly
      reason: FrequentKubeletRestart
      status: "False"
      type: FrequentKubeletRestart
    - lastHeartbeatTime: "2020-01-31T01:16:33Z"
      lastTransitionTime: "2019-08-14T20:45:47Z"
      message: docker is functioning properly
      reason: FrequentDockerRestart
      status: "False"
      type: FrequentDockerRestart
    - lastHeartbeatTime: "2020-01-31T01:16:33Z"
      lastTransitionTime: "2019-08-14T20:45:48Z"
      message: containerd is functioning properly
      reason: FrequentContainerdRestart
      status: "False"
      type: FrequentContainerdRestart
    - lastHeartbeatTime: "2020-01-31T01:16:33Z"
      lastTransitionTime: "2019-08-14T20:45:46Z"
      message: docker overlay2 is functioning properly
      reason: CorruptDockerOverlay2
      status: "False"
      type: CorruptDockerOverlay2
    - lastHeartbeatTime: "2020-01-31T01:16:33Z"
      lastTransitionTime: "2019-08-14T20:40:45Z"
      message: kernel has no deadlock
      reason: KernelHasNoDeadlock
      status: "False"
      type: KernelDeadlock
    - lastHeartbeatTime: "2019-11-15T06:02:40Z"
      lastTransitionTime: "2019-11-15T06:02:40Z"
      message: NodeController create implicit route
      reason: RouteCreated
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2020-01-31T01:16:34Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: "2020-01-31T01:16:34Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2020-01-31T01:16:34Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2020-01-31T01:16:34Z"
      lastTransitionTime: "2019-08-14T20:40:52Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2020-01-31T01:16:34Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      - k8s.gcr.io/redis:e2e
      sizeBytes: 419003740
    - names:
      - k8s.gcr.io/node-problem-detector@sha256:f95cab985c26b2f46e9bd43283e0bfa88860c14e0fb0649266babe8b65e9eb2b
      - k8s.gcr.io/node-problem-detector:v0.4.1
      sizeBytes: 286572743
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      sizeBytes: 264721247
    - names:
      - tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f
      - tutum/curl:latest
      sizeBytes: 224373689
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:f8d5231b67b9c53f60068b535a11811d29d1b3efd53d2b79f2a2591ea338e4f2
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:0.6-1.6.0-1
      sizeBytes: 223242132
    - names:
      - gcr.io/google-containers/fluentd-elasticsearch@sha256:5a704c386f66bb3c24e3bcf2e94269c426f1473100fcd37b31579ca8b709c558
      - gcr.io/google-containers/fluentd-elasticsearch:v2.4.0
      sizeBytes: 139904122
    - names:
      - k8s.gcr.io/fluentd-elasticsearch@sha256:a54e7a450c0bdd19f49f56e487427a08c50f99ea8f8846179acf7d4182ce1fc0
      - k8s.gcr.io/fluentd-elasticsearch:v2.2.0
      sizeBytes: 138313727
    - names:
      - gcr.io/google_samples/gb-redisslave@sha256:90f62695e641e1a27d1a5e0bbb8b622205a48e18311b51b0da419ffad24b9016
      - gcr.io/google_samples/gb-redisslave:v1
      sizeBytes: 109508753
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
      sizeBytes: 102319441
    - names:
      - gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      - k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      sizeBytes: 100625828
    - names:
      - k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:a13c60e2a9d49f965095a1e003388926f3f2a6189ed4aecb1541f114c955f8ec
      - k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      sizeBytes: 86980681
    - names:
      - k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff
      - k8s.gcr.io/fluentd-gcp-scaler:0.5.1
      sizeBytes: 86637208
    - names:
      - k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:e55cbd5361a86bf0a01bfeaca2e958e15571f1e741356eab83bb444a13020d4c
      - k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      sizeBytes: 79319492
    - names:
      - gcr.io/stackdriver-agents/heapster-amd64@sha256:b00d659e538bedecf7014fffb7a6d0d3377326a921b4416a7727a7022a7be901
      - gcr.io/stackdriver-agents/heapster-amd64:v1.6.1
      sizeBytes: 78522086
    - names:
      - k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:0a70a8a9ae8cfe752021de84f13b3ecd109d9b5fbe3f1541c52fcd1d4c2c0b45
      - k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      sizeBytes: 77756591
    - names:
      - k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521
      - k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      sizeBytes: 76016169
    - names:
      - k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca
      - k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3
      sizeBytes: 71797285
    - names:
      - k8s.gcr.io/kube-addon-manager@sha256:d53486c3a0b49ebee019932878dc44232735d5622a51dbbdcec7124199020d09
      - k8s.gcr.io/kube-addon-manager:v8.7
      sizeBytes: 63322109
    - names:
      - k8s.gcr.io/cpvpa-amd64@sha256:cfe7b0a11c9c8e18c87b1eb34fef9a7cbb8480a8da11fc2657f78dbf4739f869
      - k8s.gcr.io/cpvpa-amd64:v0.6.0
      sizeBytes: 51785854
    - names:
      - k8s.gcr.io/k8s-dns-kube-dns@sha256:c54a527a4ba8f1bc15e4796b09bf5d69313c7f42af9911dc437e056c0264a2fe
      - k8s.gcr.io/k8s-dns-kube-dns:1.14.13
      sizeBytes: 51157394
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:36359630278b119e7dd78f5437be1c667080108fa59ecba1b81cda3610dcf4d7
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
      sizeBytes: 50258329
    - names:
      - k8s.gcr.io/ip-masq-agent-amd64@sha256:269e0fb9d53fd37f7a135d6a55ea265a67279ba218aa148323f015cf70167340
      - k8s.gcr.io/ip-masq-agent-amd64:v2.3.0
      sizeBytes: 50144412
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:003f98d9f411ddfa6ff6d539196355e03ddd69fa4ed38c7ffb8fec6f729afe2d
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2
      sizeBytes: 49648481
    - names:
      - k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc
      - k8s.gcr.io/event-exporter:v0.2.4
      sizeBytes: 47261019
    - names:
      - k8s.gcr.io/k8s-dns-sidecar@sha256:c4f36a12fa297c48cb80779c89f6a58818d1fa7edccb08bc67d79acacc3871d7
      - k8s.gcr.io/k8s-dns-sidecar:1.14.13
      sizeBytes: 42852039
    nodeInfo:
      architecture: amd64
      bootID: 6c7d7269-4520-4479-a4cf-d2924516bc49
      containerRuntimeVersion: docker://17.3.2
      kernelVersion: 4.14.119+
      kubeProxyVersion: v1.12.8-gke.6
      kubeletVersion: v1.12.8-gke.6
      machineID: d23666200dad64ee87e9b6a3ed99de05
      operatingSystem: linux
      osImage: Container-Optimized OS from Google
      systemUUID: D2366620-0DAD-64EE-87E9-B6A3ED99DE05
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      container.googleapis.com/instance_id: "7058830000734396184"
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2019-06-18T22:22:26Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/fluentd-ds-ready: "true"
      beta.kubernetes.io/instance-type: n1-standard-1
      beta.kubernetes.io/os: linux
      cloud.google.com/gke-nodepool: default-pool
      cloud.google.com/gke-os-distribution: cos
      failure-domain.beta.kubernetes.io/region: us-central1
      failure-domain.beta.kubernetes.io/zone: us-central1-a
      kubernetes.io/hostname: gke-tesg1-default-pool-ff7a1295-nvv4
    name: gke-tesg1-default-pool-ff7a1295-nvv4
    resourceVersion: "67989779"
    selfLink: /api/v1/nodes/gke-tesg1-default-pool-ff7a1295-nvv4
    uid: 8d7e5928-9217-11e9-a049-42010a8000d3
  spec:
    podCIDR: 10.8.2.0/24
    providerID: gce://closercriticalhop/us-central1-a/gke-tesg1-default-pool-ff7a1295-nvv4
  status:
    addresses:
    - address: 10.128.0.15
      type: InternalIP
    - address: 35.222.46.198
      type: ExternalIP
    - address: gke-tesg1-default-pool-ff7a1295-nvv4.us-central1-a.c.closercriticalhop.internal
      type: InternalDNS
    - address: gke-tesg1-default-pool-ff7a1295-nvv4.us-central1-a.c.closercriticalhop.internal
      type: Hostname
    allocatable:
      attachable-volumes-gce-pd: "128"
      cpu: 940m
      ephemeral-storage: "47093746742"
      hugepages-2Mi: "0"
      memory: 2702284Ki
      pods: "110"
    capacity:
      attachable-volumes-gce-pd: "128"
      cpu: "1"
      ephemeral-storage: 98868448Ki
      hugepages-2Mi: "0"
      memory: 3787724Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2020-01-31T01:16:29Z"
      lastTransitionTime: "2019-06-18T22:13:35Z"
      message: kernel has no deadlock
      reason: KernelHasNoDeadlock
      status: "False"
      type: KernelDeadlock
    - lastHeartbeatTime: "2020-01-31T01:16:29Z"
      lastTransitionTime: "2019-06-18T22:13:35Z"
      message: Filesystem is not read-only
      reason: FilesystemIsNotReadOnly
      status: "False"
      type: ReadonlyFilesystem
    - lastHeartbeatTime: "2020-01-31T01:16:29Z"
      lastTransitionTime: "2019-06-18T22:18:37Z"
      reason: UnregisterNetDevice
      status: "False"
      type: FrequentUnregisterNetDevice
    - lastHeartbeatTime: "2020-01-31T01:16:29Z"
      lastTransitionTime: "2019-06-18T22:48:36Z"
      message: kubelet is functioning properly
      reason: FrequentKubeletRestart
      status: "False"
      type: FrequentKubeletRestart
    - lastHeartbeatTime: "2020-01-31T01:16:29Z"
      lastTransitionTime: "2019-06-18T22:18:38Z"
      reason: FrequentDockerRestart
      status: "False"
      type: FrequentDockerRestart
    - lastHeartbeatTime: "2020-01-31T01:16:29Z"
      lastTransitionTime: "2019-06-18T22:18:39Z"
      reason: FrequentContainerdRestart
      status: "False"
      type: FrequentContainerdRestart
    - lastHeartbeatTime: "2020-01-31T01:16:29Z"
      lastTransitionTime: "2019-06-18T22:18:37Z"
      reason: CorruptDockerOverlay2
      status: "False"
      type: CorruptDockerOverlay2
    - lastHeartbeatTime: "2019-11-15T06:02:40Z"
      lastTransitionTime: "2019-11-15T06:02:40Z"
      message: NodeController create implicit route
      reason: RouteCreated
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2020-01-31T01:16:32Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: "2020-01-31T01:16:32Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2020-01-31T01:16:32Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2020-01-31T01:16:32Z"
      lastTransitionTime: "2019-06-18T22:22:26Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2020-01-31T01:16:32Z"
      lastTransitionTime: "2020-01-20T16:40:16Z"
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - gcr.io/google-samples/gb-frontend@sha256:d44e7d7491a537f822e7fe8615437e4a8a08f3a7a1d7d4cb9066b92f7556ba6d
      - gcr.io/google-samples/gb-frontend:v4
      sizeBytes: 512161546
    - names:
      - k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      - k8s.gcr.io/redis:e2e
      sizeBytes: 419003740
    - names:
      - k8s.gcr.io/node-problem-detector@sha256:f95cab985c26b2f46e9bd43283e0bfa88860c14e0fb0649266babe8b65e9eb2b
      - k8s.gcr.io/node-problem-detector:v0.4.1
      sizeBytes: 286572743
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      sizeBytes: 264721247
    - names:
      - tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f
      - tutum/curl:latest
      sizeBytes: 224373689
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:f8d5231b67b9c53f60068b535a11811d29d1b3efd53d2b79f2a2591ea338e4f2
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:0.6-1.6.0-1
      sizeBytes: 223242132
    - names:
      - gcr.io/google-containers/fluentd-elasticsearch@sha256:5a704c386f66bb3c24e3bcf2e94269c426f1473100fcd37b31579ca8b709c558
      - gcr.io/google-containers/fluentd-elasticsearch:v2.4.0
      sizeBytes: 139904122
    - names:
      - k8s.gcr.io/fluentd-elasticsearch@sha256:a54e7a450c0bdd19f49f56e487427a08c50f99ea8f8846179acf7d4182ce1fc0
      - k8s.gcr.io/fluentd-elasticsearch:v2.2.0
      sizeBytes: 138313727
    - names:
      - nginx@sha256:189cce606b29fb2a33ebc2fcecfa8e33b0b99740da4737133cdbcee92f3aba0a
      - nginx:latest
      sizeBytes: 126323486
    - names:
      - gcr.io/google_samples/gb-redisslave@sha256:90f62695e641e1a27d1a5e0bbb8b622205a48e18311b51b0da419ffad24b9016
      - gcr.io/google_samples/gb-redisslave:v1
      sizeBytes: 109508753
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
      sizeBytes: 102319441
    - names:
      - gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      - k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      sizeBytes: 100625828
    - names:
      - k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:a13c60e2a9d49f965095a1e003388926f3f2a6189ed4aecb1541f114c955f8ec
      - k8s.gcr.io/k8s-dns-kube-dns-amd64:1.15.4
      sizeBytes: 86980681
    - names:
      - k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff
      - k8s.gcr.io/fluentd-gcp-scaler:0.5.1
      sizeBytes: 86637208
    - names:
      - k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:e55cbd5361a86bf0a01bfeaca2e958e15571f1e741356eab83bb444a13020d4c
      - k8s.gcr.io/k8s-dns-sidecar-amd64:1.15.4
      sizeBytes: 79319492
    - names:
      - gcr.io/stackdriver-agents/heapster-amd64@sha256:b00d659e538bedecf7014fffb7a6d0d3377326a921b4416a7727a7022a7be901
      - gcr.io/stackdriver-agents/heapster-amd64:v1.6.1
      sizeBytes: 78522086
    - names:
      - k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64@sha256:0a70a8a9ae8cfe752021de84f13b3ecd109d9b5fbe3f1541c52fcd1d4c2c0b45
      - k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.15.4
      sizeBytes: 77756591
    - names:
      - k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521
      - k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      sizeBytes: 76016169
    - names:
      - k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca
      - k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3
      sizeBytes: 71797285
    - names:
      - k8s.gcr.io/kube-addon-manager@sha256:d53486c3a0b49ebee019932878dc44232735d5622a51dbbdcec7124199020d09
      - k8s.gcr.io/kube-addon-manager:v8.7
      sizeBytes: 63322109
    - names:
      - k8s.gcr.io/cpvpa-amd64@sha256:cfe7b0a11c9c8e18c87b1eb34fef9a7cbb8480a8da11fc2657f78dbf4739f869
      - k8s.gcr.io/cpvpa-amd64:v0.6.0
      sizeBytes: 51785854
    - names:
      - k8s.gcr.io/k8s-dns-kube-dns@sha256:c54a527a4ba8f1bc15e4796b09bf5d69313c7f42af9911dc437e056c0264a2fe
      - k8s.gcr.io/k8s-dns-kube-dns:1.14.13
      sizeBytes: 51157394
    - names:
      - magalixcorp/agent@sha256:6b70c4adacd72552a4fd0994fb2de4cd7d0b1550cb630a6bd750507d23878d08
      - magalixcorp/agent:latest
      sizeBytes: 50888217
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:36359630278b119e7dd78f5437be1c667080108fa59ecba1b81cda3610dcf4d7
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
      sizeBytes: 50258329
    - names:
      - k8s.gcr.io/ip-masq-agent-amd64@sha256:269e0fb9d53fd37f7a135d6a55ea265a67279ba218aa148323f015cf70167340
      - k8s.gcr.io/ip-masq-agent-amd64:v2.3.0
      sizeBytes: 50144412
    nodeInfo:
      architecture: amd64
      bootID: 960d769b-6862-4874-84ce-a8ad5c81ac7c
      containerRuntimeVersion: docker://17.3.2
      kernelVersion: 4.14.119+
      kubeProxyVersion: v1.12.8-gke.6
      kubeletVersion: v1.12.8-gke.6
      machineID: ee59f2468cc0dd1c4dafa7dbd4b0e08e
      operatingSystem: linux
      osImage: Container-Optimized OS from Google
      systemUUID: EE59F246-8CC0-DD1C-4DAF-A7DBD4B0E08E
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      container.googleapis.com/instance_id: "3319695570928721688"
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2019-06-18T22:22:26Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/fluentd-ds-ready: "true"
      beta.kubernetes.io/instance-type: n1-standard-1
      beta.kubernetes.io/os: linux
      cloud.google.com/gke-nodepool: default-pool
      cloud.google.com/gke-os-distribution: cos
      failure-domain.beta.kubernetes.io/region: us-central1
      failure-domain.beta.kubernetes.io/zone: us-central1-a
      kubernetes.io/hostname: gke-tesg1-default-pool-ff7a1295-z7lx
    name: gke-tesg1-default-pool-ff7a1295-z7lx
    resourceVersion: "67989770"
    selfLink: /api/v1/nodes/gke-tesg1-default-pool-ff7a1295-z7lx
    uid: 8d768464-9217-11e9-a049-42010a8000d3
  spec:
    podCIDR: 10.8.0.0/24
    providerID: gce://closercriticalhop/us-central1-a/gke-tesg1-default-pool-ff7a1295-z7lx
  status:
    addresses:
    - address: 10.128.0.13
      type: InternalIP
    - address: 35.238.66.195
      type: ExternalIP
    - address: gke-tesg1-default-pool-ff7a1295-z7lx.us-central1-a.c.closercriticalhop.internal
      type: InternalDNS
    - address: gke-tesg1-default-pool-ff7a1295-z7lx.us-central1-a.c.closercriticalhop.internal
      type: Hostname
    allocatable:
      attachable-volumes-gce-pd: "128"
      cpu: 940m
      ephemeral-storage: "47093746742"
      hugepages-2Mi: "0"
      memory: 2702292Ki
      pods: "110"
    capacity:
      attachable-volumes-gce-pd: "128"
      cpu: "1"
      ephemeral-storage: 98868448Ki
      hugepages-2Mi: "0"
      memory: 3787732Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2020-01-31T01:15:46Z"
      lastTransitionTime: "2019-06-18T22:23:49Z"
      message: kubelet is functioning properly
      reason: FrequentKubeletRestart
      status: "False"
      type: FrequentKubeletRestart
    - lastHeartbeatTime: "2020-01-31T01:15:46Z"
      lastTransitionTime: "2019-06-18T22:23:50Z"
      message: docker is functioning properly
      reason: FrequentDockerRestart
      status: "False"
      type: FrequentDockerRestart
    - lastHeartbeatTime: "2020-01-31T01:15:46Z"
      lastTransitionTime: "2019-06-18T22:23:51Z"
      message: containerd is functioning properly
      reason: FrequentContainerdRestart
      status: "False"
      type: FrequentContainerdRestart
    - lastHeartbeatTime: "2020-01-31T01:15:46Z"
      lastTransitionTime: "2019-06-18T22:23:49Z"
      message: docker overlay2 is functioning properly
      reason: CorruptDockerOverlay2
      status: "False"
      type: CorruptDockerOverlay2
    - lastHeartbeatTime: "2020-01-31T01:15:46Z"
      lastTransitionTime: "2019-06-18T22:18:48Z"
      message: kernel has no deadlock
      reason: KernelHasNoDeadlock
      status: "False"
      type: KernelDeadlock
    - lastHeartbeatTime: "2020-01-31T01:15:46Z"
      lastTransitionTime: "2019-06-18T22:18:48Z"
      message: Filesystem is not read-only
      reason: FilesystemIsNotReadOnly
      status: "False"
      type: ReadonlyFilesystem
    - lastHeartbeatTime: "2020-01-31T01:15:46Z"
      lastTransitionTime: "2019-06-18T22:23:49Z"
      message: node is functioning properly
      reason: UnregisterNetDevice
      status: "False"
      type: FrequentUnregisterNetDevice
    - lastHeartbeatTime: "2019-11-15T06:02:40Z"
      lastTransitionTime: "2019-11-15T06:02:40Z"
      message: NodeController create implicit route
      reason: RouteCreated
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2020-01-31T01:16:30Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: "2020-01-31T01:16:30Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2020-01-31T01:16:30Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2020-01-31T01:16:30Z"
      lastTransitionTime: "2019-06-18T22:22:26Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2020-01-31T01:16:30Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - gcr.io/google-samples/gb-frontend@sha256:d44e7d7491a537f822e7fe8615437e4a8a08f3a7a1d7d4cb9066b92f7556ba6d
      - gcr.io/google-samples/gb-frontend:v4
      sizeBytes: 512161546
    - names:
      - k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      - k8s.gcr.io/redis:e2e
      sizeBytes: 419003740
    - names:
      - k8s.gcr.io/node-problem-detector@sha256:f95cab985c26b2f46e9bd43283e0bfa88860c14e0fb0649266babe8b65e9eb2b
      - k8s.gcr.io/node-problem-detector:v0.4.1
      sizeBytes: 286572743
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      sizeBytes: 264721247
    - names:
      - tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f
      - tutum/curl:latest
      sizeBytes: 224373689
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:f8d5231b67b9c53f60068b535a11811d29d1b3efd53d2b79f2a2591ea338e4f2
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:0.6-1.6.0-1
      sizeBytes: 223242132
    - names:
      - gcr.io/google-containers/fluentd-elasticsearch@sha256:5a704c386f66bb3c24e3bcf2e94269c426f1473100fcd37b31579ca8b709c558
      - gcr.io/google-containers/fluentd-elasticsearch:v2.4.0
      sizeBytes: 139904122
    - names:
      - k8s.gcr.io/fluentd-elasticsearch@sha256:a54e7a450c0bdd19f49f56e487427a08c50f99ea8f8846179acf7d4182ce1fc0
      - k8s.gcr.io/fluentd-elasticsearch:v2.2.0
      sizeBytes: 138313727
    - names:
      - gcr.io/google_samples/gb-redisslave@sha256:90f62695e641e1a27d1a5e0bbb8b622205a48e18311b51b0da419ffad24b9016
      - gcr.io/google_samples/gb-redisslave:v1
      sizeBytes: 109508753
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
      sizeBytes: 102319441
    - names:
      - gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      - k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      sizeBytes: 100625828
    - names:
      - k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff
      - k8s.gcr.io/fluentd-gcp-scaler:0.5.1
      sizeBytes: 86637208
    - names:
      - gke.gcr.io/heapster@sha256:a9c899ab08312640c779837e3345c8e0a8591b352cd421167f7c7af852d40393
      - gke.gcr.io/heapster:v1.7.0
      sizeBytes: 84026759
    - names:
      - k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521
      - k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      sizeBytes: 76016169
    - names:
      - k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca
      - k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3
      sizeBytes: 71797285
    - names:
      - k8s.gcr.io/kube-addon-manager@sha256:d53486c3a0b49ebee019932878dc44232735d5622a51dbbdcec7124199020d09
      - k8s.gcr.io/kube-addon-manager:v8.7
      sizeBytes: 63322109
    - names:
      - k8s.gcr.io/cpvpa-amd64@sha256:cfe7b0a11c9c8e18c87b1eb34fef9a7cbb8480a8da11fc2657f78dbf4739f869
      - k8s.gcr.io/cpvpa-amd64:v0.6.0
      sizeBytes: 51785854
    - names:
      - k8s.gcr.io/k8s-dns-kube-dns-amd64@sha256:618a82fa66cf0c75e4753369a6999032372be7308866fc9afb381789b1e5ad52
      - k8s.gcr.io/k8s-dns-kube-dns@sha256:c54a527a4ba8f1bc15e4796b09bf5d69313c7f42af9911dc437e056c0264a2fe
      - k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.13
      - k8s.gcr.io/k8s-dns-kube-dns:1.14.13
      sizeBytes: 51157394
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:36359630278b119e7dd78f5437be1c667080108fa59ecba1b81cda3610dcf4d7
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
      sizeBytes: 50258329
    - names:
      - k8s.gcr.io/ip-masq-agent-amd64@sha256:269e0fb9d53fd37f7a135d6a55ea265a67279ba218aa148323f015cf70167340
      - k8s.gcr.io/ip-masq-agent-amd64:v2.3.0
      sizeBytes: 50144412
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:003f98d9f411ddfa6ff6d539196355e03ddd69fa4ed38c7ffb8fec6f729afe2d
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2
      sizeBytes: 49648481
    - names:
      - k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc
      - k8s.gcr.io/event-exporter:v0.2.4
      sizeBytes: 47261019
    - names:
      - k8s.gcr.io/k8s-dns-sidecar-amd64@sha256:cedc8fe2098dffc26d17f64061296b7aa54258a31513b6c52df271a98bb522b3
      - k8s.gcr.io/k8s-dns-sidecar@sha256:c4f36a12fa297c48cb80779c89f6a58818d1fa7edccb08bc67d79acacc3871d7
      - k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.13
      - k8s.gcr.io/k8s-dns-sidecar:1.14.13
      sizeBytes: 42852039
    - names:
      - k8s.gcr.io/gke-node-termination-handler@sha256:e08ca863a547754fa7b75064bdad04f04cbef86c7b0a181ecc7304e747623181
      sizeBytes: 42465304
    - names:
      - k8s.gcr.io/prometheus-to-sd@sha256:916d01e197885450ca45447f1c8a9cbc04d30c93d5382461eb202dcbf2b8d0b0
      - k8s.gcr.io/prometheus-to-sd:v0.5.2
      sizeBytes: 42110407
    nodeInfo:
      architecture: amd64
      bootID: 6564162d-c803-423a-8bbf-95fe253ddc46
      containerRuntimeVersion: docker://17.3.2
      kernelVersion: 4.14.119+
      kubeProxyVersion: v1.12.8-gke.6
      kubeletVersion: v1.12.8-gke.6
      machineID: bf5138a46948adda5c9ff5e303e7c758
      operatingSystem: linux
      osImage: Container-Optimized OS from Google
      systemUUID: BF5138A4-6948-ADDA-5C9F-F5E303E7C758
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      container.googleapis.com/instance_id: "1708505217667559698"
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2019-08-14T18:17:23Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/fluentd-ds-ready: "true"
      beta.kubernetes.io/instance-type: n1-standard-1
      beta.kubernetes.io/os: linux
      cloud.google.com/gke-nodepool: default-pool
      cloud.google.com/gke-os-distribution: cos
      failure-domain.beta.kubernetes.io/region: us-central1
      failure-domain.beta.kubernetes.io/zone: us-central1-a
      kubernetes.io/hostname: gke-tesg1-default-pool-ff7a1295-zpc5
    name: gke-tesg1-default-pool-ff7a1295-zpc5
    resourceVersion: "67989764"
    selfLink: /api/v1/nodes/gke-tesg1-default-pool-ff7a1295-zpc5
    uid: c333d73c-bebf-11e9-98f3-42010a8000c4
  spec:
    podCIDR: 10.8.3.0/24
    providerID: gce://closercriticalhop/us-central1-a/gke-tesg1-default-pool-ff7a1295-zpc5
    taints:
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      timeAdded: "2020-01-24T04:23:35Z"
    unschedulable: true
  status:
    addresses:
    - address: 10.128.0.16
      type: InternalIP
    - address: 34.68.171.234
      type: ExternalIP
    - address: gke-tesg1-default-pool-ff7a1295-zpc5.us-central1-a.c.closercriticalhop.internal
      type: InternalDNS
    - address: gke-tesg1-default-pool-ff7a1295-zpc5.us-central1-a.c.closercriticalhop.internal
      type: Hostname
    allocatable:
      attachable-volumes-gce-pd: "128"
      cpu: 940m
      ephemeral-storage: "47093746742"
      hugepages-2Mi: "0"
      memory: 2702284Ki
      pods: "110"
    capacity:
      attachable-volumes-gce-pd: "128"
      cpu: "1"
      ephemeral-storage: 98868448Ki
      hugepages-2Mi: "0"
      memory: 3787724Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2020-01-31T01:16:10Z"
      lastTransitionTime: "2019-08-14T18:17:03Z"
      message: kernel has no deadlock
      reason: KernelHasNoDeadlock
      status: "False"
      type: KernelDeadlock
    - lastHeartbeatTime: "2020-01-31T01:16:10Z"
      lastTransitionTime: "2019-08-14T18:17:03Z"
      message: Filesystem is not read-only
      reason: FilesystemIsNotReadOnly
      status: "False"
      type: ReadonlyFilesystem
    - lastHeartbeatTime: "2020-01-31T01:16:10Z"
      lastTransitionTime: "2019-08-14T18:22:05Z"
      message: node is functioning properly
      reason: UnregisterNetDevice
      status: "False"
      type: FrequentUnregisterNetDevice
    - lastHeartbeatTime: "2020-01-31T01:16:10Z"
      lastTransitionTime: "2019-08-14T18:22:05Z"
      message: kubelet is functioning properly
      reason: FrequentKubeletRestart
      status: "False"
      type: FrequentKubeletRestart
    - lastHeartbeatTime: "2020-01-31T01:16:10Z"
      lastTransitionTime: "2019-08-14T18:22:06Z"
      message: docker is functioning properly
      reason: FrequentDockerRestart
      status: "False"
      type: FrequentDockerRestart
    - lastHeartbeatTime: "2020-01-31T01:16:10Z"
      lastTransitionTime: "2019-08-14T18:22:07Z"
      message: containerd is functioning properly
      reason: FrequentContainerdRestart
      status: "False"
      type: FrequentContainerdRestart
    - lastHeartbeatTime: "2020-01-31T01:16:10Z"
      lastTransitionTime: "2019-08-14T18:22:05Z"
      message: docker overlay2 is functioning properly
      reason: CorruptDockerOverlay2
      status: "False"
      type: CorruptDockerOverlay2
    - lastHeartbeatTime: "2019-11-15T06:02:40Z"
      lastTransitionTime: "2019-11-15T06:02:40Z"
      message: NodeController create implicit route
      reason: RouteCreated
      status: "False"
      type: NetworkUnavailable
    - lastHeartbeatTime: "2020-01-31T01:16:28Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet has sufficient disk space available
      reason: KubeletHasSufficientDisk
      status: "False"
      type: OutOfDisk
    - lastHeartbeatTime: "2020-01-31T01:16:28Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2020-01-31T01:16:28Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2020-01-31T01:16:28Z"
      lastTransitionTime: "2019-08-14T18:17:23Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2020-01-31T01:16:28Z"
      lastTransitionTime: "2020-01-20T16:40:17Z"
      message: kubelet is posting ready status. AppArmor enabled
      reason: KubeletReady
      status: "True"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - gcr.io/closercriticalhop/my-kube-scheduler@sha256:99e537dc22449f3a79a13bf2a1d5d5e5a97176f520987111e76cebb0bc568ca6
      - gcr.io/closercriticalhop/my-kube-scheduler:1.0
      sizeBytes: 783983104
    - names:
      - k8s.gcr.io/redis@sha256:f066bcf26497fbc55b9bf0769cb13a35c0afa2aa42e737cc46b7fb04b23a2f25
      - k8s.gcr.io/redis:e2e
      sizeBytes: 419003740
    - names:
      - wordpress@sha256:6216f64ab88fc51d311e38c7f69ca3f9aaba621492b4f1fa93ddf63093768845
      - wordpress:4.8-apache
      sizeBytes: 408405324
    - names:
      - k8s.gcr.io/node-problem-detector@sha256:f95cab985c26b2f46e9bd43283e0bfa88860c14e0fb0649266babe8b65e9eb2b
      - k8s.gcr.io/node-problem-detector:v0.4.1
      sizeBytes: 286572743
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:6c8574a40816676cd908cfa89d16463002b56ca05fa76d0c912e116bc0ab867e
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:1.6.8
      sizeBytes: 264721247
    - names:
      - mysql@sha256:3b00a364fb74246ca119d16111eb62f7302b2ff66d51e373c2bb209f8a1f3b9e
      - mysql:5.6
      sizeBytes: 256505910
    - names:
      - tutum/curl@sha256:b6f16e88387acd4e6326176b212b3dae63f5b2134e69560d0b0673cfb0fb976f
      - tutum/curl:latest
      sizeBytes: 224373689
    - names:
      - gcr.io/stackdriver-agents/stackdriver-logging-agent@sha256:f8d5231b67b9c53f60068b535a11811d29d1b3efd53d2b79f2a2591ea338e4f2
      - gcr.io/stackdriver-agents/stackdriver-logging-agent:0.6-1.6.0-1
      sizeBytes: 223242132
    - names:
      - gcr.io/google-containers/fluentd-elasticsearch@sha256:5a704c386f66bb3c24e3bcf2e94269c426f1473100fcd37b31579ca8b709c558
      - gcr.io/google-containers/fluentd-elasticsearch:v2.4.0
      sizeBytes: 139904122
    - names:
      - k8s.gcr.io/fluentd-elasticsearch@sha256:a54e7a450c0bdd19f49f56e487427a08c50f99ea8f8846179acf7d4182ce1fc0
      - k8s.gcr.io/fluentd-elasticsearch:v2.2.0
      sizeBytes: 138313727
    - names:
      - gcr.io/google_samples/gb-redisslave@sha256:90f62695e641e1a27d1a5e0bbb8b622205a48e18311b51b0da419ffad24b9016
      - gcr.io/google_samples/gb-redisslave:v1
      sizeBytes: 109508753
    - names:
      - k8s.gcr.io/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0
      - k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
      sizeBytes: 102319441
    - names:
      - gcr.io/google_containers/kube-proxy:v1.12.8-gke.6
      - k8s.gcr.io/kube-proxy:v1.12.8-gke.6
      sizeBytes: 100625828
    - names:
      - k8s.gcr.io/fluentd-gcp-scaler@sha256:a5ace7506d393c4ed65eb2cbb6312c64ab357fcea16dff76b9055bc6e498e5ff
      - k8s.gcr.io/fluentd-gcp-scaler:0.5.1
      sizeBytes: 86637208
    - names:
      - k8s.gcr.io/heapster-amd64@sha256:9fae0af136ce0cf4f88393b3670f7139ffc464692060c374d2ae748e13144521
      - k8s.gcr.io/heapster-amd64:v1.6.0-beta.1
      sizeBytes: 76016169
    - names:
      - k8s.gcr.io/ingress-gce-glbc-amd64@sha256:14f14351a03038b238232e60850a9cfa0dffbed0590321ef84216a432accc1ca
      - k8s.gcr.io/ingress-gce-glbc-amd64:v1.2.3
      sizeBytes: 71797285
    - names:
      - k8s.gcr.io/kube-addon-manager@sha256:d53486c3a0b49ebee019932878dc44232735d5622a51dbbdcec7124199020d09
      - k8s.gcr.io/kube-addon-manager:v8.7
      sizeBytes: 63322109
    - names:
      - k8s.gcr.io/cpvpa-amd64@sha256:cfe7b0a11c9c8e18c87b1eb34fef9a7cbb8480a8da11fc2657f78dbf4739f869
      - k8s.gcr.io/cpvpa-amd64:v0.6.0
      sizeBytes: 51785854
    - names:
      - k8s.gcr.io/k8s-dns-kube-dns@sha256:c54a527a4ba8f1bc15e4796b09bf5d69313c7f42af9911dc437e056c0264a2fe
      - k8s.gcr.io/k8s-dns-kube-dns:1.14.13
      sizeBytes: 51157394
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:36359630278b119e7dd78f5437be1c667080108fa59ecba1b81cda3610dcf4d7
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.2.0
      sizeBytes: 50258329
    - names:
      - k8s.gcr.io/ip-masq-agent-amd64@sha256:269e0fb9d53fd37f7a135d6a55ea265a67279ba218aa148323f015cf70167340
      - k8s.gcr.io/ip-masq-agent-amd64:v2.3.0
      sizeBytes: 50144412
    - names:
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64@sha256:003f98d9f411ddfa6ff6d539196355e03ddd69fa4ed38c7ffb8fec6f729afe2d
      - k8s.gcr.io/cluster-proportional-autoscaler-amd64:1.1.2-r2
      sizeBytes: 49648481
    - names:
      - k8s.gcr.io/event-exporter@sha256:16ca66e2b5dc7a1ce6a5aafcb21d0885828b75cdfc08135430480f7ad2364adc
      - k8s.gcr.io/event-exporter:v0.2.4
      sizeBytes: 47261019
    - names:
      - k8s.gcr.io/k8s-dns-sidecar@sha256:c4f36a12fa297c48cb80779c89f6a58818d1fa7edccb08bc67d79acacc3871d7
      - k8s.gcr.io/k8s-dns-sidecar:1.14.13
      sizeBytes: 42852039
    - names:
      - k8s.gcr.io/gke-node-termination-handler@sha256:e08ca863a547754fa7b75064bdad04f04cbef86c7b0a181ecc7304e747623181
      sizeBytes: 42465304
    nodeInfo:
      architecture: amd64
      bootID: f69b7223-09a1-4756-9644-1af7492b288f
      containerRuntimeVersion: docker://17.3.2
      kernelVersion: 4.14.119+
      kubeProxyVersion: v1.12.8-gke.6
      kubeletVersion: v1.12.8-gke.6
      machineID: 95ae2ccbed38b5f7c6ec7a1d6a3db01f
      operatingSystem: linux
      osImage: Container-Optimized OS from Google
      systemUUID: 95AE2CCB-ED38-B5F7-C6EC-7A1D6A3DB01F
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
apiVersion: v1
items:
- apiVersion: scheduling.k8s.io/v1beta1
  kind: PriorityClass
  metadata:
    creationTimestamp: "2019-10-23T19:33:52Z"
    generation: 1
    name: high-priority
    resourceVersion: "34997983"
    selfLink: /apis/scheduling.k8s.io/v1beta1/priorityclasses/high-priority
    uid: 0b498121-f5cc-11e9-9caf-42010a800177
  value: 1000000
- apiVersion: scheduling.k8s.io/v1beta1
  description: Used for system critical pods that must run in the cluster, but can
    be moved to another node if necessary.
  kind: PriorityClass
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"scheduling.k8s.io/v1beta1","description":"Used for system critical pods that must run in the cluster, but can be moved to another node if necessary.","kind":"PriorityClass","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:21:51Z","generation":1,"name":"system-cluster-critical","resourceVersion":"38","selfLink":"/apis/scheduling.k8s.io/v1beta1/priorityclasses/system-cluster-critical","uid":"78b6ec0c-9217-11e9-a049-42010a8000d3"},"value":2000000000}
    creationTimestamp: "2019-06-18T22:21:51Z"
    generation: 1
    name: system-cluster-critical
    resourceVersion: "24658597"
    selfLink: /apis/scheduling.k8s.io/v1beta1/priorityclasses/system-cluster-critical
    uid: 78b6ec0c-9217-11e9-a049-42010a8000d3
  value: 2000000000
- apiVersion: scheduling.k8s.io/v1beta1
  description: Used for system critical pods that must not be moved from their current
    node.
  kind: PriorityClass
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"scheduling.k8s.io/v1beta1","description":"Used for system critical pods that must not be moved from their current node.","kind":"PriorityClass","metadata":{"annotations":{},"creationTimestamp":"2019-06-18T22:21:51Z","generation":1,"name":"system-node-critical","resourceVersion":"37","selfLink":"/apis/scheduling.k8s.io/v1beta1/priorityclasses/system-node-critical","uid":"78b57dd0-9217-11e9-a049-42010a8000d3"},"value":2000001000}
    creationTimestamp: "2019-06-18T22:21:51Z"
    generation: 1
    name: system-node-critical
    resourceVersion: "24658598"
    selfLink: /apis/scheduling.k8s.io/v1beta1/priorityclasses/system-node-critical
    uid: 78b57dd0-9217-11e9-a049-42010a8000d3
  value: 2000001000
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
apiVersion: v1
items:
- apiVersion: v1
  kind: LimitRange
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"LimitRange","metadata":{"annotations":{},"name":"limits","namespace":"default"},"spec":{"limits":[{"defaultRequest":{"cpu":"100m"},"type":"Container"}]}}
    creationTimestamp: "2019-06-18T22:22:12Z"
    name: limits
    namespace: default
    resourceVersion: "281"
    selfLink: /api/v1/namespaces/default/limitranges/limits
    uid: 84e7bd7a-9217-11e9-a049-42010a8000d3
  spec:
    limits:
    - defaultRequest:
        cpu: 100m
      type: Container
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
apiVersion: v1
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    creationTimestamp: "2019-06-18T22:27:27Z"
    name: gke-resource-quotas
    namespace: default
    resourceVersion: "66994990"
    selfLink: /api/v1/namespaces/default/resourcequotas/gke-resource-quotas
    uid: 40b9ef9e-9218-11e9-a049-42010a8000d3
  spec:
    hard:
      count/ingresses.extensions: "100"
      count/jobs.batch: 5k
      pods: "1500"
      services: "500"
  status:
    hard:
      count/ingresses.extensions: "100"
      count/jobs.batch: 5k
      pods: "1500"
      services: "500"
    used:
      count/ingresses.extensions: "0"
      count/jobs.batch: "0"
      pods: "8"
      services: "6"
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    creationTimestamp: "2019-06-18T22:27:27Z"
    name: gke-resource-quotas
    namespace: kube-public
    resourceVersion: "1600"
    selfLink: /api/v1/namespaces/kube-public/resourcequotas/gke-resource-quotas
    uid: 40cc1431-9218-11e9-a049-42010a8000d3
  spec:
    hard:
      count/ingresses.extensions: "100"
      count/jobs.batch: 5k
      pods: "1500"
      services: "500"
  status:
    hard:
      count/ingresses.extensions: "100"
      count/jobs.batch: 5k
      pods: "1500"
      services: "500"
    used:
      count/ingresses.extensions: "0"
      count/jobs.batch: "0"
      pods: "0"
      services: "0"
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    creationTimestamp: "2019-06-18T22:27:27Z"
    name: gke-resource-quotas
    namespace: kube-system
    resourceVersion: "65677306"
    selfLink: /api/v1/namespaces/kube-system/resourcequotas/gke-resource-quotas
    uid: 40de71fc-9218-11e9-a049-42010a8000d3
  spec:
    hard:
      count/ingresses.extensions: "100"
      count/jobs.batch: 5k
      pods: "1500"
      services: "500"
  status:
    hard:
      count/ingresses.extensions: "100"
      count/jobs.batch: 5k
      pods: "1500"
      services: "500"
    used:
      count/ingresses.extensions: "0"
      count/jobs.batch: "0"
      pods: "26"
      services: "4"
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
apiVersion: v1
items: []
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
